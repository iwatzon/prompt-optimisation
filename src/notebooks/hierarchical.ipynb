{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Hierarchical Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptReview(BaseModel):\n",
    "    \"\"\"Review of the prompt\"\"\"\n",
    "    feedback: str = Field(description=\"Feedback on the most recent prompt\")\n",
    "\n",
    "class CorePrinciples:\n",
    "    def __init__(self, core_principles: List[str]):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "class WorkerAgent:\n",
    "    \"\"\"\n",
    "    Worker Agent class defining agents that provide feedback on prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, temp: float = 1.0, model: str = \"gpt-4o\"):\n",
    "        self.position = position\n",
    "        self.core_principles = str(core_principles)\n",
    "        self.system_message = SystemMessage(content=f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\")        \n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=temp,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "    def review_prompt(self, prompt: str, criteria: str) -> PromptReview:\n",
    "        \"\"\"\n",
    "        Generates a review of the prompt.\n",
    "        \"\"\"\n",
    "        template = \"\"\"```{prompt}```\n",
    "\n",
    "Your task is to think outside the box and provide feedback on the prompt above with creative recommendations on how to improve it in light of your core principles.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your feedback and recommendations.\n",
    "\n",
    "Your feedback must be actionable and less than 100 words. \n",
    "Provide clear and concise instructions on how to implement your recommendations.\n",
    "\n",
    "Below are strict guidelines that you MUST follow when providing feedback and recommendations:\n",
    "- DO NOT suggest modifying existing restrictions.\n",
    "- DO NOT suggest modifying or removing negations.\n",
    "- DO NOT suggest adding, modifying or removing placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized is you do not follow these guidelines.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt carefully as an expert {position}. \n",
    "2. Identify the most critical aspects of the prompt that need improvement. \n",
    "3. Think outside the box to provide creative feedback with recommendations on how to improve the prompt in light of your core principles.\n",
    "4. Ensure that your feedback is less than 100 words.\n",
    "5. Ensure that your feedback follows the strict guidelines provided above.\n",
    "6. Submit your feedback.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output only your feedback. Example output:\n",
    "\n",
    "{{\n",
    "    \"feedback\": \"Feedback on the original prompt\"\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "        pydantic_parser = PydanticOutputParser(pydantic_object=PromptReview)\n",
    "        prompt_template = PromptTemplate(\n",
    "            system_message=self.system_message,\n",
    "            template=template,\n",
    "            input_variables=[\"position\", \"prompt\", \"criteria\"],\n",
    "            partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt_template | self.llm | pydantic_parser\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                completion = chain.invoke({\"position\": self.position, \"prompt\": prompt, \"criteria\": criteria})\n",
    "                # Validate the output before returning\n",
    "                if completion.feedback:\n",
    "                    return completion\n",
    "                # else:\n",
    "                #     print(\"Validation failed: Missing required fields in completion\")\n",
    "                #     print(\"Raw output:\", completion)\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred:\", e)\n",
    "                continue\n",
    "        else:\n",
    "            raise Exception(\"Failed to parse output after 3 attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Workforce(BaseModel):\n",
    "    \"\"\"Details of workforce generated by the leader agent.\"\"\"\n",
    "    positions: List[str] = Field(description=\"Positions of the workers in the workforce\")\n",
    "    core_principles: List[List[str]] = Field(description=\"Core principles of the workers in the workforce\")\n",
    "    \n",
    "\n",
    "class FeedbackSummary(BaseModel):\n",
    "    \"\"\"Summary of feedback from the worker agents.\"\"\"\n",
    "    feedback_summary: str = Field(description=\"Collated and summarised feedback from the workforce\")\n",
    "\n",
    "\n",
    "class ApprovalDecision(BaseModel):\n",
    "    \"\"\"Decision of the leader agent to approve or disapprove the prompt.\"\"\"\n",
    "    approved: bool = Field(description=\"Decision to approve or disapprove the prompt\")\n",
    "\n",
    "\n",
    "class TeamLeaderAgent:\n",
    "    \"\"\"\n",
    "    TeamLeaderAgent class defining an agent that manages a team of worker agents to help optimise prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, team: str, core_principles: CorePrinciples, prompt: str, criteria: str = None, temp: float = 0.5, model: str = \"gpt-4o\", workforce: List[WorkerAgent] = None):\n",
    "        self.position = team\n",
    "        self.core_principles = str(core_principles)\n",
    "        self.system_message = SystemMessage(content=f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\")\n",
    "        self.prompt = prompt\n",
    "        self.criteria = criteria\n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=temp,\n",
    "            model=model,\n",
    "        )\n",
    "        self.workforce = workforce\n",
    "\n",
    "    def leader_feedback(self, prompt: str, feedback: list) -> str:\n",
    "        \"\"\"\n",
    "        LeaderAgent to decide the next worker or to finish.\n",
    "        \"\"\"\n",
    "        # All workers except the current worker\n",
    "        template = \"\"\"Your task is to refine and summarise the feedback below.    \n",
    "```{feedback}```\n",
    "\n",
    "\n",
    "You must capture the important aspects of the feedback and recommendations provided by your team.\n",
    "Your summary must be actionable and precise, providing clear recommendations for improvements.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your decision.\n",
    "\n",
    "Below are strict guidelines that you MUST follow when summarising feedback and recommendations:\n",
    "- DO NOT suggest modifying existing restrictions.\n",
    "- DO NOT suggest modifying or removing negations.\n",
    "- DO NOT suggest adding, modifying or removing placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your summarisation process should be as follows:\n",
    "1. Read the feedback carefully as an experienced {position}.\n",
    "2. Identify the important aspects of the feedback and recommendations provided by your team.\n",
    "3. Refine and summarise the feedback and recommendations into a concise and actionable summary.\n",
    "4. Ensure that your summary adheres to the strict guidelines provided above.\n",
    "5. Ensure that your summary is actionable and precise, providing clear recommendations for improvements to the prompt.\n",
    "6. Submit your summary.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output only the feedback summary. Example output:\n",
    "\n",
    "{{\n",
    "    \"feedback_summary\": \"Feedback summary\",\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "        pydantic_parser = PydanticOutputParser(pydantic_object=FeedbackSummary)\n",
    "        prompt_template = PromptTemplate(\n",
    "            system_message=self.system_message,\n",
    "            template=template,\n",
    "            input_variables=[\"feedback\", \"prompt\", \"criteria\", \"position\"],\n",
    "            partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt_template | self.llm | pydantic_parser\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                output = chain.invoke({\"feedback\": feedback, \"prompt\": prompt, \"criteria\": self.criteria, \"position\": self.position})\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred:\", e)\n",
    "                continue\n",
    "        return output.feedback_summary\n",
    "       \n",
    "    def get_feedback(self, state):\n",
    "        \"\"\"\n",
    "        Get feedback from wokers. Feedback collected concurrently.\n",
    "        \"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = [executor.submit(worker.review_prompt, state['prompt'], self.criteria) for worker in self.workforce]\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "            # feedback = [result[-1].content for result in results]\n",
    "            feedback = [result.feedback for result in results]\n",
    "\n",
    "        summary = self.leader_feedback(state[\"prompt\"], feedback)\n",
    "\n",
    "        return summary\n",
    "    \n",
    "    def get_approval(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Agent to approve or reject the prompt.\n",
    "        \"\"\"\n",
    "        template = \"\"\"```{prompt}```\n",
    "\n",
    "Your task is to review the prompt above and decide whether or not it should be approved in light of your team and team role.\n",
    "\n",
    "Return True if you approve and think the prompt already optimal in the aspects your team:, {members} specialises in. \n",
    "Return False if you disapprove and think the prompt could be improved with the insights of your team.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your feedback.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt carefully as an expert {position}\n",
    "2. Determine whether the prompt will be effective in instructing the model to perform the desired task.\n",
    "3. If you believe the prompt is effective, approve it for use by the model.\n",
    "4. Submit your decision.\n",
    "    \n",
    "{format_instructions}\n",
    "\n",
    "Output only your approval decision. Example output:\n",
    "\n",
    "{{\n",
    "    \"approved\": \"Approval decision\"\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\n",
    "\"\"\"\n",
    "        pydantic_parser = PydanticOutputParser(pydantic_object=ApprovalDecision)\n",
    "        prompt_template = PromptTemplate(\n",
    "            system_message=self.system_message,\n",
    "            template=template,\n",
    "            input_variables=[\"prompt\", \"members\", \"criteria\", \"position\"],\n",
    "            partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt_template | self.llm | pydantic_parser\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                completion = chain.invoke({\"prompt\": prompt, \"members\": [worker.position for worker in self.workforce], \"criteria\": self.criteria, \"position\": self.position})\n",
    "                # Validate the output before returning\n",
    "                if completion.approved is not None:\n",
    "                    return completion.approved\n",
    "                # else:\n",
    "                #     print(\"Validation failed: Missing required fields in completion\")\n",
    "                #     print(\"Raw output:\", completion)\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred:\", e)\n",
    "                continue\n",
    "        else:\n",
    "            raise Exception(\"Failed to parse output after 3 attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain(BaseModel):\n",
    "    \"\"\"Domain of the prompt.\"\"\"\n",
    "    domain: str = Field(description=\"Domain of the prompt\")\n",
    "    description: str = Field(description=\"Description of the domain team role\")\n",
    "\n",
    "\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Decision on the next worker to process.\"\"\"\n",
    "    next: str = Field(description=\"The next team to process\")\n",
    "\n",
    "\n",
    "class UpdatedPrompt(BaseModel):\n",
    "    \"\"\"Updated prompt based on feedback from the team leader.\"\"\"\n",
    "    updated_prompt: str = Field(description=\"Updated prompt based on feedback\")\n",
    "\n",
    "\n",
    "class LeaderAgent:\n",
    "    \"\"\"\n",
    "    LeaderAgent class defining an agent that generates and communicates worker agents to help optimise prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_prompt: str, criteria: str = None, temp: float = 0.0, model: str = \"gpt-4o\", team_leaders: List[TeamLeaderAgent] = None):\n",
    "        self.system_message = SystemMessage(content=f\"\"\"You are an experienced: Head AI Engineer. Your core principles are:\n",
    "- Always adhere to strict guidelines\n",
    "- Always strive to achieve success criteria\n",
    "- Always listen to feedback\n",
    "- Always consider the limitations of AI models\"\"\")        \n",
    "        self.base_prompt = base_prompt\n",
    "        self.prompt = base_prompt\n",
    "        self.criteria = criteria\n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=temp,\n",
    "            model=model,\n",
    "        )\n",
    "        self.team_leaders = team_leaders\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def run_approval(self, prompt: str) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Run the approval process for the prompt. Run concurrently\n",
    "        \"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(team_leader.get_approval, prompt) for team_leader in self.team_leaders]\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        return results\n",
    "    \n",
    "    def leader_decision(self, state: dict) -> RouteDecision:\n",
    "        \"\"\"\n",
    "        LeaderAgent to decide the next worker or to finish.\n",
    "        \"\"\"\n",
    "        self.iterations += 1\n",
    "        approval_results = self.run_approval(state[\"prompt\"])\n",
    "        print(\"Approval results:\", approval_results)\n",
    "        if all(approval_results) or (self.iterations > 10):\n",
    "            return {\"next\": \"FINISH\", \"prompt\": state[\"prompt\"], \"messages\": state[\"messages\"]} \n",
    "        else:\n",
    "            self.iterations += 1\n",
    "            disapproved_team_leaders = [team_leader for i, team_leader in enumerate(self.team_leaders) if not approval_results[i]]\n",
    "            options = [team_leaders.position for team_leaders in disapproved_team_leaders]\n",
    "            # shuffle options to avoid positional bias\n",
    "            random.shuffle(options)\n",
    "            # options = [\"FINISH\"] + members\n",
    "            template = \"\"\"```{prompt}```\n",
    "\n",
    "Your task is to review the prompt above and decide the next team to provide feedback.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your decision.\n",
    "\n",
    "Select one of the following teams to provide feedback on the prompt: \n",
    "{options}\n",
    "\n",
    "Think carefully about pairing the aspects of the prompt that need improvement and how they relate to the expertise of each team.\n",
    "If you think multiple aspects of the prompt need improvement, select the most suitable team to provide feedback on the most critical aspects of the prompt.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output only the next team name to process. Example output:\n",
    "\n",
    "{{\n",
    "    \"next\": \"Next team\",\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "            pydantic_parser = PydanticOutputParser(pydantic_object=RouteDecision)\n",
    "            prompt_template = PromptTemplate(\n",
    "                system_message=self.system_message,\n",
    "                template=template,\n",
    "                input_variables=[\"prompt\", \"criteria\", \"options\"],\n",
    "                partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "            )\n",
    "            chain = prompt_template | self.llm | pydantic_parser\n",
    "            for _ in range(3):\n",
    "                try:\n",
    "                    output = chain.invoke({\"prompt\": state[\"prompt\"], \"criteria\": self.criteria, \"options\": options})\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(\"Exception occurred:\", e)\n",
    "                    continue\n",
    "            return {\"next\": output.next, \"prompt\": state[\"prompt\"], \"messages\": state[\"messages\"]}\n",
    "\n",
    "    def update_prompt(self, prompt: str, feedback: str, history) -> str:\n",
    "        \"\"\"\n",
    "        Updates the prompt with the feedback from the worker agent.\n",
    "        \"\"\"\n",
    "        template = \"\"\"```{prompt}```\n",
    "\n",
    "Your task is to make updates to the prompt above by implementing the feedback below. You must action the feedback keeping in mind your core principles.\n",
    "```{feedback}```\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "You will be penalized if the updated prompt does not meet this criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if your do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Read the prompt and feedback carefully.\n",
    "2. Review the discussion history to ensure you do not repeat any mistakes.\n",
    "3. Implement the feedback provided to help the prompt better achieve what is expected.\n",
    "4. Ensure that your update follows the strict guidelines provided above.\n",
    "5. Submit your updated prompt.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output only your updated prompt. Example output:\n",
    "\n",
    "{{\n",
    "    \"updated_prompt\": \"Updated prompt\",\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "        pydantic_parser = PydanticOutputParser(pydantic_object=UpdatedPrompt)\n",
    "        prompt_template = PromptTemplate(\n",
    "            system_message=self.system_message,\n",
    "            template=template,\n",
    "            input_variables=[\"prompt\", \"feedback\", \"history\", \"criteria\"],\n",
    "            partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt_template | self.llm | pydantic_parser\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                output = chain.invoke({\"prompt\": prompt, \"feedback\": feedback, \"history\": history, \"criteria\": self.criteria})\n",
    "                if output.updated_prompt:\n",
    "                    return output.updated_prompt\n",
    "                # else:\n",
    "                #     print(\"Validation failed: Missing required fields in completion\")\n",
    "                #     print(\"Raw output:\", output)\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred:\", e)\n",
    "                continue\n",
    "\n",
    "    def construct_team_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of team leader agents.\n",
    "        \"\"\"\n",
    "        # The agent state is the input to each node in the graph\n",
    "        class TeamState(TypedDict):\n",
    "            # The annotation tells the graph that new messages will always be added to the current states\n",
    "            messages: Sequence[BaseMessage]\n",
    "            prompt: str\n",
    "            next: str\n",
    "\n",
    "        def team_node(state, team_leader):\n",
    "            try:\n",
    "                feedback = team_leader.get_feedback(state)\n",
    "                updated_prompt = self.update_prompt(state[\"prompt\"], feedback, state[\"messages\"])\n",
    "                # updated_prompt = output[-1].content \n",
    "                self.prompt = updated_prompt\n",
    "                return {\n",
    "                    \"messages\": state[\"messages\"] + [\n",
    "                        HumanMessage(content=f\"Feedback: {feedback}\", name=team_leader.position),\n",
    "                        AIMessage(content=f\"Updated Prompt: {updated_prompt}\", name=\"Leader\")\n",
    "                    ],\n",
    "                    \"prompt\": updated_prompt,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                # Log the error and return to leader with the most recent prompt\n",
    "                print(f\"Parsing failed for {team_leader.position}: {e}\")\n",
    "                return {\n",
    "                    \"messages\": state[\"messages\"] + [\n",
    "                        HumanMessage(content=f\"Error: Parsing failed for {team_leader} - {e}\", name=team_leader.position),\n",
    "                        AIMessage(content=f\"Updated Prompt: {updated_prompt}\", name=\"Leader\")\n",
    "                    ],                    \n",
    "                    \"prompt\": state[\"prompt\"],\n",
    "                    \"next\": \"leader\",\n",
    "                }\n",
    "\n",
    "        workflow = StateGraph(TeamState)\n",
    "        for team_leader in self.team_leaders:\n",
    "            # Create a node for each team leader agent\n",
    "            # team_leader.construct_worker_graph()\n",
    "            node = functools.partial(team_node, team_leader=team_leader)\n",
    "            workflow.add_node(team_leader.position, node)\n",
    "        workflow.add_node(\"leader\", self.leader_decision)\n",
    "\n",
    "        members = [team_leader.position for team_leader in self.team_leaders]\n",
    "        for member in members:\n",
    "            # We want our workers to ALWAYS \"report back\" to the leader when done\n",
    "            workflow.add_edge(member, \"leader\")\n",
    "        # The leader populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(\"leader\", lambda x: x[\"next\"], conditional_map)\n",
    "        # Finally, add entrypoint\n",
    "        workflow.set_entry_point(\"leader\")\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def optimise_prompt(self):\n",
    "        \"\"\"\n",
    "        Optimises a prompt by invoking a graph of worker agents.\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=f\"{self.base_prompt}\", name=\"User\")],\n",
    "            \"prompt\": self.base_prompt,\n",
    "            \"next\": \"leader\",\n",
    "        }\n",
    "\n",
    "        # Construct the graph\n",
    "        graph = self.construct_team_graph()\n",
    "        # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        n = random.randint(0, 1000)\n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": str(n)},\n",
    "            \"recursion_limit\": 50,\n",
    "            }    \n",
    "\n",
    "        # Run the graph\n",
    "        for s in graph.stream(\n",
    "            initial_state,\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "            ):\n",
    "            if \"__end__\" not in s:\n",
    "                if len(s[\"messages\"]) > 2:\n",
    "                    s[\"messages\"][-2].pretty_print()\n",
    "                    s[\"messages\"][-1].pretty_print()\n",
    "                elif len(s[\"messages\"]) > 1:\n",
    "                    s[\"messages\"][-1].pretty_print()\n",
    "                continue\n",
    "\n",
    "        # if not os.path.exists(\"prompt_history_hierarchical.json\"):\n",
    "        #     with open(\"prompt_history_hierarchical.json\", \"w\") as f:\n",
    "        #         json.dump([], f)\n",
    "        \n",
    "        # with open(\"prompt_history_hierarchical.json\", \"r\") as f:\n",
    "        #     data = json.load(f)\n",
    "        #     data.append(self.prompt_history)\n",
    "            \n",
    "        # with open(\"prompt_history_hierarchical.json\", \"w\") as f:\n",
    "        #     json.dump(data, f, indent=4)\n",
    "                \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt design team members\n",
    "style_and_structure_principles = CorePrinciples([\n",
    "    \"Always structure prompts logically, for example COSTAR (Context, Objective, Steps, Task, Additional Information, Result)\",\n",
    "    \"Always use a style and tone in prompts that is appropriate for the task\",\n",
    "    \"Always consider the complexity of the task when designing prompts\",\n",
    "])\n",
    "style_and_structure_expert = WorkerAgent(\"Style and Structure Expert\", style_and_structure_principles)\n",
    "\n",
    "conciseness_and_clarity_principles = CorePrinciples([\n",
    "    \"Always write clear and concise prompts\",\n",
    "    \"Always use simple and direct language in prompts\",\n",
    "    \"Always consider the task criteria when formatting and structuring prompts\",\n",
    "])\n",
    "conciseness_and_clarity_expert = WorkerAgent(\"Conciseness and Clarity Expert\", conciseness_and_clarity_principles)\n",
    "\n",
    "contextual_relevance_principles = CorePrinciples([\n",
    "    \"Always provide context to help the model understand the task\",\n",
    "    \"Always consider the context in which prompts will be used\",\n",
    "    \"Always ensure personas and roles in prompts are optimal to the context of the task\",\n",
    "])\n",
    "contextual_relevance_expert = WorkerAgent(\"Contextual Relevance Expert\", contextual_relevance_principles)\n",
    "\n",
    "task_alignment_principles = CorePrinciples([\n",
    "    \"Always ensure that prompts align with the task criteria\",\n",
    "    \"Always tailor instructions to the task to guide the model\",\n",
    "])\n",
    "task_alignment_expert = WorkerAgent(\"Task Alignment Expert\", task_alignment_principles)\n",
    "\n",
    "example_demonstration_principal = CorePrinciples([\n",
    "    \"Always provide examples to help the model understand the task\",\n",
    "    \"Always ensure examples are relevant and clear\",\n",
    "    \"Always demonstrate the expected output of the model\",\n",
    "])\n",
    "example_demonstration_expert = WorkerAgent(\"Example Demonstration Expert\", example_demonstration_principal)\n",
    "\n",
    "avoiding_bias_principles = CorePrinciples([\n",
    "    \"Always avoid bias in prompts\",\n",
    "    \"Always consider the ethical implications of prompts\",\n",
    "])\n",
    "avoiding_bias_expert = WorkerAgent(\"Avoiding Bias Expert\", avoiding_bias_principles)\n",
    "\n",
    "incremental_prompting_principles = CorePrinciples([\n",
    "    \"Always provide clear step-by-step instructions to guide the model\",\n",
    "    \"Always consider the complexity of the task when providing instructions\",\n",
    "])\n",
    "incremental_prompting_expert = WorkerAgent(\"Incremental Prompting Expert\", incremental_prompting_principles)\n",
    "\n",
    "programming_logic_principles = CorePrinciples([\n",
    "    \"Only implement programming logic if you think it is beneficial for the task\",\n",
    "    \"Structure prompts logically, similar to programming logic\",\n",
    "    \"Implement programming logic including loops, conditionals, and functions and even pseudo-code\",\n",
    "])\n",
    "programming_logic_expert = WorkerAgent(\"Programming Logic Expert\", programming_logic_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain team members\n",
    "mathematics_principles = CorePrinciples([\n",
    "    \"Always follow good mathematical practices\",\n",
    "    \"Always use mathetical operators correctly\",\n",
    "    \"Always consider the mathematical principles relevant to the task\",\n",
    "])\n",
    "mathematician = WorkerAgent(\"Mathematician\", mathematics_principles)\n",
    "\n",
    "word_problem_solving_principles = CorePrinciples([\n",
    "    \"Always pay attention to the keywords in problems\",\n",
    "    \"Always approach problems systematically\",\n",
    "    \"Always consider multiple approaches to solving problems\",\n",
    "])\n",
    "word_problem_solver = WorkerAgent(\"Word Problem Solver\", word_problem_solving_principles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams and members:\n",
      "Lead Prompt Writer team:\n",
      "Position: Style and Structure Expert, Core Principles: \n",
      "- Always structure prompts logically, for example COSTAR (Context, Objective, Steps, Task, Additional Information, Result)\n",
      "- Always use a style and tone in prompts that is appropriate for the task\n",
      "- Always consider the complexity of the task when designing prompts\n",
      "Position: Conciseness and Clarity Expert, Core Principles: \n",
      "- Always write clear and concise prompts\n",
      "- Always use simple and direct language in prompts\n",
      "- Always consider the task criteria when formatting and structuring prompts\n",
      "Position: Contextual Relevance Expert, Core Principles: \n",
      "- Always provide context to help the model understand the task\n",
      "- Always consider the context in which prompts will be used\n",
      "- Always ensure personas and roles in prompts are optimal to the context of the task\n",
      "Position: Task Alignment Expert, Core Principles: \n",
      "- Always ensure that prompts align with the task criteria\n",
      "- Always tailor instructions to the task to guide the model\n",
      "Position: Example Demonstration Expert, Core Principles: \n",
      "- Always provide examples to help the model understand the task\n",
      "- Always ensure examples are relevant and clear\n",
      "- Always demonstrate the expected output of the model\n",
      "Position: Incremental Prompting Expert, Core Principles: \n",
      "- Always provide clear step-by-step instructions to guide the model\n",
      "- Always consider the complexity of the task when providing instructions\n",
      "Position: Programming Logic Expert, Core Principles: \n",
      "- Only implement programming logic if you think it is beneficial for the task\n",
      "- Structure prompts logically, similar to programming logic\n",
      "- Implement programming logic including loops, conditionals, and functions and even pseudo-code\n",
      "----\n",
      "Lead Mathematician team:\n",
      "Position: Mathematician, Core Principles: \n",
      "- Always follow good mathematical practices\n",
      "- Always use mathetical operators correctly\n",
      "- Always consider the mathematical principles relevant to the task\n",
      "Position: Word Problem Solver, Core Principles: \n",
      "- Always pay attention to the keywords in problems\n",
      "- Always approach problems systematically\n",
      "- Always consider multiple approaches to solving problems\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "base_prompt = \"{content}\\nPlease output your answer at the end as ##<your answer (arabic numerals)>.\"\n",
    "criteria = \"\"\"- The prompt correctly instructs an LLM to solve the simple maths problem provided.\n",
    "- The prompt correctly outputs the answer at the end as ##<answer> with no spaces or units.\"\"\"\n",
    "\n",
    "prompt_design_team = TeamLeaderAgent(\n",
    "    \"Lead Prompt Writer\",\n",
    "    CorePrinciples([\n",
    "        \"Always follow good prompt design practices\",\n",
    "        \"Always adhere to strict guidelines\",\n",
    "        \"Always listen to feedback if received\",\n",
    "    ]),\n",
    "    base_prompt,\n",
    "    criteria,\n",
    "    workforce=[\n",
    "        style_and_structure_expert,\n",
    "        conciseness_and_clarity_expert,\n",
    "        contextual_relevance_expert,\n",
    "        task_alignment_expert,\n",
    "        example_demonstration_expert,\n",
    "        incremental_prompting_expert,\n",
    "        programming_logic_expert,\n",
    "    ]\n",
    ")\n",
    "domain_team = TeamLeaderAgent(\n",
    "    \"Lead Mathematician\",\n",
    "    CorePrinciples([\n",
    "        \"Always follow good mathematical practices\",\n",
    "        \"Always adhere to strict guidelines\",\n",
    "        \"Always listen to feedback if received\",\n",
    "    ]),\n",
    "    base_prompt,\n",
    "    criteria,\n",
    "    workforce=[\n",
    "        mathematician,\n",
    "        word_problem_solver,\n",
    "    ]\n",
    ")\n",
    "\n",
    "leader_agent = LeaderAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    "    team_leaders=[prompt_design_team, domain_team]\n",
    ")\n",
    "\n",
    "# leader_agent.generate_teams()\n",
    "# print teams and members\n",
    "print(\"Teams and members:\")\n",
    "for team_leader in leader_agent.team_leaders:\n",
    "    print(f\"{team_leader.position} team:\")\n",
    "    for worker in team_leader.workforce:\n",
    "        print(f\"Position: {worker.position}, Core Principles: \\n{worker.core_principles}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approval results: [False, False]\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lead Prompt Writer\n",
      "\n",
      "Feedback: To improve the prompt, explicitly state the specific maths problem to be solved to avoid ambiguity. Rephrase the instructions to ensure clarity by stating: 'Solve the math problem and display the answer as ##<your answer> with no spaces or units at the end.' Emphasize the importance of solving the problem first, then formatting the answer correctly. Additionally, remind the LLM to double-check the answer format to maintain consistency.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Leader\n",
      "\n",
      "Updated Prompt: Solve the math problem and display the answer as ##<your answer> with no spaces or units at the end. Please double-check the answer format to maintain consistency. Your task is to make updates to the prompt above by implementing the feedback below. You must action the feedback keeping in mind your core principles. ##<your answer (arabic numerals)>.\n",
      "Approval results: [False, False]\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lead Prompt Writer\n",
      "\n",
      "Feedback: To improve the prompt, explicitly state the specific maths problem to be solved to avoid ambiguity. Rephrase the instructions to ensure clarity by stating: 'Solve the math problem and display the answer as ##<your answer> with no spaces or units at the end.' Emphasize the importance of solving the problem first, then formatting the answer correctly. Additionally, remind the LLM to double-check the answer format to maintain consistency.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Leader\n",
      "\n",
      "Updated Prompt: Solve the math problem and display the answer as ##<your answer> with no spaces or units at the end. Please double-check the answer format to maintain consistency. Your task is to make updates to the prompt above by implementing the feedback below. You must action the feedback keeping in mind your core principles. ##<your answer (arabic numerals)>.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lead Prompt Writer\n",
      "\n",
      "Feedback: The original prompt is clear but can be made more concise by removing redundant phrases and breaking instructions into shorter sentences. Ensure the prompt directs to solve the math problem and display the answer as ##<answer> with no spaces or units, using Arabic numerals. Adding an example, like 'For 2+2, the output should be ##4', can further clarify the format and eliminate ambiguity. Emphasize double-checking the format to maintain consistency.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Leader\n",
      "\n",
      "Updated Prompt: Solve the math problem and display the answer as ##<answer> with no spaces or units at the end. Use Arabic numerals. For example, for 2+2, the output should be ##4. Double-check the answer format to maintain consistency.\n",
      "Approval results: [True, True]\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lead Prompt Writer\n",
      "\n",
      "Feedback: The original prompt is clear but can be made more concise by removing redundant phrases and breaking instructions into shorter sentences. Ensure the prompt directs to solve the math problem and display the answer as ##<answer> with no spaces or units, using Arabic numerals. Adding an example, like 'For 2+2, the output should be ##4', can further clarify the format and eliminate ambiguity. Emphasize double-checking the format to maintain consistency.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Leader\n",
      "\n",
      "Updated Prompt: Solve the math problem and display the answer as ##<answer> with no spaces or units at the end. Use Arabic numerals. For example, for 2+2, the output should be ##4. Double-check the answer format to maintain consistency.\n"
     ]
    }
   ],
   "source": [
    "result = leader_agent.optimise_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Solve the math problem and display the answer as ##<answer> with no spaces or units at the end. Use Arabic numerals. For example, for 2+2, the output should be ##4. Double-check the answer format to maintain consistency.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
