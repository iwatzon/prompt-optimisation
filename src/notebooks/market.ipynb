{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidding Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Market Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptUpdate(BaseModel):\n",
    "    \"\"\"Review of the prompt\"\"\"\n",
    "    updated_prompt: str = Field(..., description=\"Updated prompt based on the review\")\n",
    "    justification: str = Field(..., description=\"Justification for the updates made\")\n",
    "\n",
    "class Bid(BaseModel):\n",
    "    \"\"\"Bid value for the prompt\"\"\"\n",
    "    position: str = Field(..., description=\"Position of the expert providing the bid\")\n",
    "    bid: int = Field(..., description=\"Bid value for the prompt\")\n",
    "\n",
    "class CorePrinciples:\n",
    "    def __init__(self, core_principles: List[str]):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "class ExpertAgent:\n",
    "    \"\"\"\n",
    "    Expert Agent class defining agents that provide feedback on prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, llm = ChatOpenAI(temperature=0.5, model=\"gpt-4o\")):\n",
    "        self.position = position\n",
    "        self.core_principles = core_principles\n",
    "        self.system_message = SystemMessage(content=f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\")\n",
    "        self.llm = llm\n",
    "\n",
    "    def bid(self, prompt: str, criteria: str) -> float:\n",
    "        \"\"\"\n",
    "        Bids on the prompt based on the expert's expertise.\n",
    "        \"\"\"\n",
    "        template = \"\"\"```{prompt}```\n",
    "\n",
    "Your task is to bid on the prompt above.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your bid.\n",
    "\n",
    "The bid must reflect how much you believe the prompt needs improving in light of your core principles.\n",
    "\n",
    "The bid must be an integer between 1 and 10. Below is a scale to guide your bid:\n",
    "- 1: The prompt is perfect in light of your core principles.\n",
    "- 2: The prompt is very good and does not require edits in light of your core principles.\n",
    "- 5: The prompt is okay but improvements are needed in light of your core principles.\n",
    "- 8: The prompt is poor and significant improvements are needed in light of your core principles.\n",
    "- 10: The prompt is very poor and wholesale changes are needed in light of your core principles.\n",
    "\n",
    "If you are unsure, be critical rather than lenient; it is better to overbid than underbid.\n",
    "\n",
    "Your bid process should be as follows:\n",
    "1. Read the prompt carefully as an expert {position}.\n",
    "2. Think carefully about how much you believe the prompt needs improving in light of your core principles.\n",
    "3. Critically bid on the prompt.\n",
    "4. Ensure that your bid is an integer between 1 and 10.\n",
    "5. Submit your bid.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output ONLY your positon ({position}) and bid value. DO NOT output any additional information.\n",
    "Example JSON output: {{\"position\": \"Position\", \"bid\": \"Bid value\"}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "        pydantic_parser = PydanticOutputParser(pydantic_object=Bid)\n",
    "        prompt_template = PromptTemplate(\n",
    "            system_message=self.system_message,\n",
    "            template=template,\n",
    "            input_variables=[\"position\", \"prompt\", \"history\", \"criteria\"],\n",
    "            partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt_template | self.llm | pydantic_parser\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                completion = chain.invoke({\"position\": self.position, \"prompt\": prompt, \"criteria\": criteria})\n",
    "                # Validate the output before returning\n",
    "                if completion.position and completion.bid:\n",
    "                    return completion\n",
    "                # else:\n",
    "                #     print(\"Validation failed: Missing required fields in completion\")\n",
    "                #     print(\"Raw output:\", completion)\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred:\", e)\n",
    "                continue\n",
    "        else:\n",
    "            raise Exception(\"Failed to parse output after 3 attempts\")\n",
    "        \n",
    "    def self_reflection_graph(self, criteria) -> MessageGraph:\n",
    "        \"\"\"\n",
    "        Constructs a graph for self-reflection and improvement of prompts.\n",
    "        \"\"\"\n",
    "\n",
    "        def generation_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "            pydantic_parser = PydanticOutputParser(pydantic_object=PromptUpdate)\n",
    "            prompt_text = \"\"\"Your task is to improve the user's prompt in light of your core principles.\n",
    "If the user provides feedback and recommendations for the prompt, respond with a revised version of your previous attempts.\n",
    "DO NOT directly insert the raw feedback into the prompt. You must action the feedback, keeping in mind your core principles.\n",
    "You must provide justification, in less than 100 words, for any changes you make to the prompt.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "You will be penalized if your updated prompt does not meet the success criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Read the prompt carefully as an expert {position}.\n",
    "2. Think carefully about how you can implement the user's feedback.\n",
    "3. Implement the changes to the prompt.\n",
    "4. Ensure that your changes adhere to the strict guidelines. DO NOT REMOVE PLACEHOLDERS.\n",
    "5. Provide justification for the changes you made.\n",
    "6. Ensure that your justification is less than 100 words.\n",
    "7. Submit your updated prompt and justification.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Output ONLY the updated prompt and justification. DO NOT output any additional information.\n",
    "Example JSON output: {{\"updated_prompt\": \"Updated prompt\", \"justification\": \"Justification\"}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\n",
    "\"\"\"\n",
    "            prompt = HumanMessagePromptTemplate(\n",
    "                prompt = PromptTemplate(\n",
    "                    system_message=self.system_message,\n",
    "                    template=prompt_text,\n",
    "                    input_variables=[\"history\", \"criteria\", \"position\"],\n",
    "                    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    "                )\n",
    "            )\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    'system', \n",
    "                    prompt,\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                ]\n",
    "            )\n",
    "            for _ in range(3):\n",
    "                try:\n",
    "                    generate = prompt | self.llm | pydantic_parser\n",
    "                    res = generate.invoke({\"messages\": state, \"criteria\": criteria, \"position\": self.position})\n",
    "                    updated_prompt_message = AIMessage(content=res.updated_prompt)\n",
    "                    justification_message = AIMessage(content=res.justification)\n",
    "                    new_state = state + [updated_prompt_message, justification_message]\n",
    "                    # print(f\"New state: {new_state}\")\n",
    "                    return new_state\n",
    "                except Exception as e:\n",
    "                    print(\"Exception occurred:\", e)\n",
    "                    continue\n",
    "            else:\n",
    "                raise Exception(\"Failed to parse output after 3 attempts\")\n",
    "            \n",
    "        def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "            reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\n",
    "                        \"system\",\n",
    "                        f\"\"\"{self.system_message.content} \n",
    "Your task is to think outside the box and provide feedback on the user's prompt with creative recommendations on how to improve it in light of your core principles:\n",
    "Your feedback must be less than 100 words so think carefully about the most critical aspects of the prompt that need improvement.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your decision.\n",
    "\n",
    "Below are strict guidelines that you MUST follow when providing feedback and recommendations:\n",
    "- DO NOT suggest modifying existing restrictions.\n",
    "- DO NOT suggest modifying or removing negations.\n",
    "- DO NOT suggest adding, modifying or removing placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt carefully as an expert {self.position}.\n",
    "2. Identify the most critical aspects of the prompt that need improvement. \n",
    "3. Think outside the box to provide creative feedback with recommendations on how to improve the prompt in light of your core principles.\n",
    "4. Ensure that your feedback is less than 100 words.\n",
    "5. Ensure that your feedback follows the strict guidelines provided above. DO NOT SUGGEST REMOVING PLACEHOLDERS.\n",
    "6. Submit your feedback.\n",
    "\"\"\",\n",
    "                    ),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                ]\n",
    "            )\n",
    "            reflect = reflection_prompt | self.llm\n",
    "            res = reflect.invoke({\"messages\": messages})\n",
    "            # We treat the output of this as human feedback for the generator\n",
    "            return HumanMessage(content=res.content)\n",
    "\n",
    "        builder = MessageGraph()\n",
    "        builder.add_node(\"generate\", generation_node)\n",
    "        builder.add_node(\"reflect\", reflection_node)\n",
    "        builder.set_entry_point(\"generate\")\n",
    "\n",
    "        def should_continue(state: List[BaseMessage]):\n",
    "            if len(state) > 6:\n",
    "                # End after 3 iterations\n",
    "                return END\n",
    "            return \"reflect\"\n",
    "\n",
    "        builder.add_conditional_edges(\"generate\", should_continue)\n",
    "        builder.add_edge(\"reflect\", \"generate\")\n",
    "        graph = builder.compile()\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    def update_prompt(self, prompt: str, criteria: str) -> str:\n",
    "        \"\"\"\n",
    "        Uses self_reflection_graph to iteratively act on feedback and update prompt\n",
    "        \"\"\"\n",
    "        graph = self.self_reflection_graph(criteria)\n",
    "        inputs = [HumanMessage(content=prompt)]\n",
    "        updated_prompt = graph.invoke(inputs)\n",
    "        return updated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts(BaseModel):\n",
    "    \"\"\"Details of experts generated by the leader agent.\"\"\"\n",
    "    positions: List[str] = Field(..., description=\"List of positions of the experts\")    \n",
    "    core_principles: List[List[str]] = Field(..., description=\"Core principles of the workers in the workforce\")\n",
    "\n",
    "class MarketPlace:\n",
    "    \"\"\"\n",
    "    Moderator Agent class defining an agent that builds the experts and moderates the bidding process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_prompt: str, criteria: str = None, experts: List[ExpertAgent] = None):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.criteria = criteria\n",
    "        self.experts = experts\n",
    "        self.iteration = 0\n",
    "\n",
    "    def bidding_process(self, state: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Execution of bidding process to select the next expert to process the prompt.\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        # Collect bids from all experts\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(expert.bid, state['prompt'], self.criteria) for expert in self.experts]\n",
    "            bids = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        # Sort bids in descending order\n",
    "        bids.sort(key=lambda x: x.bid, reverse=True)\n",
    "        for i in range(len(bids)):\n",
    "            print(\"Position: {position}, Bid: {bid}\".format(position=bids[i].position, bid=bids[i].bid))\n",
    "        # If a tie occurs, randomly select a expert from the tied experts, else select the expert with the highest bid\n",
    "        max_bid = max(bids, key=lambda x: x.bid).bid\n",
    "        tied_experts = [expert for expert in bids if expert.bid == max_bid]\n",
    "        if len(tied_experts) > 1:\n",
    "            next_expert = tied_experts[random.randint(0, len(tied_experts) - 1)]\n",
    "        else:\n",
    "            next_expert = tied_experts[0]\n",
    "        # Update the state with the next expert to process\n",
    "        if max_bid <= 2.0 or self.iteration >= 10:\n",
    "            return {\n",
    "                \"next\": \"FINISH\", \n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=f\"All bids lower than 2\", name=\"Moderator\")]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"next\": next_expert.position, \n",
    "                \"messages\": state[\"messages\"] + [AIMessage(content=f\"Highest Bidder: {next_expert.position}, Bid: {next_expert.bid}\", name=\"Moderator\")]\n",
    "            }\n",
    "        \n",
    "    def construct_expert_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of expert agents based on their roles and functions.\n",
    "        \"\"\"\n",
    "\n",
    "        def agent_node(state, agent, position):\n",
    "            try:\n",
    "                result = agent.update_prompt(state[\"prompt\"], self.criteria)\n",
    "                # print(f\"Result: {result}\")\n",
    "                updated_prompt = result[-2].content\n",
    "                justification = result[-1].content\n",
    "                return {\n",
    "                    \"messages\": state[\"messages\"] + [HumanMessage(content=f\"Updated Prompt: {updated_prompt}, Justification: {justification}\", name=position)],\n",
    "                    \"prompt\": updated_prompt,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                # Log the error and return to moderator with the most recent prompt\n",
    "                print(f\"Parsing failed for {position}: {e}\")\n",
    "                return {\n",
    "                    \"messages\": state[\"messages\"] + [HumanMessage(content=f\"Error: Parsing failed for {position} - {e}\", name=position)],\n",
    "                    \"prompt\": state[\"prompt\"],\n",
    "                    \"next\": \"moderator\"\n",
    "                }\n",
    "        \n",
    "        # The agent state is the input to each node in the graph\n",
    "        class AgentState(TypedDict):\n",
    "            # The annotation tells the graph that new messages will always be added to the current states\n",
    "            messages: Sequence[BaseMessage]\n",
    "            next: str\n",
    "            prompt: str\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        for expert in self.experts:\n",
    "            # Create a node for each expert agent\n",
    "            node = functools.partial(agent_node, agent=expert, position=expert.position)\n",
    "            workflow.add_node(expert.position, node)\n",
    "        workflow.add_node(\"moderator\", self.bidding_process)\n",
    "\n",
    "        members = [expert.position for expert in self.experts]\n",
    "        for member in members:\n",
    "            # We want our experts to ALWAYS \"report back\" to the moderator when done\n",
    "            workflow.add_edge(member, \"moderator\")\n",
    "        # The moderator populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(\"moderator\", lambda x: x[\"next\"], conditional_map)\n",
    "        # Finally, add entrypoint\n",
    "        workflow.set_entry_point(\"moderator\")\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def optimise_prompt(self):\n",
    "        \"\"\"\n",
    "        Optimises a prompt by invoking a graph of expert agents.\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=f\"Base Prompt: {self.base_prompt}\", name=\"Leader\")],\n",
    "            \"prompt\": self.base_prompt,\n",
    "            \"next\": \"moderator\"\n",
    "        }\n",
    "\n",
    "        # Construct the graph\n",
    "        graph = self.construct_expert_graph()\n",
    "        # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        n = random.randint(1, 100)\n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": \"1\"},\n",
    "            \"recursion_limit\": 50,\n",
    "            }    \n",
    "\n",
    "        # Run the graph\n",
    "        for s in graph.stream(\n",
    "            initial_state,\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "            ):\n",
    "            if \"__end__\" not in s:\n",
    "                try:\n",
    "                    s[\"messages\"][-1].pretty_print()\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "        # if not os.path.exists(\"prompt_history_market.json\"):\n",
    "        #     with open(\"prompt_history_market.json\", \"w\") as f:\n",
    "        #         json.dump([], f)\n",
    "        \n",
    "        # with open(\"prompt_history_market.json\", \"r\") as f:\n",
    "        #     data = json.load(f)\n",
    "        #     data.append(self.prompt_history)\n",
    "            \n",
    "        # with open(\"prompt_history_market.json\", \"w\") as f:\n",
    "        #     json.dump(data, f, indent=4)\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatAnthropic(temperature=1.0, model=\"claude-3-5-sonnet-20240620\")\n",
    "llm = ChatOpenAI(temperature=1.0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt design team members\n",
    "style_and_structure_principles = CorePrinciples([\n",
    "    \"Always structure prompts logically, for example COSTAR (Context, Objective, Steps, Task, Additional Information, Result)\",\n",
    "    \"Always use a style and tone in prompts that is appropriate for the task\",\n",
    "    \"Always consider the complexity of the task when designing prompts\",\n",
    "])\n",
    "style_and_structure_expert = ExpertAgent(\"Style and Structure Expert\", style_and_structure_principles, llm)\n",
    "\n",
    "conciseness_and_clarity_principles = CorePrinciples([\n",
    "    \"Always write clear and concise prompts\",\n",
    "    \"Always use simple and direct language in prompts\",\n",
    "    \"Always consider the task criteria when formatting and structuring prompts\",\n",
    "])\n",
    "conciseness_and_clarity_expert = ExpertAgent(\"Conciseness and Clarity Expert\", conciseness_and_clarity_principles, llm)\n",
    "\n",
    "contextual_relevance_principles = CorePrinciples([\n",
    "    \"Always provide context to help the model understand the task\",\n",
    "    \"Always consider the context in which prompts will be used\",\n",
    "    \"Always ensure personas and roles in prompts are optimal to the context of the task\",\n",
    "])\n",
    "contextual_relevance_expert = ExpertAgent(\"Contextual Relevance Expert\", contextual_relevance_principles, llm)\n",
    "\n",
    "task_alignment_principles = CorePrinciples([\n",
    "    \"Always ensure that prompts align with the task criteria\",\n",
    "    \"Always tailor instructions to the task to guide the model\",\n",
    "])\n",
    "task_alignment_expert = ExpertAgent(\"Task Alignment Expert\", task_alignment_principles, llm)\n",
    "\n",
    "example_demonstration_principal = CorePrinciples([\n",
    "    \"Always provide examples to help the model understand the task\",\n",
    "    \"Always ensure examples are relevant and clear\",\n",
    "    \"Always demonstrate the expected output of the model\",\n",
    "])\n",
    "example_demonstration_expert = ExpertAgent(\"Example Demonstration Expert\", example_demonstration_principal, llm)\n",
    "\n",
    "avoiding_bias_principles = CorePrinciples([\n",
    "    \"Always avoid bias in prompts\",\n",
    "    \"Always consider the ethical implications of prompts\",\n",
    "])\n",
    "avoiding_bias_expert = ExpertAgent(\"Avoiding Bias Expert\", avoiding_bias_principles, llm)\n",
    "\n",
    "incremental_prompting_principles = CorePrinciples([\n",
    "    \"Always provide clear step-by-step instructions to guide the model\",\n",
    "    \"Always consider the complexity of the task when providing instructions\",\n",
    "])\n",
    "incremental_prompting_expert = ExpertAgent(\"Incremental Prompting Expert\", incremental_prompting_principles, llm)\n",
    "\n",
    "programming_logic_principles = CorePrinciples([\n",
    "    \"Only implement programming logic if you think it is beneficial for the task\",\n",
    "    \"Structure prompts logically, similar to programming logic\",\n",
    "    \"Implement programming logic including loops, conditionals, and functions and even pseudo-code\",\n",
    "])\n",
    "programming_logic_expert = ExpertAgent(\"Programming Logic Expert\", programming_logic_principles, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain team members\n",
    "mathematics_principles = CorePrinciples([\n",
    "    \"Always follow good mathematical practices\",\n",
    "    \"Always use mathetical operators correctly\",\n",
    "    \"Always consider the mathematical principles relevant to the task\",\n",
    "])\n",
    "mathematician = ExpertAgent(\"Mathematician\", mathematics_principles, llm)\n",
    "\n",
    "word_problem_solving_principles = CorePrinciples([\n",
    "    \"Always pay attention to the keywords in problems\",\n",
    "    \"Always approach problems systematically\",\n",
    "    \"Always consider multiple approaches to solving problems\",\n",
    "])\n",
    "word_problem_solver = ExpertAgent(\"Word Problem Solver\", word_problem_solving_principles, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:  Style and Structure Expert\n",
      "Core Principles:  - Always structure prompts logically, for example COSTAR (Context, Objective, Steps, Task, Additional Information, Result)\n",
      "- Always use a style and tone in prompts that is appropriate for the task\n",
      "- Always consider the complexity of the task when designing prompts\n",
      "Position:  Conciseness and Clarity Expert\n",
      "Core Principles:  - Always write clear and concise prompts\n",
      "- Always use simple and direct language in prompts\n",
      "- Always consider the task criteria when formatting and structuring prompts\n",
      "Position:  Contextual Relevance Expert\n",
      "Core Principles:  - Always provide context to help the model understand the task\n",
      "- Always consider the context in which prompts will be used\n",
      "- Always ensure personas and roles in prompts are optimal to the context of the task\n",
      "Position:  Task Alignment Expert\n",
      "Core Principles:  - Always ensure that prompts align with the task criteria\n",
      "- Always tailor instructions to the task to guide the model\n",
      "Position:  Example Demonstration Expert\n",
      "Core Principles:  - Always provide examples to help the model understand the task\n",
      "- Always ensure examples are relevant and clear\n",
      "- Always demonstrate the expected output of the model\n",
      "Position:  Incremental Prompting Expert\n",
      "Core Principles:  - Always provide clear step-by-step instructions to guide the model\n",
      "- Always consider the complexity of the task when providing instructions\n",
      "Position:  Mathematician\n",
      "Core Principles:  - Always follow good mathematical practices\n",
      "- Always use mathetical operators correctly\n",
      "- Always consider the mathematical principles relevant to the task\n",
      "Position:  Word Problem Solver\n",
      "Core Principles:  - Always pay attention to the keywords in problems\n",
      "- Always approach problems systematically\n",
      "- Always consider multiple approaches to solving problems\n"
     ]
    }
   ],
   "source": [
    "base_prompt = \"{content}\\nPlease output your answer at the end as ##<your answer (arabic numerals)>.\"\n",
    "criteria = \"\"\"- Correctly instructs the LLM to solve a maths question. \n",
    "- Correctly outputs the answer at the end as ##<answer> with no spaces or units.\"\"\"\n",
    "\n",
    "leader_agent = MarketPlace(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    "    experts=[\n",
    "        style_and_structure_expert,\n",
    "        conciseness_and_clarity_expert,\n",
    "        contextual_relevance_expert,\n",
    "        task_alignment_expert,\n",
    "        example_demonstration_expert,\n",
    "        incremental_prompting_expert,\n",
    "        mathematician,\n",
    "        word_problem_solver,\n",
    "    ],\n",
    "    )\n",
    "for expert in leader_agent.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nCore Principles: \", expert.core_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Leader\n",
      "\n",
      "Base Prompt: {content}\n",
      "Please output your answer at the end as ##<your answer (arabic numerals)>.\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 3\n",
      "Position: Example Demonstration Expert, Bid: 3\n",
      "Position: Task Alignment Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Mathematician, Bid: 5\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Mathematician\n",
      "\n",
      "Updated Prompt: Solve the problem presented in {content}. Output the final answer at the end as ##<answer>., Justification: The updated prompt now provides clarity and precision by explicitly instructing the LLM to solve the problem and to format the final answer correctly. This ensures that the LLM understands the task and outputs the answer in the desired format, adhering to the success criteria.\n",
      "Position: Word Problem Solver, Bid: 6\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 4\n",
      "Position: Style and Structure Expert, Bid: 3\n",
      "Position: Contextual Relevance Expert, Bid: 2\n",
      "Position: Mathematician, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Word Problem Solver, Bid: 6\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Word Problem Solver\n",
      "\n",
      "Updated Prompt: Systematically solve the problem presented in {content}. Ensure the correct output by providing the final answer at the end as ##<answer> with no spaces or units., Justification: The updated prompt now emphasizes both a systematic approach to solving the problem and ensures adherence to the specified output format, meeting the user's feedback and success criteria.\n",
      "Position: Incremental Prompting Expert, Bid: 6\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 4\n",
      "Position: Mathematician, Bid: 3\n",
      "Position: Conciseness and Clarity Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Incremental Prompting Expert, Bid: 6\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Incremental Prompting Expert\n",
      "\n",
      "Updated Prompt: 1. Identify given values and unknowns in {content}. 2. Derive the formula or method needed. 3. Substitute values and solve step-by-step. 4. Verify the answer. 5. Provide the final answer as ##<answer> with no spaces or units., Justification: The updated prompt now includes step-by-step instructions for clarity. This structured approach ensures the LLM correctly identifies values, applies the formula, solves, and verifies the answer before outputting it as ##<answer> with no spaces or units.\n",
      "Position: Incremental Prompting Expert, Bid: 6\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 4\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Incremental Prompting Expert, Bid: 6\n",
      "Exception occurred: Failed to parse PromptUpdate from completion 1. Got: 1 validation error for PromptUpdate\n",
      "__root__\n",
      "  PromptUpdate expected dict not int (type=type_error)\n",
      "Exception occurred: Failed to parse PromptUpdate from completion 1. Got: 1 validation error for PromptUpdate\n",
      "__root__\n",
      "  PromptUpdate expected dict not int (type=type_error)\n",
      "Exception occurred: Failed to parse PromptUpdate from completion 1. Got: 1 validation error for PromptUpdate\n",
      "__root__\n",
      "  PromptUpdate expected dict not int (type=type_error)\n",
      "Parsing failed for Incremental Prompting Expert: Failed to parse output after 3 attempts\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Incremental Prompting Expert\n",
      "\n",
      "Error: Parsing failed for Incremental Prompting Expert - Failed to parse output after 3 attempts\n",
      "Position: Conciseness and Clarity Expert, Bid: 6\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Conciseness and Clarity Expert, Bid: 6\n",
      "Exception occurred: Failed to parse PromptUpdate from completion 1. Got: 1 validation error for PromptUpdate\n",
      "__root__\n",
      "  PromptUpdate expected dict not int (type=type_error)\n",
      "Exception occurred: Failed to parse PromptUpdate from completion 1. Got: 1 validation error for PromptUpdate\n",
      "__root__\n",
      "  PromptUpdate expected dict not int (type=type_error)\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Conciseness and Clarity Expert\n",
      "\n",
      "Updated Prompt: 1. Identify values and unknowns in {content}, then derive and apply the necessary formula. 2. Substitute values and solve step-by-step. 3. Verify the answer for accuracy. 4. Provide the final answer as ##<answer> with no spaces or units., Justification: Combined steps 1 and 2 for smoother flow, improving clarity while reducing redundancy. Emphasized the final answer format to ensure precise output. This maintains clear, concise instructions, meeting all criteria and guidelines.\n",
      "Position: Incremental Prompting Expert, Bid: 6\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Incremental Prompting Expert, Bid: 6\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Incremental Prompting Expert\n",
      "\n",
      "Updated Prompt: 1. Identify all given values and unknowns in {content}. 2. Determine the necessary formula to use. 3. Substitute the identified values into the formula. 4. Solve the equation step-by-step. 5. Verify the obtained answer for accuracy. 6. Provide the final answer formatted as ##<answer> with no spaces or units., Justification: The revised prompt explicitly states each step, enhancing clarity and guiding the process methodically. This incremental breakdown aids accuracy and ensures correct formatting, adhering to the success criteria and guidelines.\n",
      "Position: Contextual Relevance Expert, Bid: 6\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 4\n",
      "Position: Word Problem Solver, Bid: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Contextual Relevance Expert, Bid: 6\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Contextual Relevance Expert\n",
      "\n",
      "Updated Prompt: 1. Identify all given values and unknowns in {content}. 2. Choose the appropriate formula. 3. Substitute the identified values into the formula. 4. Solve the equation step-by-step. 5. Verify the calculated answer. 6. Output the final answer formatted as ##<answer> with no spaces or units., Justification: The enhanced prompt improves clarity and flow without altering the core instructions. It uses simpler language like 'Choose the appropriate formula' and 'Output the final answer' for smooth comprehension. This ensures the LLM effectively understands and follows each step, adhering to the criteria for a clear and precise solution.\n",
      "Position: Contextual Relevance Expert, Bid: 6\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Contextual Relevance Expert, Bid: 6\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Contextual Relevance Expert\n",
      "\n",
      "Updated Prompt: To solve the math problem presented in {content}, please follow these steps: 1. Identify all given values and unknowns in {content}. 2. Choose the appropriate formula. 3. Substitute the identified values into the formula. 4. Solve the equation step-by-step. 5. Verify the calculated answer. 6. Output the final answer formatted as ##<answer> with no spaces or units., Justification: The introduction ensures that the context for solving the math problem is clear, providing a more structured approach while adhering to all given guidelines.\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 4\n",
      "Position: Contextual Relevance Expert, Bid: 3\n",
      "Position: Mathematician, Bid: 3\n",
      "Position: Example Demonstration Expert, Bid: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "Highest Bidder: Incremental Prompting Expert, Bid: 5\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Incremental Prompting Expert\n",
      "\n",
      "Updated Prompt: To solve the math problem presented in {content}, please follow these steps: 1. List all given values and identify unknowns. 2. Choose the appropriate formula. 3. Substitute the identified values into the formula. 4. Solve the equation step-by-step. 5. Verify the calculated answer. 6. Output the verified final answer as ##<answer> with no spaces or units., Justification: The updates improve clarity and directness without changing the essential content. The first step is simplified by instructing to 'List all given values and identify unknowns.' The final step is clarified to 'Output the verified final answer as ##<answer> with no spaces or units,' ensuring the instructions are straightforward.\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Moderator\n",
      "\n",
      "All bids lower than 2\n"
     ]
    }
   ],
   "source": [
    "result = leader_agent.optimise_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To solve the math problem presented in {content}, please follow these steps: 1. List all given values and identify unknowns. 2. Choose the appropriate formula. 3. Substitute the identified values into the formula. 4. Solve the equation step-by-step. 5. Verify the calculated answer. 6. Output the verified final answer as ##<answer> with no spaces or units.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:  Linguistic Analyst\n",
      "Role:  The Linguistic Analyst is responsible for ensuring that the prompt is clear, concise, and unambiguous. They will analyze the language used in the prompt to ensure it is easily understandable and free of any potential biases.\n",
      "Function:  The Linguistic Analyst will review and refine the wording of the prompt to ensure clarity and neutrality, preventing any language that could lead to misinterpretation or bias in classification.\n",
      "\n",
      "Position:  Machine Learning Engineer\n",
      "Role:  The Machine Learning Engineer is responsible for optimizing the prompt for machine learning models. They will ensure that the prompt is structured in a way that maximizes the model's ability to accurately classify the sentence as positive or negative.\n",
      "Function:  The Machine Learning Engineer will test and iterate on the prompt structure to enhance the performance of classification algorithms, ensuring that the prompt facilitates accurate and reliable model outputs.\n",
      "\n",
      "Position:  User Experience Researcher\n",
      "Role:  The User Experience Researcher is responsible for evaluating the prompt from the end-user's perspective. They will ensure that the prompt is user-friendly and intuitive, and that it provides a positive experience for users interacting with the classification task.\n",
      "Function:  The User Experience Researcher will conduct user testing and gather feedback to refine the prompt, ensuring it is straightforward and easy to use, thereby improving overall user satisfaction and engagement with the task.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert \u001b[38;5;129;01min\u001b[39;00m moderator_agent_1\u001b[38;5;241m.\u001b[39mexperts:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRole: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFunction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mfunction \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m moderator_agent_2 \u001b[38;5;241m=\u001b[39m \u001b[43mModeratorAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert \u001b[38;5;129;01min\u001b[39;00m moderator_agent_2\u001b[38;5;241m.\u001b[39mexperts:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRole: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFunction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mfunction \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 159\u001b[0m, in \u001b[0;36mModeratorAgent.__init__\u001b[0;34m(self, base_prompt, additional_info, temp, model, experts)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_info \u001b[38;5;241m=\u001b[39m additional_info\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m    156\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemp,\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    158\u001b[0m )\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts \u001b[38;5;241m=\u001b[39m experts \u001b[38;5;28;01mif\u001b[39;00m experts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_experts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_evolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n",
      "Cell \u001b[0;32mIn[7], line 198\u001b[0m, in \u001b[0;36mModeratorAgent.generate_experts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_info\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mpositions \u001b[38;5;129;01mand\u001b[39;00m output\u001b[38;5;241m.\u001b[39mroles \u001b[38;5;129;01mand\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfunctions:\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2399\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2399\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:442\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    437\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    441\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:357\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_base_client.py:882\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    879\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_auth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_prompt = \"Classify the sentence as positive or negative: {content}\"\n",
    "criteria = \"This is a classification task with only two classes: positive and negative. The prompt should generalize.\"\n",
    "\n",
    "moderator_agent_1 = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    ")\n",
    "for expert in moderator_agent_1.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nRole: \", expert.role + \"\\nFunction: \", expert.function + \"\\n\")\n",
    "\n",
    "moderator_agent_2 = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    ")\n",
    "for expert in moderator_agent_2.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nRole: \", expert.role + \"\\nFunction: \", expert.function + \"\\n\")\n",
    "\n",
    "moderator_agent_3 = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    ")\n",
    "for expert in moderator_agent_3.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nRole: \", expert.role + \"\\nFunction: \", expert.function + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_prompt': 'Classify the sentence as positive or negative: {content}', 'prompt_evolution': []}\n",
      "{'base_prompt': 'Classify the sentence as positive or negative: {content}', 'prompt_evolution': [{'review': \"The prompt should explicitly instruct the LLM to consider sentiment polarity, specify that the classes are only 'positive' or 'negative,' and ensure clarity in classification without introducing unnecessary complexities or ambiguities.\", 'updated_prompt': \"Classify the sentiment of the given sentence as either 'positive' or 'negative': {content}. Ensure the classification is based strictly on sentiment polarity.\"}]}\n",
      "{'base_prompt': 'Classify the sentence as positive or negative: {content}', 'prompt_evolution': [{'review': 'The prompt needs clarity on the definition of positive and negative sentiment. Additionally, it should encourage consideration of context. Explicitly requiring the LLM to focus only on the sentiment will improve performance.', 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Consider the tone and context to make an accurate classification.'}, {'review': 'The original prompt is clear but could benefit from emphasizing neutrality and clarifying tone and context consideration.', 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Ensure to carefully evaluate the tone and context for a precise classification. Focus on neutrality in your assessment.'}, {'review': \"The prompt lacks clarity on when to declare neutrality and doesn't specify how to handle ambiguous cases. Also, the context evaluation instruction could be strengthened.\", 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Carefully evaluate the tone and context for precise classification. Avoid neutrality in your assessment. If the sentiment is ambiguous, choose the more likely classification.'}, {'review': 'The prompt is detailed but could be more concise and direct. Removing redundancy and emphasizing direct instructions will improve clarity and focus.', 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Evaluate tone and context carefully for precise classification. Avoid neutrality. If sentiment is ambiguous, choose the more likely classification.'}]}\n",
      "Classify the sentence as positive or negative: {content}\n",
      "----\n",
      "Classify the sentiment of the given sentence as either 'positive' or 'negative': {content}. Ensure the classification is based strictly on sentiment polarity.\n",
      "----\n",
      "Classify the sentiment of the following sentence as either positive or negative: {content}. Evaluate tone and context carefully for precise classification. Avoid neutrality. If sentiment is ambiguous, choose the more likely classification.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Assuming leader_agent is already defined and initialized\n",
    "def run_optimisation(agent: LeaderAgent):\n",
    "    return agent.optimise_prompt()\n",
    "\n",
    "# Run 3 concurrent instances\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(run_optimisation, agent) for agent in [leader_agent_1, leader_agent_2, leader_agent_3]]\n",
    "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"----\")\n",
    "\n",
    "class PromptMerge(BaseModel):\n",
    "    \"\"\"Merged prompt based on the best parts of each prompt.\"\"\"\n",
    "    final_prompt: str = Field(description=\"Result of merging prompts\")\n",
    "\n",
    "# OpenAI Agent to pull togther best parts of each result\n",
    "def merge_results(results):\n",
    "    \"\"\"\n",
    "    Agent to merge best parts of each prompt\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=1.0,\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    system_message = \"\"\"You are an experienced AI prompt engineer. Your role is to combine good prompts to create great prompts!\n",
    "You have in-depth knowledge of large language models and prompt engineering best practices. Use knowledge at all times to guide your thinking.\"\"\"\n",
    "\n",
    "    template = \"\"\"Your task is to merge the best parts of the prompts below to create a better prompt.\n",
    "Carefully consider the strengths of each prompt and how they can be combined effectively whilst maintaining clarity and relevance.\n",
    "You will be penalized if the prompt is repetitive, lacks clarity or is incoherent.\n",
    "Aspects of the prompts to consider:\n",
    "- Conciseness and clarity\n",
    "- Contextual relevance\n",
    "- Task alignment\n",
    "- Example Demonstrations\n",
    "- Avoiding bias\n",
    "Consider aspects of good prompts beyond those listed above.\n",
    "Placeholders are notated using curly braces. You must not remove placeholders or add additional placeholders.\n",
    "I repeat, you must not remove placeholders or add additional placeholders.\n",
    "Do not make assumptions on what the placeholders represent.\n",
    "\n",
    "Prompts: {results}\n",
    "\n",
    "Return only the next worker to process or 'FINISH' in JSON format below:\n",
    "\n",
    "{{\n",
    "    \"final_prompt\": \"Result of merging prompts\",\n",
    "}}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "    pydantic_parser = PydanticOutputParser(pydantic_object=PromptMerge)\n",
    "    prompt_template = PromptTemplate(\n",
    "        system_message=system_message,\n",
    "        template=template,\n",
    "        input_variables=[\"results\"],\n",
    "        partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt_template | llm | pydantic_parser\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            output = chain.invoke({\"results\": results})\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "            continue\n",
    "    return output.final_prompt\n",
    "\n",
    "final_result = merge_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following sentence as either positive or negative: {content}. Evaluate tone and context carefully for precise classification. Ensure the classification is based strictly on sentiment polarity. Avoid neutrality. If sentiment is ambiguous, choose the more likely classification.\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
