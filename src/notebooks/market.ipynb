{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidding Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Market Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptUpdate(BaseModel):\n",
    "    \"\"\"Review of the prompt\"\"\"\n",
    "    updated_prompt: str = Field(description=\"Updated prompt based on the review\")\n",
    "    justification: str = Field(description=\"Justification for the changes made\")\n",
    "\n",
    "class Bid(BaseModel):\n",
    "    \"\"\"Bid value for the prompt\"\"\"\n",
    "    position: str = Field(description=\"Position of the expert providing the bid\")\n",
    "    bid: int = Field(description=\"Bid value for the prompt\")\n",
    "\n",
    "class CorePrinciples:\n",
    "    def __init__(self, core_principles: List[str]):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "class ExpertAgent:\n",
    "    \"\"\"\n",
    "    Expert Agent class defining agents that provide feedback on prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, temp: float = 1.0, model: str = \"gpt-4o\"):\n",
    "        self.position = position\n",
    "        self.core_principles = core_principles\n",
    "        self.system_message = SystemMessage(content=f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\")\n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=temp,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "    def bid(self, prompt: str, criteria: str, history) -> float:\n",
    "        \"\"\"\n",
    "        Bids on the prompt based on the expert's expertise.\n",
    "        \"\"\"\n",
    "        template = \"\"\"```{prompt}```\n",
    "\n",
    "Your task is to bid on the prompt above.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your bid.\n",
    "\n",
    "The bid must reflect how much you believe the prompt needs improving in light of your core principles.\n",
    "\n",
    "The bid must be an integer between 1 and 10. Below is a scale to guide your bid:\n",
    "- 1: The prompt is perfect in light of your core principles.\n",
    "- 2: The prompt is excellent in light of your core principles.\n",
    "- 5: The prompt is okay but improvements are needed in light of your core principles.\n",
    "- 8: The prompt is not good enough and significant improvements are needed in light of your core principles.\n",
    "- 10: The prompt is terrible and wholesale changes are needed in light of your core principles.\n",
    "\n",
    "If you are unsure, be critical rather than lenient; it is better to overbid than underbid.\n",
    "\n",
    "Your bid process should be as follows:\n",
    "1. Read the prompt carefully as an expert {position}.\n",
    "2. Think carefully about how much you believe the prompt needs improving in light of your core principles.\n",
    "3. Critically bid on the prompt.\n",
    "4. Ensure that your bid is an integer between 1 and 10.\n",
    "5. Submit your bid.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Return only your position and bid. Example output:\n",
    "\n",
    "{{\n",
    "    \"position\": \"{position}\",\n",
    "    \"bid\": \"Bid value\",\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "        pydantic_parser = PydanticOutputParser(pydantic_object=Bid)\n",
    "        prompt_template = PromptTemplate(\n",
    "            system_message=self.system_message,\n",
    "            template=template,\n",
    "            input_variables=[\"position\", \"prompt\", \"history\", \"criteria\"],\n",
    "            partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "        )\n",
    "        chain = prompt_template | self.llm | pydantic_parser\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                completion = chain.invoke({\"position\": self.position, \"prompt\": prompt, \"history\": history, \"criteria\": criteria})\n",
    "                # Validate the output before returning\n",
    "                if completion.position and completion.bid:\n",
    "                    return completion\n",
    "                # else:\n",
    "                #     print(\"Validation failed: Missing required fields in completion\")\n",
    "                #     print(\"Raw output:\", completion)\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred:\", e)\n",
    "                continue\n",
    "        else:\n",
    "            raise Exception(\"Failed to parse output after 3 attempts\")\n",
    "        \n",
    "    def self_reflection_graph(self, criteria, history) -> MessageGraph:\n",
    "        \"\"\"\n",
    "        Constructs a graph for self-reflection and improvement of prompts.\n",
    "        \"\"\"\n",
    "\n",
    "        def generation_node(state: Sequence[BaseMessage], history) -> List[BaseMessage]:\n",
    "            pydantic_parser = PydanticOutputParser(pydantic_object=PromptUpdate)\n",
    "            prompt_text = \"\"\"Your task is to improve the user's prompt in light of your core principles.\n",
    "If the user provides feedback and recommendations for the prompt, respond with a revised version of your previous attempts.\n",
    "DO NOT directly insert the raw feedback into the prompt. You must action the feedback, keeping in mind your core principles.\n",
    "You must provide justification, in less than 100 words, for any changes you make to the prompt.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "You will be penalized if your updated prompt does not meet the success criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Read the prompt carefully as an expert {position}.\n",
    "2. Think carefully about how you can implement the user's feedback.\n",
    "3. Implement the changes to the prompt.\n",
    "4. Ensure that your changes adhere to the strict guidelines. DO NOT REMOVE PLACEHOLDERS.\n",
    "5. Provide justification for the changes you made.\n",
    "6. Ensure that your justification is less than 100 words.\n",
    "7. Submit your updated prompt and justification.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Return the updated prompt along with your justification. Example output:\n",
    "\n",
    "{{\n",
    "    \"updated_prompt\": \"Updated prompt based on the review\",\n",
    "    \"justification\": \"Justification for the changes made\"\n",
    "}}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\n",
    "\"\"\"\n",
    "            prompt = HumanMessagePromptTemplate(\n",
    "                prompt = PromptTemplate(\n",
    "                    system_message=self.system_message,\n",
    "                    template=prompt_text,\n",
    "                    input_variables=[\"history\", \"criteria\", \"position\"],\n",
    "                    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    "                )\n",
    "            )\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    prompt,\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                ]\n",
    "            )\n",
    "            for _ in range(3):\n",
    "                try:\n",
    "                    generate = prompt | self.llm | pydantic_parser\n",
    "                    res = generate.invoke({\"messages\": state, \"history\": history, \"criteria\": criteria, \"position\": self.position})\n",
    "                    updated_prompt_message = AIMessage(content=res.updated_prompt)\n",
    "                    justification_message = AIMessage(content=res.justification)\n",
    "                    new_state = state + [updated_prompt_message, justification_message]\n",
    "                    # print(f\"New state: {new_state}\")\n",
    "                    return new_state\n",
    "                except Exception as e:\n",
    "                    print(\"Exception occurred:\", e)\n",
    "                    continue\n",
    "            else:\n",
    "                raise Exception(\"Failed to parse output after 3 attempts\")\n",
    "            \n",
    "        def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "            reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\n",
    "                        \"system\",\n",
    "                        f\"\"\"{self.system_message.content} \n",
    "Your task is to think outside the box and provide feedback on the user's prompt with creative recommendations on how to improve it in light of your core principles:\n",
    "Your feedback must be less than 100 words so think carefully about the most critical aspects of the prompt that need improvement.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "Use this information to inform your decision.\n",
    "\n",
    "Below are strict guidelines that you MUST follow when providing feedback and recommendations:\n",
    "- DO NOT suggest modifying existing restrictions.\n",
    "- DO NOT suggest modifying or removing negations.\n",
    "- DO NOT suggest adding, modifying or removing placeholders denoted by curly braces. IT IS ESSENTIAL PLACEHOLDERS REMAIN UNCHANGED.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt carefully as an expert {self.position}.\n",
    "2. Identify the most critical aspects of the prompt that need improvement. \n",
    "3. Think outside the box to provide creative feedback with recommendations on how to improve the prompt in light of your core principles.\n",
    "4. Ensure that your feedback is less than 100 words.\n",
    "5. Ensure that your feedback follows the strict guidelines provided above. DO NOT SUGGEST REMOVING PLACEHOLDERS.\n",
    "6. Submit your feedback.\n",
    "\"\"\",\n",
    "                    ),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                ]\n",
    "            )\n",
    "            reflect = reflection_prompt | self.llm\n",
    "            res = reflect.invoke({\"messages\": messages})\n",
    "            # We treat the output of this as human feedback for the generator\n",
    "            return HumanMessage(content=res.content)\n",
    "\n",
    "        builder = MessageGraph()\n",
    "        generation_node = functools.partial(generation_node, history=history)\n",
    "        builder.add_node(\"generate\", generation_node)\n",
    "        builder.add_node(\"reflect\", reflection_node)\n",
    "        builder.set_entry_point(\"generate\")\n",
    "\n",
    "        def should_continue(state: List[BaseMessage]):\n",
    "            if len(state) > 6:\n",
    "                # End after 3 iterations\n",
    "                return END\n",
    "            return \"reflect\"\n",
    "\n",
    "        builder.add_conditional_edges(\"generate\", should_continue)\n",
    "        builder.add_edge(\"reflect\", \"generate\")\n",
    "        graph = builder.compile()\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    def update_prompt(self, prompt: str, criteria: str, history) -> str:\n",
    "        \"\"\"\n",
    "        Uses self_reflection_graph to iteratively act on feedback and update prompt\n",
    "        \"\"\"\n",
    "        graph = self.self_reflection_graph(criteria, history)\n",
    "        inputs = [HumanMessage(content=prompt)]\n",
    "        updated_prompt = graph.invoke(inputs)\n",
    "        return updated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experts(BaseModel):\n",
    "    \"\"\"Details of experts generated by the leader agent.\"\"\"\n",
    "    positions: List[str] = Field(description=\"List of positions of the experts\")    \n",
    "    core_principles: List[List[str]] = Field(description=\"Core principles of the workers in the workforce\")\n",
    "\n",
    "class ModeratorAgent:\n",
    "    \"\"\"\n",
    "    Moderator Agent class defining an agent that builds the experts and moderates the bidding process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_prompt: str, criteria: str = None, experts: List[ExpertAgent] = None):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.criteria = criteria\n",
    "        self.experts = experts\n",
    "        self.iteration = 0\n",
    "\n",
    "    def bidding_process(self, state: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Execution of bidding process to select the next expert to process the prompt.\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        # Collect bids from all experts\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(expert.bid, state['prompt'], self.criteria, state[\"messages\"]) for expert in self.experts]\n",
    "            bids = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        # Sort bids in descending order\n",
    "        bids.sort(key=lambda x: x.bid, reverse=True)\n",
    "        for i in range(len(bids)):\n",
    "            print(\"Position: {position}, Bid: {bid}\".format(position=bids[i].position, bid=bids[i].bid))\n",
    "        # If a tie occurs, randomly select a expert from the tied experts, else select the expert with the highest bid\n",
    "        max_bid = max(bids, key=lambda x: x.bid).bid\n",
    "        tied_experts = [expert for expert in bids if expert.bid == max_bid]\n",
    "        if len(tied_experts) > 1:\n",
    "            next_expert = tied_experts[random.randint(0, len(tied_experts) - 1)]\n",
    "        else:\n",
    "            next_expert = tied_experts[0]\n",
    "        # Update the state with the next expert to process\n",
    "        if max_bid <= 2.0 or self.iteration >= 10:\n",
    "            return {\"next\": \"FINISH\", \"prompt\": state[\"prompt\"], \"messages\": state[\"messages\"]}\n",
    "        else:\n",
    "            return {\"next\": next_expert.position, \"prompt\": state[\"prompt\"], \"messages\": state[\"messages\"]}\n",
    "        \n",
    "    def construct_expert_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of expert agents based on their roles and functions.\n",
    "        \"\"\"\n",
    "\n",
    "        def agent_node(state, agent, position):\n",
    "            try:\n",
    "                result = agent.update_prompt(state[\"prompt\"], self.criteria, state[\"messages\"])\n",
    "                print(f\"Result: {result}\")\n",
    "                updated_prompt = result[-2].content\n",
    "                justification = result[-1].content\n",
    "                return {\n",
    "                    \"messages\": state[\"messages\"] + [AIMessage(content=f\"Updated Prompt: {updated_prompt}, Justification: {justification}\", name=position)],\n",
    "                    \"prompt\": updated_prompt,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                # Log the error and return to moderator with the most recent prompt\n",
    "                print(f\"Parsing failed for {position}: {e}\")\n",
    "                return {\n",
    "                    \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error: Parsing failed for {position} - {e}\", name=position)],\n",
    "                    \"prompt\": state[\"prompt\"],\n",
    "                    \"next\": \"moderator\"\n",
    "                }\n",
    "        \n",
    "        # The agent state is the input to each node in the graph\n",
    "        class AgentState(TypedDict):\n",
    "            # The annotation tells the graph that new messages will always be added to the current states\n",
    "            messages: Sequence[BaseMessage]\n",
    "            next: str\n",
    "            prompt: str\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        for expert in self.experts:\n",
    "            # Create a node for each expert agent\n",
    "            node = functools.partial(agent_node, agent=expert, position=expert.position)\n",
    "            workflow.add_node(expert.position, node)\n",
    "        workflow.add_node(\"moderator\", self.bidding_process)\n",
    "\n",
    "        members = [expert.position for expert in self.experts]\n",
    "        for member in members:\n",
    "            # We want our experts to ALWAYS \"report back\" to the moderator when done\n",
    "            workflow.add_edge(member, \"moderator\")\n",
    "        # The moderator populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(\"moderator\", lambda x: x[\"next\"], conditional_map)\n",
    "        # Finally, add entrypoint\n",
    "        workflow.set_entry_point(\"moderator\")\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def optimise_prompt(self):\n",
    "        \"\"\"\n",
    "        Optimises a prompt by invoking a graph of expert agents.\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=f\"{self.base_prompt}\", name=\"User\")],\n",
    "            \"prompt\": self.base_prompt,\n",
    "            \"next\": \"moderator\"\n",
    "        }\n",
    "\n",
    "        # Construct the graph\n",
    "        graph = self.construct_expert_graph()\n",
    "        # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": \"1\"},\n",
    "            \"recursion_limit\": 50,\n",
    "            }    \n",
    "\n",
    "        # Run the graph\n",
    "        for s in graph.stream(\n",
    "            initial_state,\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "            ):\n",
    "            if \"__end__\" not in s:\n",
    "                s[\"messages\"][-1].pretty_print()\n",
    "                continue\n",
    "\n",
    "        # if not os.path.exists(\"prompt_history_market.json\"):\n",
    "        #     with open(\"prompt_history_market.json\", \"w\") as f:\n",
    "        #         json.dump([], f)\n",
    "        \n",
    "        # with open(\"prompt_history_market.json\", \"r\") as f:\n",
    "        #     data = json.load(f)\n",
    "        #     data.append(self.prompt_history)\n",
    "            \n",
    "        # with open(\"prompt_history_market.json\", \"w\") as f:\n",
    "        #     json.dump(data, f, indent=4)\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iwatson/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Prompt design team members\n",
    "style_and_structure_principles = CorePrinciples([\n",
    "    \"Always structure prompts logically, for example COSTAR (Context, Objective, Steps, Task, Additional Information, Result)\",\n",
    "    \"Always use a style and tone in prompts that is appropriate for the task\",\n",
    "    \"Always consider the complexity of the task when designing prompts\",\n",
    "])\n",
    "style_and_structure_expert = ExpertAgent(\"Style and Structure Expert\", style_and_structure_principles)\n",
    "\n",
    "conciseness_and_clarity_principles = CorePrinciples([\n",
    "    \"Always write clear and concise prompts\",\n",
    "    \"Always use simple and direct language in prompts\",\n",
    "    \"Always consider the task criteria when formatting and structuring prompts\",\n",
    "])\n",
    "conciseness_and_clarity_expert = ExpertAgent(\"Conciseness and Clarity Expert\", conciseness_and_clarity_principles)\n",
    "\n",
    "contextual_relevance_principles = CorePrinciples([\n",
    "    \"Always provide context to help the model understand the task\",\n",
    "    \"Always consider the context in which prompts will be used\",\n",
    "    \"Always ensure personas and roles in prompts are optimal to the context of the task\",\n",
    "])\n",
    "contextual_relevance_expert = ExpertAgent(\"Contextual Relevance Expert\", contextual_relevance_principles)\n",
    "\n",
    "task_alignment_principles = CorePrinciples([\n",
    "    \"Always ensure that prompts align with the task criteria\",\n",
    "    \"Always tailor instructions to the task to guide the model\",\n",
    "])\n",
    "task_alignment_expert = ExpertAgent(\"Task Alignment Expert\", task_alignment_principles)\n",
    "\n",
    "example_demonstration_principal = CorePrinciples([\n",
    "    \"Always provide examples to help the model understand the task\",\n",
    "    \"Always ensure examples are relevant and clear\",\n",
    "    \"Always demonstrate the expected output of the model\",\n",
    "])\n",
    "example_demonstration_expert = ExpertAgent(\"Example Demonstration Expert\", example_demonstration_principal)\n",
    "\n",
    "avoiding_bias_principles = CorePrinciples([\n",
    "    \"Always avoid bias in prompts\",\n",
    "    \"Always consider the ethical implications of prompts\",\n",
    "])\n",
    "avoiding_bias_expert = ExpertAgent(\"Avoiding Bias Expert\", avoiding_bias_principles)\n",
    "\n",
    "incremental_prompting_principles = CorePrinciples([\n",
    "    \"Always provide clear step-by-step instructions to guide the model\",\n",
    "    \"Always consider the complexity of the task when providing instructions\",\n",
    "])\n",
    "incremental_prompting_expert = ExpertAgent(\"Incremental Prompting Expert\", incremental_prompting_principles)\n",
    "\n",
    "programming_logic_principles = CorePrinciples([\n",
    "    \"Only implement programming logic if you think it is beneficial for the task\",\n",
    "    \"Structure prompts logically, similar to programming logic\",\n",
    "    \"Implement programming logic including loops, conditionals, and functions and even pseudo-code\",\n",
    "])\n",
    "programming_logic_expert = ExpertAgent(\"Programming Logic Expert\", programming_logic_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain team members\n",
    "mathematics_principles = CorePrinciples([\n",
    "    \"Always follow good mathematical practices\",\n",
    "    \"Always use mathetical operators correctly\",\n",
    "    \"Always consider the mathematical principles relevant to the task\",\n",
    "])\n",
    "mathematician = ExpertAgent(\"Mathematician\", mathematics_principles)\n",
    "\n",
    "word_problem_solving_principles = CorePrinciples([\n",
    "    \"Always pay attention to the keywords in problems\",\n",
    "    \"Always approach problems systematically\",\n",
    "    \"Always consider multiple approaches to solving problems\",\n",
    "])\n",
    "word_problem_solver = ExpertAgent(\"Word Problem Solver\", word_problem_solving_principles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:  Style and Structure Expert\n",
      "Core Principles:  - Always structure prompts logically, for example COSTAR (Context, Objective, Steps, Task, Additional Information, Result)\n",
      "- Always use a style and tone in prompts that is appropriate for the task\n",
      "- Always consider the complexity of the task when designing prompts\n",
      "Position:  Conciseness and Clarity Expert\n",
      "Core Principles:  - Always write clear and concise prompts\n",
      "- Always use simple and direct language in prompts\n",
      "- Always consider the task criteria when formatting and structuring prompts\n",
      "Position:  Contextual Relevance Expert\n",
      "Core Principles:  - Always provide context to help the model understand the task\n",
      "- Always consider the context in which prompts will be used\n",
      "- Always ensure personas and roles in prompts are optimal to the context of the task\n",
      "Position:  Task Alignment Expert\n",
      "Core Principles:  - Always ensure that prompts align with the task criteria\n",
      "- Always tailor instructions to the task to guide the model\n",
      "Position:  Example Demonstration Expert\n",
      "Core Principles:  - Always provide examples to help the model understand the task\n",
      "- Always ensure examples are relevant and clear\n",
      "- Always demonstrate the expected output of the model\n",
      "Position:  Incremental Prompting Expert\n",
      "Core Principles:  - Always provide clear step-by-step instructions to guide the model\n",
      "- Always consider the complexity of the task when providing instructions\n",
      "Position:  Programming Logic Expert\n",
      "Core Principles:  - Only implement programming logic if you think it is beneficial for the task\n",
      "- Structure prompts logically, similar to programming logic\n",
      "- Implement programming logic including loops, conditionals, and functions and even pseudo-code\n",
      "Position:  Mathematician\n",
      "Core Principles:  - Always follow good mathematical practices\n",
      "- Always use mathetical operators correctly\n",
      "- Always consider the mathematical principles relevant to the task\n",
      "Position:  Word Problem Solver\n",
      "Core Principles:  - Always pay attention to the keywords in problems\n",
      "- Always approach problems systematically\n",
      "- Always consider multiple approaches to solving problems\n"
     ]
    }
   ],
   "source": [
    "base_prompt = \"`{content}`\\nPlease output your answer at the end as ##<your answer (arabic numerals)>.\"\n",
    "criteria = \"\"\"- The prompt correctly instructs the LLM to solve a maths problem. \n",
    "- The prompt correctly outputs the answer at the end as ##<answer> with no spaces or units.\"\"\"\n",
    "\n",
    "leader_agent = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    "    experts=[\n",
    "        style_and_structure_expert,\n",
    "        conciseness_and_clarity_expert,\n",
    "        contextual_relevance_expert,\n",
    "        task_alignment_expert,\n",
    "        example_demonstration_expert,\n",
    "        incremental_prompting_expert,\n",
    "        programming_logic_expert,\n",
    "        mathematician,\n",
    "        word_problem_solver,\n",
    "    ])\n",
    "for expert in leader_agent.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nCore Principles: \", expert.core_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: User\n",
      "\n",
      "`{content}`\n",
      "Please output your answer at the end as ##<your answer (arabic numerals)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: Style and Structure Expert, Bid: 8\n",
      "Position: Task Alignment Expert, Bid: 7\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 3\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: User\n",
      "\n",
      "`{content}`\n",
      "Please output your answer at the end as ##<your answer (arabic numerals)>.\n",
      "Result: [HumanMessage(content='`{content}`\\nPlease output your answer at the end as ##<your answer (arabic numerals)>.', id='2a8b24e3-6f1d-4d03-b96e-c7c90bd44c7d'), AIMessage(content='Using the provided {content}, please solve the maths problem. Output your answer at the end as ##<answer> with no spaces or units.', id='4043b2cd-941d-4fd8-983b-c7eb475ac0e3'), AIMessage(content='I made the prompt more explicit by specifying that the answer should be solved using the provided content and emphasized that the answer should be formatted as ##<answer> without any spaces or units.', id='3c32b51f-cd24-4da9-b2f9-c11f24021ad7'), HumanMessage(content='1. The context or task for solving the math problem is not clearly stated in the prompt.\\n2. The directive to solve the math problem is too brief and lacks logical structure.\\n3. The tone could be a little more formal to ensure clarity.\\n\\n**Recommendation:**\\n\\n`Using the provided {content}, solve the math problem.` \\n`Ensure your calculations are correct.`\\n`Finally, format your answer as ##<answer> with no spaces or units.`', id='b57c9c43-ea6c-41c5-aeba-0b1a571ca84a'), AIMessage(content='Using the provided {content}, solve the math problem. Ensure your calculations are correct. Finally, format your answer as ##<answer> with no spaces or units.', id='cdc55847-ce48-40db-82fb-e474a00c3b74'), AIMessage(content=\"I followed the user's recommendation to provide clear instructions for solving the math problem. This updated prompt includes a directive for correctness and maintains a formal tone, ensuring clarity. The prompt adheres to the guidelines by keeping the placeholders unchanged and specifying the correct answer format.\", id='a7eeda09-a2c8-4060-9766-ee35ea7bacca'), HumanMessage(content='Here\\'s concise feedback to improve the prompt:\\n\\n1. Add context to clarify the task.\\n2. Enhance logical flow for understanding.\\n3. Maintain a formal tone for clarity.\\n\\n**Recommended Prompt:**\\n\\n\"Using the provided {content}, solve the math problem step-by-step. Ensure all calculations are accurate. Finally, format your answer as ##<answer> with no spaces or units.\"\\n\\nThis structure improves clarity, logical progression, and formality, ensuring the task is clear and concise.', id='41a3f34c-c927-48f6-b4e0-211a16371203'), AIMessage(content='Using the provided {content}, solve the math problem step-by-step. Ensure all calculations are accurate. Finally, format your answer as ##<answer> with no spaces or units.', id='f0f75a18-c819-48fb-884e-d668ba0a5ff9'), AIMessage(content='The revised prompt adds context by specifying that the math problem should be solved step-by-step, enhancing clarity and logical flow. Additionally, the instruction to ensure all calculations are accurate adds precision and the formal tone improves overall clarity, aligning with the feedback while adhering to the strict guidelines.', id='9064b982-d437-4494-b2ef-9cef134adfe5')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Style and Structure Expert\n",
      "\n",
      "Updated Prompt: Using the provided {content}, solve the math problem step-by-step. Ensure all calculations are accurate. Finally, format your answer as ##<answer> with no spaces or units., Justification: The revised prompt adds context by specifying that the math problem should be solved step-by-step, enhancing clarity and logical flow. Additionally, the instruction to ensure all calculations are accurate adds precision and the formal tone improves overall clarity, aligning with the feedback while adhering to the strict guidelines.\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 4\n",
      "Position: Incremental Prompting Expert, Bid: 3\n",
      "Position: Contextual Relevance Expert, Bid: 3\n",
      "Position: Example Demonstration Expert, Bid: 2\n",
      "Position: Word Problem Solver, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Style and Structure Expert\n",
      "\n",
      "Updated Prompt: Using the provided {content}, solve the math problem step-by-step. Ensure all calculations are accurate. Finally, format your answer as ##<answer> with no spaces or units., Justification: The revised prompt adds context by specifying that the math problem should be solved step-by-step, enhancing clarity and logical flow. Additionally, the instruction to ensure all calculations are accurate adds precision and the formal tone improves overall clarity, aligning with the feedback while adhering to the strict guidelines.\n",
      "Result: [HumanMessage(content='Using the provided {content}, solve the math problem step-by-step. Ensure all calculations are accurate. Finally, format your answer as ##<answer> with no spaces or units.', id='483f491a-0ef4-4cbb-a7d0-adcb5c110356'), AIMessage(content='Using the provided {content}, solve the math problem step-by-step. Ensure all calculations are accurate. Finally, format your answer as ##<answer> with no spaces or units.', id='c6d1005d-4bb1-4c1c-9c47-e942e33c6fd8'), AIMessage(content='The original prompt already met the success criteria and adhered to the strict guidelines. It correctly instructed the LLM to solve a math problem step-by-step and format the answer as required. Therefore, no changes were necessary.', id='3113f83f-f8b2-4b7b-9115-055989dc7d97'), HumanMessage(content='The prompt is clear but can be made more structured:\\n\\n1. Define input and expected output clearly.\\n2. Add a reminder about output formatting.\\n\\nConsider this structure:\\n\\n1. Read {content}. \\n2. Solve math step-by-step ensuring accuracy.\\n3. Output final answer formatted as ##<answer>.\\n\\nThis structure logically mirrors programming-style instructions for clarity and accuracy.', id='b8d96144-de8e-4166-b5ee-bbf3643c7d6a'), AIMessage(content='1. Read the provided {content}. \\n2. Solve the math problem step-by-step ensuring all calculations are accurate. \\n3. Output the final answer formatted as ##<answer> with no spaces or units.', id='f6eee7d0-e601-4250-8024-ea0f7c07824d'), AIMessage(content='The prompt has been made more structured by breaking down the instructions into clear, numbered steps. This helps in logically mirroring programming-style instructions, ensuring clarity and accuracy. Additionally, a specific reminder has been added about the required output formatting.', id='da2a9859-2cd7-4a77-a553-5e07cfc2ffdf'), HumanMessage(content=\"Your revised structure adds clarity and better follows programming logic. Here's how to integrate:\\n\\n1. Read the provided {content}.\\n2. Solve the math problem step-by-step, ensuring accuracy.\\n3. Format and output the final answer as ##<answer> with no spaces or units.\\n\\nThis logical sequence ensures clear instructions and accurate output formatting.\", id='6c72db25-fd0f-4931-8266-464a80ce3bdf'), AIMessage(content='1. Read the provided {content}.\\n2. Solve the math problem step-by-step, ensuring accuracy.\\n3. Format and output the final answer as ##<answer> with no spaces or units.', id='199b20b4-9f3e-46ca-832a-f2e574186f2f'), AIMessage(content='The revised structure adds clarity and follows a logical sequence similar to programming instructions. This ensures clear understanding of steps and accurate output formatting.', id='c842294a-a626-4bd7-95e9-c1c35c61e6f4')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Programming Logic Expert\n",
      "\n",
      "Updated Prompt: 1. Read the provided {content}.\n",
      "2. Solve the math problem step-by-step, ensuring accuracy.\n",
      "3. Format and output the final answer as ##<answer> with no spaces or units., Justification: The revised structure adds clarity and follows a logical sequence similar to programming instructions. This ensures clear understanding of steps and accurate output formatting.\n",
      "Position: Conciseness and Clarity Expert, Bid: 8\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 4\n",
      "Position: Example Demonstration Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Programming Logic Expert\n",
      "\n",
      "Updated Prompt: 1. Read the provided {content}.\n",
      "2. Solve the math problem step-by-step, ensuring accuracy.\n",
      "3. Format and output the final answer as ##<answer> with no spaces or units., Justification: The revised structure adds clarity and follows a logical sequence similar to programming instructions. This ensures clear understanding of steps and accurate output formatting.\n",
      "Result: [HumanMessage(content='1. Read the provided {content}.\\n2. Solve the math problem step-by-step, ensuring accuracy.\\n3. Format and output the final answer as ##<answer> with no spaces or units.', id='6e8380d5-b363-4f65-b62c-00918a3e6527'), AIMessage(content='1. Read the provided {content}. 2. Solve the math problem step-by-step, ensuring accuracy. 3. Format and output the final answer as ##<answer> with no spaces or units.', id='565d6c6d-d8d3-4b2a-a3e0-c3557d013921'), AIMessage(content='The original prompt already satisfies the given success criteria and strictly follows the guidelines. No modifications were needed, as it clearly instructs the LLM to solve a math problem and correctly format the final answer.', id='edf0c037-c9ef-4846-92a3-ea8551cdedd8'), HumanMessage(content=\"Your prompt is clear but can be more concise. Here's my updated suggestion: \\n\\n1. Read the {content}.\\n2. Solve the math problem step-by-step.\\n3. Output final answer as ##<answer> with no spaces or units.\", id='a6f2a7e4-9b9f-44b6-9c6d-331f64865ef4'), AIMessage(content='1. Read the {content}. 2. Solve the math problem step-by-step. 3. Output final answer as ##<answer> with no spaces or units.', id='6ea20482-aa61-465b-8d81-f501f0f9823e'), AIMessage(content='The updated suggestion maintains clarity while being more concise. It eliminates unnecessary words without altering the instructions or constraints, ensuring the task remains clear and correctly formatted.', id='61552fa8-527e-4c7c-9664-900aa2fbdf15'), HumanMessage(content='Your suggestion improves clarity and conciseness. It retains the necessary instructions and formatting requirements while making the language more direct. Great job!', id='03d6fb03-68f6-438e-89f0-97d65efa236a'), AIMessage(content='1. Read the {content}. 2. Solve the math problem step-by-step. 3. Output final answer as ##<answer> with no spaces or units.', id='1fe4e1fa-b777-4e7a-9bfe-41d847bfec0a'), AIMessage(content='The updated prompt improves clarity and conciseness, making the instructions more direct while retaining necessary details and formatting requirements. This ensures that the task is understood easily and executed correctly.', id='e6577dd0-4caf-48e3-af31-e0c3ae557e0b')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Conciseness and Clarity Expert\n",
      "\n",
      "Updated Prompt: 1. Read the {content}. 2. Solve the math problem step-by-step. 3. Output final answer as ##<answer> with no spaces or units., Justification: The updated prompt improves clarity and conciseness, making the instructions more direct while retaining necessary details and formatting requirements. This ensures that the task is understood easily and executed correctly.\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 2\n",
      "Position: Incremental Prompting Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Conciseness and Clarity Expert\n",
      "\n",
      "Updated Prompt: 1. Read the {content}. 2. Solve the math problem step-by-step. 3. Output final answer as ##<answer> with no spaces or units., Justification: The updated prompt improves clarity and conciseness, making the instructions more direct while retaining necessary details and formatting requirements. This ensures that the task is understood easily and executed correctly.\n",
      "Result: [HumanMessage(content='1. Read the {content}. 2. Solve the math problem step-by-step. 3. Output final answer as ##<answer> with no spaces or units.', id='c63a15e3-27f7-4351-82fd-1b480e5652ab'), AIMessage(content='1. Read the {content}. 2. Solve the math problem step-by-step showing all calculations clearly. 3. Output final answer as ##<answer> with no spaces or units.', id='9c311913-fc86-48ee-8793-9742dec3d938'), AIMessage(content=\"I added 'showing all calculations clearly' to ensure detailed steps are provided during the solution process, which aligns with the user's feedback to solve the math problem step-by-step. The rest of the prompt remains unchanged to respect the placeholder requirements and success criteria.\", id='003e547e-6b13-4f82-abf9-7388d161e53f'), HumanMessage(content='Consider emphasizing keywords and potential alternative solutions. Also, specify showing intermediate steps in \"step-by-step.\"\\n\\nUpdated prompt: 1. Read the {content} carefully focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer as ##<answer> with no spaces or units.', id='5976407b-6420-4694-a4af-8c51a55d6be8'), AIMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer as ##<answer> with no spaces or units.', id='0d9f4fc4-abd1-4146-9570-143a7aaf2656'), AIMessage(content=\"Included 'carefully focusing on keywords' to emphasize critical terms. Added 'showing all intermediate steps' to ensure clarity in the step-by-step solution process. Included 'considering multiple approaches' to explore alternative methods, providing a more comprehensive understanding.\", id='0b89bd0c-1173-4e77-b1bb-295a462ca991'), HumanMessage(content=\"The updated prompt effectively emphasizes keywords, specifies showing intermediate steps, and considers multiple approaches. This ensures a precise and detailed response, aligning with the success criteria and user's feedback.\", id='c544d6c9-efb6-4f02-ab90-9fdbb2c21bd8'), AIMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer as ##<answer> with no spaces or units.', id='ce851e32-583a-4822-93c6-406d0b2da2bf'), AIMessage(content=\"The updated prompt effectively emphasizes keywords, specifies showing intermediate steps, and considers multiple approaches. This ensures a precise and detailed response, aligning with the success criteria and user's feedback.\", id='f0c98d1a-0026-4134-b2b7-2cce8b477729')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Word Problem Solver\n",
      "\n",
      "Updated Prompt: 1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer as ##<answer> with no spaces or units., Justification: The updated prompt effectively emphasizes keywords, specifies showing intermediate steps, and considers multiple approaches. This ensures a precise and detailed response, aligning with the success criteria and user's feedback.\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 4\n",
      "Position: Mathematician, Bid: 3\n",
      "Position: Conciseness and Clarity Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Word Problem Solver\n",
      "\n",
      "Updated Prompt: 1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer as ##<answer> with no spaces or units., Justification: The updated prompt effectively emphasizes keywords, specifies showing intermediate steps, and considers multiple approaches. This ensures a precise and detailed response, aligning with the success criteria and user's feedback.\n",
      "Result: [HumanMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer as ##<answer> with no spaces or units.', id='8525a502-9bd8-4254-b8c0-8b5f4689b3ec'), AIMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Ensure the final answer is formatted as ##<answer> with no spaces or units.', id='7a1f6a6b-31fe-42ff-b14e-dc4c2c9d5627'), AIMessage(content='The original prompt needed a slight clarifying modification to explicitly emphasize the requirement for the final formatting of the answer. This ensures that the output strictly adheres to the specified format.', id='6ebf22f4-fdc7-4e70-975c-f34aaf548b70'), HumanMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units.\\n\\nYour prompt is nearly perfect. Adding \"strictly\" in step 3 emphasizes the importance of the required format, ensuring precision in the model\\'s output.', id='429e2c9d-41e0-4bf5-b0c1-1431d5ce3771'), AIMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units.', id='f2086290-19c7-4f2e-a3f7-460108472e0d'), AIMessage(content=\"Adding the word 'strictly' in step 3 emphasizes the importance of adhering to the exact format required. This ensures that the output precisely matches the specified formatting criteria, reducing the likelihood of errors.\", id='fe8a50a9-3e64-4d01-b060-bec50c7927d0'), HumanMessage(content='Adding \"strictly\" in step 3 underscores the importance of the format, ensuring accuracy in the model\\'s output. This minor adjustment enhances clarity without altering the core instructions.', id='6e82d5e2-7d9a-472c-a7c5-13a25b1ea5ed'), AIMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units.', id='3215fb17-d89a-4cf2-a947-1c54be5126c7'), AIMessage(content=\"Adding the word 'strictly' in step 3 underscores the importance of the required format, ensuring precision and accuracy in the model's output. This minor adjustment enhances clarity without altering the core instructions.\", id='33f2334d-e4c9-4c4b-9e1b-28b1e13eec1d')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Example Demonstration Expert\n",
      "\n",
      "Updated Prompt: 1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units., Justification: Adding the word 'strictly' in step 3 underscores the importance of the required format, ensuring precision and accuracy in the model's output. This minor adjustment enhances clarity without altering the core instructions.\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 3\n",
      "Position: Programming Logic Expert, Bid: 3\n",
      "Position: Example Demonstration Expert, Bid: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Example Demonstration Expert\n",
      "\n",
      "Updated Prompt: 1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units., Justification: Adding the word 'strictly' in step 3 underscores the importance of the required format, ensuring precision and accuracy in the model's output. This minor adjustment enhances clarity without altering the core instructions.\n",
      "Result: [HumanMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units.', id='f414ce10-85e1-4d77-95c0-d7563bd2499d'), AIMessage(content='1. Read the {content} carefully, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps and considering multiple approaches if applicable. 3. Output the final answer strictly as ##<answer> with no spaces or units.', id='3a5e1520-48e5-4949-99ad-bec035ae58eb'), AIMessage(content='The original prompt was clear and concise. It correctly instructed to solve the math problem step-by-step, considering multiple approaches if applicable, and to output the answer in the required format. No changes were necessary.', id='ae9cdf4a-6207-41c6-bacc-a604b6eeedd3'), HumanMessage(content='Consider simplifying the steps for better clarity:\\n\\n1. Read {content}, focusing on keywords.\\n2. Solve the math problem step-by-step, showing all intermediate steps.\\n3. Output the final answer as ##<answer> with no spaces or units.', id='cd451462-32ed-4c48-b784-85bd93943fce'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. 3. Output the final answer as ##<answer> with no spaces or units.', id='e38420cc-db7b-414d-b532-1237bb56487e'), AIMessage(content=\"Simplifying the steps enhances clarity by removing redundant wording. The instruction to consider multiple approaches was unnecessary as solving step-by-step inherently covers this. The core principles of conciseness and clarity are maintained, ensuring the task's success criteria are met.\", id='2d7b2a9d-14e1-48c8-8e20-c298af4723fc'), HumanMessage(content='The simplified steps are clearer and more concise. Removing unnecessary words like \"carefully\" and \"strictly\" improves readability without losing meaning. The instruction is easier to follow while still meeting the success criteria. Great job!', id='a6410741-b15b-45b6-9f6d-7850ea948ffd'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. 3. Output the final answer as ##<answer> with no spaces or units.', id='9c06293c-8b90-4815-9fab-4c16746f179e'), AIMessage(content='Simplifying the steps improves clarity and readability, making the instructions easier to follow without losing any meaning. The prompt now meets the success criteria in a more concise manner.', id='bd447233-5d3b-412e-93fa-5c7d69285f2e')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Conciseness and Clarity Expert\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. 3. Output the final answer as ##<answer> with no spaces or units., Justification: Simplifying the steps improves clarity and readability, making the instructions easier to follow without losing any meaning. The prompt now meets the success criteria in a more concise manner.\n",
      "Position: Style and Structure Expert, Bid: 6\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 3\n",
      "Position: Word Problem Solver, Bid: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Conciseness and Clarity Expert\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. 3. Output the final answer as ##<answer> with no spaces or units., Justification: Simplifying the steps improves clarity and readability, making the instructions easier to follow without losing any meaning. The prompt now meets the success criteria in a more concise manner.\n",
      "Result: [HumanMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. 3. Output the final answer as ##<answer> with no spaces or units.', id='a791c8a0-9d0b-4f01-b77d-2d40e16fc024'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. 3. Clearly separate and identify each step in the process. 4. Output the final answer as ##<answer> with no spaces or units.', id='0082d94d-29dd-4b95-b18d-e85b484bd056'), AIMessage(content='I added a third instruction to clearly separate and identify each step. This change ensures that the logical flow of solving the math problem is easy to follow, resulting in a clearer and more accurate final output.', id='e2db7949-d3a1-4dd9-82d0-98f9e421349e'), HumanMessage(content='The prompt structure is logical and follows COSTAR, but you can improve clarity and completeness by emphasizing structured steps. For example, \"Ensure intermediate steps are clearly labeled and separated.\" This ensures a detailed breakdown and accuracy.', id='f1c0a7e4-e9fa-4f16-88a5-095ca4186f08'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. Ensure intermediate steps are clearly labeled and separated. 3. Output the final answer as ##<answer> with no spaces or units.', id='b45a61fb-a06e-425a-b890-386e735f2ac0'), AIMessage(content='Added the instruction to clearly label and separate intermediate steps to enhance clarity and ensure a detailed breakdown. This aligns with the feedback to emphasize structure and completeness while maintaining the logical flow.', id='cac27fed-4f4a-4aa7-8b5d-4203a3cde3cf'), HumanMessage(content=\"The recommendation adds clarity by emphasizing the labeling and separation of steps, thus enhancing the prompt's structure and ensuring detailed breakdowns for better accuracy.\", id='3101b1ab-3f4c-495b-8fdc-bab6ab84c137'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. Ensure intermediate steps are clearly labeled and separated. 3. Output the final answer as ##<answer> with no spaces or units.', id='051a6a82-d388-4807-92d6-807bc3aaeb73'), AIMessage(content='Added the requirement to clearly label and separate intermediate steps to improve clarity and structure. This ensures a detailed breakdown of the problem-solving process, enhancing the overall accuracy of the solution.', id='f2ef6a3a-908d-40bd-a371-63f05b53187a')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Style and Structure Expert\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. Ensure intermediate steps are clearly labeled and separated. 3. Output the final answer as ##<answer> with no spaces or units., Justification: Added the requirement to clearly label and separate intermediate steps to improve clarity and structure. This ensures a detailed breakdown of the problem-solving process, enhancing the overall accuracy of the solution.\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 4\n",
      "Position: Contextual Relevance Expert, Bid: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Style and Structure Expert\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. Ensure intermediate steps are clearly labeled and separated. 3. Output the final answer as ##<answer> with no spaces or units., Justification: Added the requirement to clearly label and separate intermediate steps to improve clarity and structure. This ensures a detailed breakdown of the problem-solving process, enhancing the overall accuracy of the solution.\n",
      "Result: [HumanMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps. Ensure intermediate steps are clearly labeled and separated. 3. Output the final answer as ##<answer> with no spaces or units.', id='0dd03dc1-9b4c-4470-a07f-ab25287c28a3'), AIMessage(content='1. Read {content}, focusing on keywords.\\n2. Solve the math problem step-by-step, showing all intermediate steps. Ensure intermediate steps are clearly labeled and separated.\\n3. Output the final answer as ##<answer> with no spaces or units.', id='0b91c66d-df79-4908-9e72-e60ae7c48a54'), AIMessage(content='The original prompt largely satisfies the success criteria requirements, so no substantial changes were needed. I retained the clear structure and readability while ensuring the final answer meets the specified format. Additionally, placeholders were preserved as per the guidelines.', id='3928e0ff-4b5c-439b-b144-854e28ca1c5a'), HumanMessage(content=\"Your prompt is mostly clear. To enhance clarity and streamline the instruction, you might consider specifying that intermediate steps should be mathematically valid and logically sequenced.\\n\\nHere's a minor adjustment:\\n2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced.\\n\\nThis ensures clarity and mathematical rigor while still adhering to your core principles and guidelines.\", id='43ef145e-db6f-451e-81a5-5194557e871a'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. 3. Output the final answer as ##<answer> with no spaces or units.', id='d482067d-a221-4b1a-88cc-ae544333f08f'), AIMessage(content='The adjustment clarifies that intermediate steps should be logically sequenced, ensuring each step follows correctly from the previous one. This enhances clarity and mathematical rigor, while preserving the structure and placeholders as specified in the guidelines.', id='72594e84-33e5-47c4-8840-9b7430a5043c'), HumanMessage(content='Your revised prompt improves clarity by ensuring intermediate steps are logically sequenced, which aligns well with good mathematical practices. This minor adjustment enhances both clarity and rigor while maintaining the essential structure and guidelines. Nice job!', id='b6ddf191-7795-43af-8e37-5083484b004f'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. 3. Output the final answer as ##<answer> with no spaces or units.', id='eaab705a-5ae9-4fa3-8ba2-40ff27aba719'), AIMessage(content='The adjustment clarifies that intermediate steps should be logically sequenced, ensuring clarity and mathematical rigor while preserving the original structure and requirements.', id='9e480ce1-dd79-4c44-8441-4dc494e17939')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Mathematician\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. 3. Output the final answer as ##<answer> with no spaces or units., Justification: The adjustment clarifies that intermediate steps should be logically sequenced, ensuring clarity and mathematical rigor while preserving the original structure and requirements.\n",
      "Position: Style and Structure Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 5\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Incremental Prompting Expert, Bid: 5\n",
      "Position: Task Alignment Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Mathematician\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. 3. Output the final answer as ##<answer> with no spaces or units., Justification: The adjustment clarifies that intermediate steps should be logically sequenced, ensuring clarity and mathematical rigor while preserving the original structure and requirements.\n",
      "Result: [HumanMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. 3. Output the final answer as ##<answer> with no spaces or units.', id='c47cf7e4-cb12-4ec6-acae-4ae49acabd43'), AIMessage(content='1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. 3. Output the final answer as ##<answer> with no spaces or units.', id='1dfc1dc9-2911-43e5-b69b-22043357ffea'), AIMessage(content='The original prompt aligns well with the criteria specified. It clearly instructs the LLM to solve a math problem and specifies the required format for the answer. No modifications were needed as it already follows the guidelines thoroughly and meets the success criteria.', id='84ab1f2f-4f75-48d4-b1f4-3db2e1164052'), HumanMessage(content='To improve clarity and ensure the expected output, consider adding an explicit example within the instructions.\\n\\nFor example:\\n1. Read {content}, focusing on keywords.\\n2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. \\n3. For instance, if the problem is \"2 + 2\", the steps should show \"Step 1: 2 + 2 = 4\".\\n4. Output the final answer as ##4## with no spaces or units.\\n\\nThis inclusion demonstrates the expected output format effectively.', id='f7dd5db5-4734-4a27-9169-de8cf673b314'), AIMessage(content=\"1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. For instance, if the problem is '2 + 2', the steps should show 'Step 1: 2 + 2 = 4'. 3. Output the final answer as ##4## with no spaces or units.\", id='c479e296-2081-4cfd-b70e-d13ed02bcaf8'), AIMessage(content='The changes include an explicit example within the instructions to enhance clarity and demonstrate the expected output format. This ensures the LLM understands how to properly format both the intermediate steps and the final answer.', id='2bedd8a4-e6c1-4ae1-bd1e-a977cc6fbc25'), HumanMessage(content=\"Your recommendation to include an explicit example enhances clarity and guides the model on formatting steps and the final answer. This additional step improves the model's understanding of expectations, ensuring it meets both the instructional and output criteria. Well done!\", id='2f77c9a1-2443-4cce-8a4f-b5a34dc91023'), AIMessage(content=\"1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. For instance, if the problem is '2 + 2', the steps should show 'Step 1: 2 + 2 = 4'. 3. Output the final answer as ##4## with no spaces or units.\", id='81a2939c-caa3-4ea9-8003-ff95315c5fb3'), AIMessage(content='Including an explicit example improves clarity and ensures the model understands the expected output format. This step-by-step guidance ensures it adheres to both the instructional and output criteria effectively.', id='97731156-5e4f-42f4-9ad9-ec6deb570f7b')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Example Demonstration Expert\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. For instance, if the problem is '2 + 2', the steps should show 'Step 1: 2 + 2 = 4'. 3. Output the final answer as ##4## with no spaces or units., Justification: Including an explicit example improves clarity and ensures the model understands the expected output format. This step-by-step guidance ensures it adheres to both the instructional and output criteria effectively.\n",
      "Position: Style and Structure Expert, Bid: 7\n",
      "Position: Task Alignment Expert, Bid: 6\n",
      "Position: Conciseness and Clarity Expert, Bid: 5\n",
      "Position: Example Demonstration Expert, Bid: 5\n",
      "Position: Contextual Relevance Expert, Bid: 5\n",
      "Position: Programming Logic Expert, Bid: 5\n",
      "Position: Mathematician, Bid: 5\n",
      "Position: Word Problem Solver, Bid: 3\n",
      "Position: Incremental Prompting Expert, Bid: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Example Demonstration Expert\n",
      "\n",
      "Updated Prompt: 1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. For instance, if the problem is '2 + 2', the steps should show 'Step 1: 2 + 2 = 4'. 3. Output the final answer as ##4## with no spaces or units., Justification: Including an explicit example improves clarity and ensures the model understands the expected output format. This step-by-step guidance ensures it adheres to both the instructional and output criteria effectively.\n"
     ]
    }
   ],
   "source": [
    "result = leader_agent.optimise_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Read {content}, focusing on keywords. 2. Solve the math problem step-by-step, showing all intermediate steps clearly labeled and logically sequenced. For instance, if the problem is '2 + 2', the steps should show 'Step 1: 2 + 2 = 4'. 3. Output the final answer as ##4## with no spaces or units.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:  Linguistic Analyst\n",
      "Role:  The Linguistic Analyst is responsible for ensuring that the prompt is clear, concise, and unambiguous. They will analyze the language used in the prompt to ensure it is easily understandable and free of any potential biases.\n",
      "Function:  The Linguistic Analyst will review and refine the wording of the prompt to ensure clarity and neutrality, preventing any language that could lead to misinterpretation or bias in classification.\n",
      "\n",
      "Position:  Machine Learning Engineer\n",
      "Role:  The Machine Learning Engineer is responsible for optimizing the prompt for machine learning models. They will ensure that the prompt is structured in a way that maximizes the model's ability to accurately classify the sentence as positive or negative.\n",
      "Function:  The Machine Learning Engineer will test and iterate on the prompt structure to enhance the performance of classification algorithms, ensuring that the prompt facilitates accurate and reliable model outputs.\n",
      "\n",
      "Position:  User Experience Researcher\n",
      "Role:  The User Experience Researcher is responsible for evaluating the prompt from the end-user's perspective. They will ensure that the prompt is user-friendly and intuitive, and that it provides a positive experience for users interacting with the classification task.\n",
      "Function:  The User Experience Researcher will conduct user testing and gather feedback to refine the prompt, ensuring it is straightforward and easy to use, thereby improving overall user satisfaction and engagement with the task.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert \u001b[38;5;129;01min\u001b[39;00m moderator_agent_1\u001b[38;5;241m.\u001b[39mexperts:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRole: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFunction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mfunction \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m moderator_agent_2 \u001b[38;5;241m=\u001b[39m \u001b[43mModeratorAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m expert \u001b[38;5;129;01min\u001b[39;00m moderator_agent_2\u001b[38;5;241m.\u001b[39mexperts:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRole: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFunction: \u001b[39m\u001b[38;5;124m\"\u001b[39m, expert\u001b[38;5;241m.\u001b[39mfunction \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 159\u001b[0m, in \u001b[0;36mModeratorAgent.__init__\u001b[0;34m(self, base_prompt, additional_info, temp, model, experts)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_info \u001b[38;5;241m=\u001b[39m additional_info\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m    156\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemp,\n\u001b[1;32m    157\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    158\u001b[0m )\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts \u001b[38;5;241m=\u001b[39m experts \u001b[38;5;28;01mif\u001b[39;00m experts \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_experts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_evolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n",
      "Cell \u001b[0;32mIn[7], line 198\u001b[0m, in \u001b[0;36mModeratorAgent.generate_experts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_info\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mpositions \u001b[38;5;129;01mand\u001b[39;00m output\u001b[38;5;241m.\u001b[39mroles \u001b[38;5;129;01mand\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfunctions:\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2399\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2399\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:442\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    437\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    441\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:357\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/openai/_base_client.py:882\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    879\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_auth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_prompt = \"Classify the sentence as positive or negative: {content}\"\n",
    "criteria = \"This is a classification task with only two classes: positive and negative. The prompt should generalize.\"\n",
    "\n",
    "moderator_agent_1 = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    ")\n",
    "for expert in moderator_agent_1.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nRole: \", expert.role + \"\\nFunction: \", expert.function + \"\\n\")\n",
    "\n",
    "moderator_agent_2 = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    ")\n",
    "for expert in moderator_agent_2.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nRole: \", expert.role + \"\\nFunction: \", expert.function + \"\\n\")\n",
    "\n",
    "moderator_agent_3 = ModeratorAgent(\n",
    "    base_prompt=base_prompt,\n",
    "    criteria=criteria,\n",
    ")\n",
    "for expert in moderator_agent_3.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nRole: \", expert.role + \"\\nFunction: \", expert.function + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_prompt': 'Classify the sentence as positive or negative: {content}', 'prompt_evolution': []}\n",
      "{'base_prompt': 'Classify the sentence as positive or negative: {content}', 'prompt_evolution': [{'review': \"The prompt should explicitly instruct the LLM to consider sentiment polarity, specify that the classes are only 'positive' or 'negative,' and ensure clarity in classification without introducing unnecessary complexities or ambiguities.\", 'updated_prompt': \"Classify the sentiment of the given sentence as either 'positive' or 'negative': {content}. Ensure the classification is based strictly on sentiment polarity.\"}]}\n",
      "{'base_prompt': 'Classify the sentence as positive or negative: {content}', 'prompt_evolution': [{'review': 'The prompt needs clarity on the definition of positive and negative sentiment. Additionally, it should encourage consideration of context. Explicitly requiring the LLM to focus only on the sentiment will improve performance.', 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Consider the tone and context to make an accurate classification.'}, {'review': 'The original prompt is clear but could benefit from emphasizing neutrality and clarifying tone and context consideration.', 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Ensure to carefully evaluate the tone and context for a precise classification. Focus on neutrality in your assessment.'}, {'review': \"The prompt lacks clarity on when to declare neutrality and doesn't specify how to handle ambiguous cases. Also, the context evaluation instruction could be strengthened.\", 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Carefully evaluate the tone and context for precise classification. Avoid neutrality in your assessment. If the sentiment is ambiguous, choose the more likely classification.'}, {'review': 'The prompt is detailed but could be more concise and direct. Removing redundancy and emphasizing direct instructions will improve clarity and focus.', 'updated_prompt': 'Classify the sentiment of the following sentence as either positive or negative: {content}. Evaluate tone and context carefully for precise classification. Avoid neutrality. If sentiment is ambiguous, choose the more likely classification.'}]}\n",
      "Classify the sentence as positive or negative: {content}\n",
      "----\n",
      "Classify the sentiment of the given sentence as either 'positive' or 'negative': {content}. Ensure the classification is based strictly on sentiment polarity.\n",
      "----\n",
      "Classify the sentiment of the following sentence as either positive or negative: {content}. Evaluate tone and context carefully for precise classification. Avoid neutrality. If sentiment is ambiguous, choose the more likely classification.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Assuming leader_agent is already defined and initialized\n",
    "def run_optimisation(agent: LeaderAgent):\n",
    "    return agent.optimise_prompt()\n",
    "\n",
    "# Run 3 concurrent instances\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(run_optimisation, agent) for agent in [leader_agent_1, leader_agent_2, leader_agent_3]]\n",
    "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"----\")\n",
    "\n",
    "class PromptMerge(BaseModel):\n",
    "    \"\"\"Merged prompt based on the best parts of each prompt.\"\"\"\n",
    "    final_prompt: str = Field(description=\"Result of merging prompts\")\n",
    "\n",
    "# OpenAI Agent to pull togther best parts of each result\n",
    "def merge_results(results):\n",
    "    \"\"\"\n",
    "    Agent to merge best parts of each prompt\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=1.0,\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    system_message = \"\"\"You are an experienced AI prompt engineer. Your role is to combine good prompts to create great prompts!\n",
    "You have in-depth knowledge of large language models and prompt engineering best practices. Use knowledge at all times to guide your thinking.\"\"\"\n",
    "\n",
    "    template = \"\"\"Your task is to merge the best parts of the prompts below to create a better prompt.\n",
    "Carefully consider the strengths of each prompt and how they can be combined effectively whilst maintaining clarity and relevance.\n",
    "You will be penalized if the prompt is repetitive, lacks clarity or is incoherent.\n",
    "Aspects of the prompts to consider:\n",
    "- Conciseness and clarity\n",
    "- Contextual relevance\n",
    "- Task alignment\n",
    "- Example Demonstrations\n",
    "- Avoiding bias\n",
    "Consider aspects of good prompts beyond those listed above.\n",
    "Placeholders are notated using curly braces. You must not remove placeholders or add additional placeholders.\n",
    "I repeat, you must not remove placeholders or add additional placeholders.\n",
    "Do not make assumptions on what the placeholders represent.\n",
    "\n",
    "Prompts: {results}\n",
    "\n",
    "Return only the next worker to process or 'FINISH' in JSON format below:\n",
    "\n",
    "{{\n",
    "    \"final_prompt\": \"Result of merging prompts\",\n",
    "}}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "You will be penalized if your output cannot be parsed correctly.\"\"\"\n",
    "    pydantic_parser = PydanticOutputParser(pydantic_object=PromptMerge)\n",
    "    prompt_template = PromptTemplate(\n",
    "        system_message=system_message,\n",
    "        template=template,\n",
    "        input_variables=[\"results\"],\n",
    "        partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt_template | llm | pydantic_parser\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            output = chain.invoke({\"results\": results})\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Exception occurred:\", e)\n",
    "            continue\n",
    "    return output.final_prompt\n",
    "\n",
    "final_result = merge_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following sentence as either positive or negative: {content}. Evaluate tone and context carefully for precise classification. Ensure the classification is based strictly on sentiment polarity. Avoid neutrality. If sentiment is ambiguous, choose the more likely classification.\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
