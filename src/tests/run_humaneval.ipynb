{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "unique_id = \"HumanEval\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iwatson/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from human_eval.data import read_problems, write_jsonl\n",
    "import promptbench as pb\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import human_eval\n",
    "\n",
    "agent_model = \"claude-3-5-sonnet\"\n",
    "infer_model = \"mistralv0.3\"\n",
    "\n",
    "\n",
    "baseline_prompt = human_eval.get_baseline_prompt()\n",
    "emotive_prompt = human_eval.get_emotive_prompt()\n",
    "CoT_prompt = human_eval.get_CoT_prompt()\n",
    "if agent_model == \"gpt-3.5-turbo\":\n",
    "    from prompts.gpt_3_5_turbo import human_eval\n",
    "    authoritarian_prompts = human_eval.get_authoritarian_prompts()\n",
    "    market_prompts = human_eval.get_market_prompts()\n",
    "    hierarchical_prompts = human_eval.get_hierarchical_prompts()\n",
    "elif agent_model == \"gpt-4o-mini\":\n",
    "    from prompts.gpt_4o_mini import human_eval\n",
    "    authoritarian_prompts = human_eval.get_authoritarian_prompts()\n",
    "    market_prompts = human_eval.get_market_prompts()\n",
    "    hierarchical_prompts = human_eval.get_hierarchical_prompts()\n",
    "elif agent_model == \"gpt-4o\":\n",
    "    from prompts.gpt_4o import human_eval\n",
    "    authoritarian_prompts = human_eval.get_authoritarian_prompts()\n",
    "    market_prompts = human_eval.get_market_prompts()\n",
    "    hierarchical_prompts = human_eval.get_hierarchical_prompts()\n",
    "elif agent_model == \"claude-3-haiku\":\n",
    "    from prompts.claude_3_haiku import human_eval\n",
    "    authoritarian_prompts = human_eval.get_authoritarian_prompts()\n",
    "    market_prompts = human_eval.get_market_prompts()\n",
    "    hierarchical_prompts = human_eval.get_hierarchical_prompts()\n",
    "elif agent_model == \"claude-3-5-sonnet\":\n",
    "    from prompts.claude_3_5_sonnet import human_eval\n",
    "    authoritarian_prompts = human_eval.get_authoritarian_prompts()\n",
    "    market_prompts = human_eval.get_market_prompts()\n",
    "    hierarchical_prompts = human_eval.get_hierarchical_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model is not supported!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mpb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLMModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/promptbench/promptbench/models/__init__.py:85\u001b[0m, in \u001b[0;36mLLMModel.__init__\u001b[0;34m(self, model, max_new_tokens, temperature, device, dtype, model_dir, system_prompt, api_key, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m, max_new_tokens: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, temperature: \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_dir: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, system_prompt: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, api_key:\u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/promptbench/promptbench/models/__init__.py:105\u001b[0m, in \u001b[0;36mLLMModel._create_model\u001b[0;34m(self, max_new_tokens, temperature, device, dtype, model_dir, system_prompt, api_key, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, max_new_tokens, temperature, device, dtype)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model is not supported!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The model is not supported!"
     ]
    }
   ],
   "source": [
    "llm = pb.LLMModel(model=infer_model, max_new_tokens=4096, temperature=0)\n",
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "def extract_code_blocks(text):\n",
    "    \"\"\"\n",
    "    Extracts code blocks from markdown text.\n",
    "    \"\"\"\n",
    "    # This regex matches content between triple backticks\n",
    "    code_blocks = re.findall(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "    return code_blocks\n",
    "\n",
    "async def async_process(data, base_prompt, llm):\n",
    "    \"\"\"\n",
    "    Processes a single data point asynchronously.\n",
    "    \"\"\"\n",
    "    task_prompt = data['prompt']\n",
    "    prompt_text = base_prompt.format(content=task_prompt)\n",
    "    try:\n",
    "        # Set individual timeout for each LLM call\n",
    "        completion = await asyncio.wait_for(llm(prompt_text), timeout=180)\n",
    "    except asyncio.TimeoutError:\n",
    "        return {\"task_id\": data['task_id'], \"completion\": \"Timeout\"}\n",
    "\n",
    "    if isinstance(completion, AIMessage):\n",
    "        completion = completion.content\n",
    "\n",
    "    code_blocks = extract_code_blocks(completion)\n",
    "    return {\"task_id\": data['task_id'], \"completion\": code_blocks[0] if code_blocks else \"\"}\n",
    "\n",
    "async def process_data(problems, base_prompt, llm):\n",
    "    \"\"\"\n",
    "    Processes all data points sequentially.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for task_id, data in tqdm(problems.items()):\n",
    "        data['task_id'] = task_id\n",
    "        result = await async_process(data, base_prompt, llm)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "async def main(base_prompt, llm, sample_size):\n",
    "    problems = read_problems()\n",
    "    if sample_size:\n",
    "        problems = dict(islice(problems.items(), sample_size))\n",
    "\n",
    "    results = await process_data(problems, base_prompt, llm)\n",
    "\n",
    "    # Write results to a JSONL file\n",
    "    output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/human_eval/generations/{agent_model}_{prompt_type}.jsonl\"\n",
    "    if not os.path.exists(os.path.dirname(output_file_path)):\n",
    "        os.makedirs(os.path.dirname(output_file_path))\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for result in results:\n",
    "            jsonl_file.write(json.dumps(result) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = read_problems()\n",
    "\n",
    "# def extract_code_blocks(text):\n",
    "#     \"\"\"\n",
    "#     Extracts code blocks from markdown text.\n",
    "#     \"\"\"\n",
    "#     # This regex matches content between triple backticks\n",
    "#     code_blocks = re.findall(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "#     return code_blocks\n",
    "\n",
    "# def generate_one_completion(base_prompt: str, task_prompt: str, llm):\n",
    "#     \"\"\"\n",
    "#     Generates code completion for a given task prompt.\n",
    "#     \"\"\"\n",
    "#     prompt_text = base_prompt.format(content=task_prompt)\n",
    "#     completion = llm(prompt_text)\n",
    "#     if isinstance(completion, AIMessage):\n",
    "#         completion = completion.content\n",
    "#     code_blocks = extract_code_blocks(completion)\n",
    "#     return code_blocks[0] if code_blocks else \"\"\n",
    "\n",
    "# def generate_llm_outputs(base_prompt: str, prompt_type, llm, sample_size: int = None):\n",
    "#     \"\"\"\n",
    "#     Generates completions for all tasks in the human evaluation set.\n",
    "#     \"\"\"\n",
    "#     from itertools import islice\n",
    "\n",
    "#     # generate completions for all tasks in parallel\n",
    "#     problems = read_problems()\n",
    "#     if sample_size:\n",
    "#         tasks = list(islice(problems.items(), sample_size))\n",
    "#     else:\n",
    "#         tasks = list(problems.items())\n",
    "\n",
    "#     samples = []\n",
    "\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = {\n",
    "#             executor.submit(generate_one_completion, base_prompt, problems[task_id][\"prompt\"], llm): task_id\n",
    "#             for task_id, _ in tasks\n",
    "#         }\n",
    "\n",
    "#         for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "#             task_id = futures[future]\n",
    "#             try:\n",
    "#                 completion = future.result()\n",
    "#                 samples.append(dict(task_id=task_id, completion=completion))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Task {task_id} generated an exception: {e}\")\n",
    "\n",
    "#     write_jsonl(f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/human_eval/generations/{agent_model}_{prompt_type}.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = authoritarian_prompts + market_prompts + hierarchical_prompts\n",
    "prompt_types = [\"authoritarian_1\", \"authoritarian_2\", \"authoritarian_3\", \"market_1\", \"market_2\", \"market_3\", \"hierarchical_1\", \"hierarchical_2\", \"hierarchical_3\"]\n",
    "# prompts = [baseline_prompt, CoT_prompt, emotive_prompt]\n",
    "# prompt_types = [\"baseline\", \"CoT\", \"emotive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating completions for authoritarian_1 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [23:04<00:00, 46.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for authoritarian_1 prompts generated successfully\n",
      "Generating completions for authoritarian_2 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [16:24<00:00, 32.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for authoritarian_2 prompts generated successfully\n",
      "Generating completions for authoritarian_3 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [08:44<00:00, 17.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for authoritarian_3 prompts generated successfully\n",
      "Generating completions for market_1 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:57<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for market_1 prompts generated successfully\n",
      "Generating completions for market_2 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [17:22<00:00, 34.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for market_2 prompts generated successfully\n",
      "Generating completions for market_3 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:15<00:00, 14.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for market_3 prompts generated successfully\n",
      "Generating completions for hierarchical_1 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:27<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for hierarchical_1 prompts generated successfully\n",
      "Generating completions for hierarchical_2 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:55<00:00, 15.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for hierarchical_2 prompts generated successfully\n",
      "Generating completions for hierarchical_3 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:32<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completions for hierarchical_3 prompts generated successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply nest_asyncio for Jupyter notebook compatibility\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "for prompt, prompt_type in zip(prompts, prompt_types):\n",
    "    print(f\"Generating completions for {prompt_type} prompts\")\n",
    "    if infer_model == \"llama3.1\" or infer_model == \"mistral:v0.3\": \n",
    "        await main(prompt, llm, sample_size=30)\n",
    "    else:\n",
    "        generate_llm_outputs(prompt, prompt_type, llm, sample_size=30)\n",
    "    print(f\"Completions for {prompt_type} prompts generated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "30it [00:00, 4166.53it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 180.63it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/baseline_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 17932.04it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n",
      "Reading samples...\n",
      "30it [00:00, 1691.55it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 208.26it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/emotive_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 10781.35it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.43333333333333335)}\n",
      "Reading samples...\n",
      "30it [00:00, 1716.07it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 199.11it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/CoT_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 15233.55it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 862.97it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 123.48it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_authoritarian_1_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 13530.01it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 2531.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 208.83it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_authoritarian_2_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 13681.54it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.13333333333333333)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 1689.21it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 201.48it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_authoritarian_3_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 10419.77it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.23333333333333334)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 1131.20it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 196.37it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_market_1_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 9916.39it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "Running test suites...\n",
      "30it [00:00, 1684.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 205.79it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_market_2_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 10770.27it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n",
      "Reading samples...\n",
      "30it [00:00, 1800.31it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 217.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_market_3_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 11133.35it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.3)}\n",
      "Reading samples...\n",
      "30it [00:00, 1250.75it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 217.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_hierarchical_1_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 16732.60it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 1568.59it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 202.88it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_hierarchical_2_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 10500.64it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.3333333333333333)}\n",
      "Reading samples...\n",
      "30it [00:00, 1203.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 215.49it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/claude-3-5-sonnet_hierarchical_3_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 17401.34it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "30it [00:00, 1181.22it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  9.92it/s] \n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_authoritarian_1_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 12165.63it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.3333333333333333)}\n",
      "Reading samples...\n",
      "30it [00:00, 1578.67it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 183.51it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_authoritarian_2_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 4999.97it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.5333333333333333)}\n",
      "Reading samples...\n",
      "30it [00:00, 4587.78it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 192.59it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_authoritarian_3_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 11621.79it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.5)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "Running test suites...\n",
      "30it [00:00, 4254.15it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 193.54it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_market_1_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 9208.13it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.2)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 4278.88it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 184.52it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_market_2_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 9792.91it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n",
      "Reading samples...\n",
      "30it [00:00, 1820.37it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 208.16it/s]\n",
      "Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_market_3_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 9974.56it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 1096.14it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 200.26it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_hierarchical_1_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 12423.89it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.36666666666666664)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "30it [00:00, 1602.51it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 212.68it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_hierarchical_2_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 13263.32it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.3333333333333333)}\n",
      "Reading samples...\n",
      "30it [00:00, 1772.64it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 30/30 [00:00<00:00, 206.98it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Writing results to /outputs/mistralv0.3/human_eval/generations/gpt-4o_hierarchical_3_results.jsonl...\n",
      "100%|██████████| 30/30 [00:00<00:00, 15889.52it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.4666666666666667)}\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"baseline\", \n",
    "    \"emotive\", \n",
    "    \"CoT\", \n",
    "    \"claude-3-5-sonnet_authoritarian_1\", \n",
    "    \"claude-3-5-sonnet_authoritarian_2\", \n",
    "    \"claude-3-5-sonnet_authoritarian_3\", \n",
    "    \"claude-3-5-sonnet_market_1\", \n",
    "    \"claude-3-5-sonnet_market_2\", \n",
    "    \"claude-3-5-sonnet_market_3\", \n",
    "    \"claude-3-5-sonnet_hierarchical_1\",\n",
    "    \"claude-3-5-sonnet_hierarchical_2\",\n",
    "    \"claude-3-5-sonnet_hierarchical_3\",\n",
    "    \"gpt-4o_authoritarian_1\",\n",
    "    \"gpt-4o_authoritarian_2\",\n",
    "    \"gpt-4o_authoritarian_3\",\n",
    "    \"gpt-4o_market_1\",\n",
    "    \"gpt-4o_market_2\",\n",
    "    \"gpt-4o_market_3\",\n",
    "    \"gpt-4o_hierarchical_1\",\n",
    "    \"gpt-4o_hierarchical_2\",\n",
    "    \"gpt-4o_hierarchical_3\",\n",
    "]\n",
    "for prompt in prompts:\n",
    "    # Define the paths\n",
    "    host_volume_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/human_eval/\"\n",
    "    container_volume_path = f\"/outputs/{infer_model}/human_eval/\"\n",
    "    input_file_path = f\"/outputs/{infer_model}/human_eval/generations/{prompt}.jsonl\"\n",
    "\n",
    "    docker_command = f'docker run --rm -v \"{host_volume_path}:{container_volume_path}\" humaneval-evaluation \"{input_file_path}\" --k 1'\n",
    "\n",
    "    !{docker_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(hierarchical_prompts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"hierarchical_1\"\n",
    "prompt_set = hierarchical_prompts_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [02:03<00:00,  1.33it/s]\n",
      "100%|██████████| 164/164 [01:08<00:00,  2.40it/s]\n",
      "100%|██████████| 164/164 [01:15<00:00,  2.17it/s]\n",
      "100%|██████████| 164/164 [02:15<00:00,  1.21it/s]\n",
      "100%|██████████| 164/164 [00:58<00:00,  2.79it/s]\n",
      "100%|██████████| 164/164 [01:24<00:00,  1.93it/s]\n",
      "100%|██████████| 164/164 [03:21<00:00,  1.23s/it]\n",
      "100%|██████████| 164/164 [01:40<00:00,  1.62it/s]\n",
      "100%|██████████| 164/164 [02:36<00:00,  1.05it/s]\n",
      "100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, prompt in enumerate(prompt_set):\n",
    "    generate_llm_outputs(prompt, \"human_eval_{name}_{idx}\".format(name=name, idx=idx), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(market_prompts_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [01:02<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate_llm_outputs(authoritarian_prompts_1[5], \"human_eval_{name}_{idx}\".format(name='authoritarian_1', idx=5), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 19112.16it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 220.68it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_0_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 16649.30it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9085365853658537)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "164it [00:00, 5851.04it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 42.41it/s] \n",
      "Writing results to /outputs/human_eval_hierarchical_1_1_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 15305.29it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9207317073170732)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "164it [00:00, 9532.64it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 48.89it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_2_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 15829.02it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9024390243902439)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "Running test suites...\n",
      "164it [00:00, 14096.19it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 41.75it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_3_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 12074.81it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.8597560975609756)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "Running test suites...\n",
      "164it [00:00, 10630.14it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 214.71it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_4_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 13393.55it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9207317073170732)}\n",
      "Reading samples...\n",
      "164it [00:00, 8957.40it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 218.27it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_5_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 11764.02it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9146341463414634)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "164it [00:00, 15141.23it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 212.43it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_6_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 8699.89it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.6158536585365854)}\n",
      "Reading samples...\n",
      "164it [00:00, 6452.29it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 164/164 [00:03<00:00, 43.86it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_7_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 14114.12it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9024390243902439)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "164it [00:00, 5141.65it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 204.83it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_8_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 9005.60it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.8719512195121951)}\n",
      "Reading samples...\n",
      "164it [00:00, 4480.77it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 180.70it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_9_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 5781.16it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.75)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in range(10):\n",
    "    output_file = \"human_eval_{name}_{i}\".format(name=name, i=i)\n",
    "    # Define the paths\n",
    "    host_volume_path = \"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs\"\n",
    "    container_volume_path = \"/outputs\"\n",
    "    input_file_path = f\"/outputs/{output_file}.jsonl\"\n",
    "\n",
    "    # Verify the file exists on the host\n",
    "    if not os.path.isfile(os.path.join(host_volume_path, f\"{output_file}.jsonl\")):\n",
    "        raise FileNotFoundError(f\"File not found: {os.path.join(host_volume_path, output_file+'jsonl')}\")\n",
    "\n",
    "    # Construct the Docker command\n",
    "    docker_command = f'docker run --rm -v \"{host_volume_path}:{container_volume_path}\" humaneval-evaluation \"{input_file_path}\" --k 1'\n",
    "\n",
    "    # Run the Docker command\n",
    "    !{docker_command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 'human_eval_authoritarian_1_0_6_results.jsonl' to 'human_eval_authoritarian_0_6_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_2_results.jsonl' to 'human_eval_authoritarian_0_2_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_5_results.jsonl' to 'human_eval_authoritarian_0_5_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_1_results.jsonl' to 'human_eval_authoritarian_0_1_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_8_results.jsonl' to 'human_eval_authoritarian_0_8_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_9_results.jsonl' to 'human_eval_authoritarian_0_9_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_0_results.jsonl' to 'human_eval_authoritarian_0_0_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_4_results.jsonl' to 'human_eval_authoritarian_0_4_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_3_results.jsonl' to 'human_eval_authoritarian_0_3_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_7_results.jsonl' to 'human_eval_authoritarian_0_7_results.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# # Path to the directory containing the files\n",
    "# directory_path = \"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs\"\n",
    "\n",
    "# # Pattern to match and replace\n",
    "# # This pattern specifically looks for three numbers separated by underscores, with 'authoritarian' preceding them.\n",
    "# pattern_to_match = r\"human_eval_authoritarian_1_(\\d+)_(\\d+)_results.jsonl\"\n",
    "# pattern_to_replace_with = r\"human_eval_authoritarian_\\1_\\2_results.jsonl\"\n",
    "\n",
    "# # List all files in the directory\n",
    "# files = os.listdir(directory_path)\n",
    "\n",
    "# # Loop through each file\n",
    "# for file_name in files:\n",
    "#     # Check if the file name matches the pattern\n",
    "#     if re.match(pattern_to_match, file_name):\n",
    "#         new_file_name = re.sub(pattern_to_match, pattern_to_replace_with, file_name)\n",
    "        \n",
    "#         # Construct the full old and new file paths\n",
    "#         old_file_path = os.path.join(directory_path, file_name)\n",
    "#         new_file_path = os.path.join(directory_path, new_file_name)\n",
    "        \n",
    "#         # Rename the file\n",
    "#         os.rename(old_file_path, new_file_path)\n",
    "#         print(f\"Renamed '{file_name}' to '{new_file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
