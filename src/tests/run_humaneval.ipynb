{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "unique_id = \"HumanEval\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from human_eval.data import read_problems, write_jsonl\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "data = read_problems()\n",
    "\n",
    "def extract_code_blocks(text):\n",
    "    \"\"\"\n",
    "    Extracts code blocks from markdown text.\n",
    "    \"\"\"\n",
    "    # This regex matches content between triple backticks\n",
    "    code_blocks = re.findall(r'```python\\n(.*?)\\n```', text, re.DOTALL)\n",
    "    return code_blocks\n",
    "\n",
    "def generate_one_completion(base_prompt: str, task_prompt: str, llm):\n",
    "    \"\"\"\n",
    "    Generates code completion for a given task prompt.\n",
    "    \"\"\"\n",
    "    prompt_text = base_prompt.format(content=task_prompt)\n",
    "    completion = llm.invoke(prompt_text)\n",
    "    code_blocks = extract_code_blocks(completion.content)\n",
    "    return code_blocks[0] if code_blocks else \"\"\n",
    "\n",
    "def generate_llm_outputs(base_prompt: str, file_name: str, llm, sample_size: int = None):\n",
    "    \"\"\"\n",
    "    Generates completions for all tasks in the human evaluation set.\n",
    "    \"\"\"\n",
    "    from itertools import islice\n",
    "\n",
    "    # generate completions for all tasks in parallel\n",
    "    problems = read_problems()\n",
    "    if sample_size:\n",
    "        tasks = list(islice(problems.items(), sample_size))\n",
    "    else:\n",
    "        tasks = list(problems.items())\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(generate_one_completion, base_prompt, problems[task_id][\"prompt\"], llm): task_id\n",
    "            for task_id, _ in tasks\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            task_id = futures[future]\n",
    "            try:\n",
    "                completion = future.result()\n",
    "                samples.append(dict(task_id=task_id, completion=completion))\n",
    "            except Exception as e:\n",
    "                print(f\"Task {task_id} generated an exception: {e}\")\n",
    "\n",
    "    # write_jsonl(f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{file_name}.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval_prompts import HumanEvalPrompts\n",
    "\n",
    "human_eval_prompts = HumanEvalPrompts()\n",
    "\n",
    "baseline_prompt = human_eval_prompts.get_baseline_prompt()\n",
    "emotive_prompt = human_eval_prompts.get_emotive_prompt()\n",
    "CoT_prompt = human_eval_prompts.get_CoT_prompt()\n",
    "reflection_prompts = human_eval_prompts.get_reflection_prompts()\n",
    "authoritarian_prompts_0 = human_eval_prompts.get_authoritarian_prompts_0()\n",
    "authoritarian_prompts_1 = human_eval_prompts.get_authoritarian_prompts_1()\n",
    "market_prompts_0 = human_eval_prompts.get_market_prompts_0()\n",
    "market_prompts_1 = human_eval_prompts.get_market_prompts_1()\n",
    "hierarchical_prompts_0 = human_eval_prompts.get_hierarchical_prompts_0()\n",
    "hierarchical_prompts_1 = human_eval_prompts.get_hierarchical_prompts_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=1.0,\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 162/164 [00:36<00:00,  4.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_llm_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman_eval_baseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m generate_llm_outputs(emotive_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_eval_emotive\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm)\n\u001b[1;32m      3\u001b[0m generate_llm_outputs(CoT_prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_eval_CoT\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm)\n",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m, in \u001b[0;36mgenerate_llm_outputs\u001b[0;34m(base_prompt, file_name, llm, sample_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     48\u001b[0m     futures \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     49\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(generate_one_completion, base_prompt, problems[task_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], llm): task_id\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task_id, _ \u001b[38;5;129;01min\u001b[39;00m tasks\n\u001b[1;32m     51\u001b[0m     }\n\u001b[0;32m---> 53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/concurrent/futures/_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 243\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    246\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_llm_outputs(baseline_prompt, \"human_eval_baseline\", llm)\n",
    "generate_llm_outputs(emotive_prompt, \"human_eval_emotive\", llm)\n",
    "generate_llm_outputs(CoT_prompt, \"human_eval_CoT\", llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 18430.08it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 164/164 [00:03<00:00, 54.22it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_baseline_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 14610.26it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9207317073170732)}\n",
      "Reading samples...\n",
      "164it [00:00, 6927.29it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 221.41it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_emotive_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 14798.86it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.8902439024390244)}\n",
      "Reading samples...\n",
      "164it [00:00, 7517.91it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:02<00:00, 55.57it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_CoT_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 16291.26it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.8597560975609756)}\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"baseline\", \"emotive\", \"CoT\"]\n",
    "for prompt in prompts:\n",
    "    output_file = f\"human_eval_{prompt}\"\n",
    "    # Define the paths\n",
    "    host_volume_path = \"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs\"\n",
    "    container_volume_path = \"/outputs\"\n",
    "    input_file_path = f\"/outputs/{output_file}.jsonl\"\n",
    "\n",
    "    docker_command = f'docker run --rm -v \"{host_volume_path}:{container_volume_path}\" humaneval-evaluation \"{input_file_path}\" --k 1'\n",
    "\n",
    "    !{docker_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(hierarchical_prompts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"hierarchical_1\"\n",
    "prompt_set = hierarchical_prompts_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [02:03<00:00,  1.33it/s]\n",
      "100%|██████████| 164/164 [01:08<00:00,  2.40it/s]\n",
      "100%|██████████| 164/164 [01:15<00:00,  2.17it/s]\n",
      "100%|██████████| 164/164 [02:15<00:00,  1.21it/s]\n",
      "100%|██████████| 164/164 [00:58<00:00,  2.79it/s]\n",
      "100%|██████████| 164/164 [01:24<00:00,  1.93it/s]\n",
      "100%|██████████| 164/164 [03:21<00:00,  1.23s/it]\n",
      "100%|██████████| 164/164 [01:40<00:00,  1.62it/s]\n",
      "100%|██████████| 164/164 [02:36<00:00,  1.05it/s]\n",
      "100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, prompt in enumerate(prompt_set):\n",
    "    generate_llm_outputs(prompt, \"human_eval_{name}_{idx}\".format(name=name, idx=idx), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(market_prompts_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [01:02<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate_llm_outputs(authoritarian_prompts_1[5], \"human_eval_{name}_{idx}\".format(name='authoritarian_1', idx=5), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 19112.16it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 220.68it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_0_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 16649.30it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9085365853658537)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "164it [00:00, 5851.04it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 42.41it/s] \n",
      "Writing results to /outputs/human_eval_hierarchical_1_1_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 15305.29it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9207317073170732)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "164it [00:00, 9532.64it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 48.89it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_2_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 15829.02it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9024390243902439)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "Running test suites...\n",
      "164it [00:00, 14096.19it/s]\n",
      "100%|██████████| 164/164 [00:03<00:00, 41.75it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_3_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 12074.81it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.8597560975609756)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "Running test suites...\n",
      "164it [00:00, 10630.14it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 214.71it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_4_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 13393.55it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9207317073170732)}\n",
      "Reading samples...\n",
      "164it [00:00, 8957.40it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 218.27it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_5_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 11764.02it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9146341463414634)}\n",
      "Reading samples...\n",
      "0it [00:00, ?it/s]Running test suites...\n",
      "164it [00:00, 15141.23it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 212.43it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_6_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 8699.89it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.6158536585365854)}\n",
      "Reading samples...\n",
      "164it [00:00, 6452.29it/s]\n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Running test suites...\n",
      "100%|██████████| 164/164 [00:03<00:00, 43.86it/s] \n",
      "  0%|          | 0/164 [00:00<?, ?it/s]Writing results to /outputs/human_eval_hierarchical_1_7_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 14114.12it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.9024390243902439)}\n",
      "0it [00:00, ?it/s]Reading samples...\n",
      "164it [00:00, 5141.65it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 204.83it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_8_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 9005.60it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.8719512195121951)}\n",
      "Reading samples...\n",
      "164it [00:00, 4480.77it/s]\n",
      "Running test suites...\n",
      "100%|██████████| 164/164 [00:00<00:00, 180.70it/s]\n",
      "Writing results to /outputs/human_eval_hierarchical_1_9_results.jsonl...\n",
      "100%|██████████| 164/164 [00:00<00:00, 5781.16it/s]\n",
      "Pass@k results: {'pass@1': np.float64(0.75)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in range(10):\n",
    "    output_file = \"human_eval_{name}_{i}\".format(name=name, i=i)\n",
    "    # Define the paths\n",
    "    host_volume_path = \"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs\"\n",
    "    container_volume_path = \"/outputs\"\n",
    "    input_file_path = f\"/outputs/{output_file}.jsonl\"\n",
    "\n",
    "    # Verify the file exists on the host\n",
    "    if not os.path.isfile(os.path.join(host_volume_path, f\"{output_file}.jsonl\")):\n",
    "        raise FileNotFoundError(f\"File not found: {os.path.join(host_volume_path, output_file+'jsonl')}\")\n",
    "\n",
    "    # Construct the Docker command\n",
    "    docker_command = f'docker run --rm -v \"{host_volume_path}:{container_volume_path}\" humaneval-evaluation \"{input_file_path}\" --k 1'\n",
    "\n",
    "    # Run the Docker command\n",
    "    !{docker_command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 'human_eval_authoritarian_1_0_6_results.jsonl' to 'human_eval_authoritarian_0_6_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_2_results.jsonl' to 'human_eval_authoritarian_0_2_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_5_results.jsonl' to 'human_eval_authoritarian_0_5_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_1_results.jsonl' to 'human_eval_authoritarian_0_1_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_8_results.jsonl' to 'human_eval_authoritarian_0_8_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_9_results.jsonl' to 'human_eval_authoritarian_0_9_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_0_results.jsonl' to 'human_eval_authoritarian_0_0_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_4_results.jsonl' to 'human_eval_authoritarian_0_4_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_3_results.jsonl' to 'human_eval_authoritarian_0_3_results.jsonl'\n",
      "Renamed 'human_eval_authoritarian_1_0_7_results.jsonl' to 'human_eval_authoritarian_0_7_results.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the directory containing the files\n",
    "directory_path = \"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs\"\n",
    "\n",
    "# Pattern to match and replace\n",
    "# This pattern specifically looks for three numbers separated by underscores, with 'authoritarian' preceding them.\n",
    "pattern_to_match = r\"human_eval_authoritarian_1_(\\d+)_(\\d+)_results.jsonl\"\n",
    "pattern_to_replace_with = r\"human_eval_authoritarian_\\1_\\2_results.jsonl\"\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Loop through each file\n",
    "for file_name in files:\n",
    "    # Check if the file name matches the pattern\n",
    "    if re.match(pattern_to_match, file_name):\n",
    "        new_file_name = re.sub(pattern_to_match, pattern_to_replace_with, file_name)\n",
    "        \n",
    "        # Construct the full old and new file paths\n",
    "        old_file_path = os.path.join(directory_path, file_name)\n",
    "        new_file_path = os.path.join(directory_path, new_file_name)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"Renamed '{file_name}' to '{new_file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
