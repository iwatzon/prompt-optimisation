{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import promptbench as pb\n",
    "\n",
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Models and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported datasets: \n",
      "  sst2\n",
      "  cola\n",
      "  qqp\n",
      "  mnli\n",
      "  mnli_matched\n",
      "  mnli_mismatched\n",
      "  qnli\n",
      "  wnli\n",
      "  rte\n",
      "  mrpc\n",
      "  mmlu\n",
      "  squad_v2\n",
      "  un_multi\n",
      "  iwslt2017\n",
      "  math\n",
      "  bool_logic\n",
      "  valid_parentheses\n",
      "  gsm8k\n",
      "  csqa\n",
      "  bigbench_date\n",
      "  bigbench_object_tracking\n",
      "  last_letter_concat\n",
      "  numersense\n",
      "  qasc\n",
      "  bbh\n",
      "  drop\n",
      "  arc-easy\n",
      "  arc-challenge\n"
     ]
    }
   ],
   "source": [
    "# print all supported datasets in promptbench\n",
    "print('All supported datasets: ')\n",
    "for dataset in pb.SUPPORTED_DATASETS:\n",
    "    print(f'  {dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported models: \n",
      "  google/flan-t5-large\n",
      "  llama2-7b\n",
      "  llama2-7b-chat\n",
      "  llama2-13b\n",
      "  llama2-13b-chat\n",
      "  llama2-70b\n",
      "  llama2-70b-chat\n",
      "  phi-1.5\n",
      "  phi-2\n",
      "  palm\n",
      "  gpt-3.5-turbo\n",
      "  gpt-4\n",
      "  gpt-4-1106-preview\n",
      "  gpt-3.5-turbo-1106\n",
      "  gpt-4-0125-preview\n",
      "  gpt-3.5-turbo-0125\n",
      "  gpt-4-turbo\n",
      "  gpt-4o\n",
      "  gpt-4o-mini\n",
      "  vicuna-7b\n",
      "  vicuna-13b\n",
      "  vicuna-13b-v1.3\n",
      "  google/flan-ul2\n",
      "  gemini-pro\n",
      "  mistralai/Mistral-7B-v0.1\n",
      "  mistralai/Mistral-7B-Instruct-v0.1\n",
      "  mistralai/Mixtral-8x7B-v0.1\n",
      "  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "  01-ai/Yi-6B\n",
      "  01-ai/Yi-34B\n",
      "  01-ai/Yi-6B-Chat\n",
      "  01-ai/Yi-34B-Chat\n",
      "  baichuan-inc/Baichuan2-7B-Base\n",
      "  baichuan-inc/Baichuan2-13B-Base\n",
      "  baichuan-inc/Baichuan2-7B-Chat\n",
      "  baichuan-inc/Baichuan2-13B-Chat\n",
      "  mistral:v0.3\n",
      "  llama3.1\n"
     ]
    }
   ],
   "source": [
    "# print all supported models in promptbench\n",
    "print('All supported models: ')\n",
    "for model in pb.SUPPORTED_MODELS:\n",
    "    print(f'  {model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1309, 228, 51, 563, 501, 457, 285, 209, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 859, 451, 919, 1206, 569, 13, 326, 865]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name = \"gsm8k\"\n",
    "\n",
    "dataset = pb.DatasetLoader.load_dataset(dataset_name)\n",
    "length = len(dataset)\n",
    "sample_indices = random.sample(range(length), 30)\n",
    "print(sample_indices)\n",
    "# extract 50 samples from the dataset\n",
    "samples = [dataset[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import gsm8k, sst2\n",
    "\n",
    "agent_model = \"claude-3-5-sonnet\"\n",
    "infer_model = \"llama3.1\"\n",
    "\n",
    "if dataset_name == \"gsm8k\":\n",
    "    baseline_prompt = gsm8k.get_baseline_prompt()\n",
    "    emotive_prompt = gsm8k.get_emotive_prompt()\n",
    "    CoT_prompt = gsm8k.get_CoT_prompt()\n",
    "    if agent_model == \"gpt-3.5-turbo\":\n",
    "        from prompts.gpt_3_5_turbo import gsm8k\n",
    "        authoritarian_prompts = gsm8k.get_authoritarian_prompts()\n",
    "        market_prompts = gsm8k.get_market_prompts()\n",
    "        hierarchical_prompts = gsm8k.get_hierarchical_prompts()\n",
    "    elif agent_model == \"gpt-4o-mini\":\n",
    "        from prompts.gpt_4o_mini import gsm8k\n",
    "        authoritarian_prompts = gsm8k.get_authoritarian_prompts()\n",
    "        market_prompts = gsm8k.get_market_prompts()\n",
    "        hierarchical_prompts = gsm8k.get_hierarchical_prompts()\n",
    "    elif agent_model == \"gpt-4o\":\n",
    "        from prompts.gpt_4o import gsm8k\n",
    "        authoritarian_prompts = gsm8k.get_authoritarian_prompts()\n",
    "        market_prompts = gsm8k.get_market_prompts()\n",
    "        hierarchical_prompts = gsm8k.get_hierarchical_prompts()\n",
    "    elif agent_model == \"claude-3-haiku\":\n",
    "        from prompts.claude_3_haiku import gsm8k\n",
    "        authoritarian_prompts = gsm8k.get_authoritarian_prompts()\n",
    "        market_prompts = gsm8k.get_market_prompts()\n",
    "        hierarchical_prompts = gsm8k.get_hierarchical_prompts()\n",
    "    elif agent_model == \"claude-3-5-sonnet\":\n",
    "        from prompts.claude_3_5_sonnet import gsm8k\n",
    "        authoritarian_prompts = gsm8k.get_authoritarian_prompts()\n",
    "        market_prompts = gsm8k.get_market_prompts()\n",
    "        hierarchical_prompts = gsm8k.get_hierarchical_prompts()\n",
    "\n",
    "elif dataset_name == \"sst2\":\n",
    "    baseline_prompt = sst2.get_baseline_prompt()\n",
    "    emotive_prompt = sst2.get_emotive_prompt()\n",
    "    CoT_prompt = sst2.get_CoT_prompt()\n",
    "    if agent_model == \"gpt-3.5-turbo\":\n",
    "        from prompts.gpt_3_5_turbo import sst2\n",
    "        authoritarian_prompts = sst2.get_authoritarian_prompts()\n",
    "        market_prompts = sst2.get_market_prompts()\n",
    "        hierarchical_prompts = sst2.get_hierarchical_prompts()\n",
    "    elif agent_model == \"gpt-4o-mini\":\n",
    "        from prompts.gpt_4o_mini import sst2\n",
    "        authoritarian_prompts = sst2.get_authoritarian_prompts()\n",
    "        market_prompts = sst2.get_market_prompts()\n",
    "        hierarchical_prompts = sst2.get_hierarchical_prompts()\n",
    "    elif agent_model == \"gpt-4o\":\n",
    "        from prompts.gpt_4o import sst2\n",
    "        authoritarian_prompts = sst2.get_authoritarian_prompts()\n",
    "        market_prompts = sst2.get_market_prompts()\n",
    "        hierarchical_prompts = sst2.get_hierarchical_prompts()\n",
    "    elif agent_model == \"claude-3-haiku\":\n",
    "        from prompts.claude_3_haiku import sst2\n",
    "        authoritarian_prompts = sst2.get_authoritarian_prompts()\n",
    "        market_prompts = sst2.get_market_prompts()\n",
    "        hierarchical_prompts = sst2.get_hierarchical_prompts()\n",
    "    elif agent_model == \"claude-3-5-sonnet\":\n",
    "        from prompts.claude_3_5_sonnet import sst2\n",
    "        authoritarian_prompts = sst2.get_authoritarian_prompts()\n",
    "        market_prompts = sst2.get_market_prompts()\n",
    "        hierarchical_prompts = sst2.get_hierarchical_prompts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1\n"
     ]
    }
   ],
   "source": [
    "llm = pb.LLMModel(model=infer_model, max_new_tokens=1024, temperature=0)\n",
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type = \"market\"\n",
    "\n",
    "if prompt_type == \"baseline\":\n",
    "    prompts = pb.Prompt([baseline_prompt])\n",
    "    iters = 3\n",
    "elif prompt_type == \"emotive\":\n",
    "    prompts = pb.Prompt([emotive_prompt])\n",
    "    iters = 3\n",
    "elif prompt_type == \"CoT\":\n",
    "    prompts = pb.Prompt([CoT_prompt])\n",
    "    iters = 3\n",
    "elif prompt_type == \"authoritarian\":\n",
    "    prompts = pb.Prompt(authoritarian_prompts)\n",
    "    iters = 1\n",
    "elif prompt_type == \"market\":\n",
    "    prompts = pb.Prompt(market_prompts)\n",
    "    iters = 1\n",
    "elif prompt_type == \"hierarchical\":\n",
    "    prompts = pb.Prompt(hierarchical_prompts)\n",
    "    iters = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve this math problem:\n",
      "{content}\n",
      "\n",
      "Show your work clearly. Include key steps and reasoning.\n",
      "\n",
      "Example 1 (Simple arithmetic):\n",
      "Problem: A train travels 120 km in 2 hours. What is its average speed?\n",
      "Work:\n",
      "1. Speed = Distance ÷ Time\n",
      "2. Distance = 120 km\n",
      "3. Time = 2 hours\n",
      "4. Speed = 120 km ÷ 2 hours = 60 km/h\n",
      "Answer: ##60\n",
      "\n",
      "Example 2 (Algebra):\n",
      "Problem: If 3x + 7 = 22, what is the value of x?\n",
      "Work:\n",
      "1. Subtract 7 from both sides: 3x = 15\n",
      "2. Divide both sides by 3: x = 5\n",
      "Answer: ##5\n",
      "\n",
      "Example 3 (Geometry - incomplete, finish it):\n",
      "Problem: A rectangle has a length of 8 cm and a width of 5 cm. What is its area?\n",
      "Work:\n",
      "1. Area of rectangle = length × width\n",
      "2. Length = 8 cm, Width = 5 cm\n",
      "3. Area = 8 cm × 5 cm = 40 cm²\n",
      "Answer: ##40\n",
      "\n",
      "Example 4 (Percentages):\n",
      "Problem: A shirt originally priced at $80 is on sale for 25% off. What is the sale price?\n",
      "Work:\n",
      "1. Calculate the discount: 25% of $80 = 0.25 × $80 = $20\n",
      "2. Subtract the discount from the original price: $80 - $20 = $60\n",
      "Answer: ##60\n",
      "\n",
      "Now solve the given problem, show your work, and provide the answer in the required format.\n",
      "\n",
      "Answer: ##<answer in arabic numerals>\n",
      "SOLVE:\n",
      "{content}\n",
      "\n",
      "INTERACTIVE PROBLEM-SOLVING JOURNEY:\n",
      "1. Warm-up: What's 7-3? (Solve this first!)\n",
      "   ##4\n",
      "   Excellent! Now let's tackle word problems.\n",
      "\n",
      "2. Problem Evolution:\n",
      "   a) If 5 pens cost $10, how much does 1 pen cost?\n",
      "      Solution: $10 / 5 = $2\n",
      "      ##2\n",
      "   \n",
      "   b) If 1 pen costs $2, how many pens can you buy with $7?\n",
      "      Solution: $7 / $2 = 3.5 pens\n",
      "      ##3.5\n",
      "   \n",
      "   c) You have $7 and pens cost $2 each. After buying as many as possible, \n",
      "      what percentage of a pen can you buy with the remainder?\n",
      "      Solution: \n",
      "      Pens bought: 3 (costing $6)\n",
      "      Remainder: $1\n",
      "      Percentage of a pen: ($1 / $2) * 100 = 50%\n",
      "      ##50\n",
      "\n",
      "ASCII ART PROBLEM-SOLVING:\n",
      "For mixture problems:\n",
      "  Solution A  Solution B  Mixture\n",
      "     30%         70%       40%\n",
      "      |           |         |\n",
      "      V           V         V\n",
      "    [====]     [========] [=====]\n",
      "     10L         20L       30L\n",
      "\n",
      "MISTAKE SPOTLIGHT:\n",
      "Problem: A shirt costs $80. It's on sale for 25% off. What's the sale price?\n",
      "Incorrect: $80 - 25 = $55 (subtracting percentage as a whole number)\n",
      "Correct: $80 - ($80 * 0.25) = $80 - $20 = $60\n",
      "##60\n",
      "\n",
      "PROBLEM-SOLVING MINDSET:\n",
      "/* Visualize the problem: Draw if needed */\n",
      "/* Break complex problems into simpler steps */\n",
      "/* Always check if your answer makes logical sense */\n",
      "\n",
      "SOLUTION STEPS:\n",
      "1. Read and analyze\n",
      "2. Plan your approach\n",
      "3. Calculate step-by-step\n",
      "4. Verify your result\n",
      "5. Present as: ##<answer>\n",
      "\n",
      "OUTPUT:\n",
      "##<your answer (arabic numerals)>\n",
      "Solve:\n",
      "\n",
      "{content}\n",
      "\n",
      "1. Estimate.\n",
      "2. Model. Define variables.\n",
      "3. Solve twice. Show steps.\n",
      "4. Check units.\n",
      "5. Verify.\n",
      "6. List errors, alternatives.\n",
      "7. Apply to real world.\n",
      "\n",
      "Answer: ##<number>\n"
     ]
    }
   ],
   "source": [
    "# prompts = prompts[2:]\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, prompt in enumerate(prompts):\n",
    "#     preds = []\n",
    "#     labels = []\n",
    "#     questions = []\n",
    "#     for data in tqdm(samples):\n",
    "#         # process input\n",
    "#         input_text = pb.InputProcess.basic_format(prompt, data)\n",
    "#         label = data['label']\n",
    "#         raw_pred = model(input_text)\n",
    "#         # process output\n",
    "#         pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(\\d+)\")\n",
    "#         # print(f\"Pred: {pred}, Label: {label}\")\n",
    "#         questions.append(data['content'])\n",
    "#         preds.append(pred)\n",
    "#         labels.append(label)\n",
    "\n",
    "#         output = list(zip(dataset, pred, label))\n",
    "\n",
    "#         # Define the output file path\n",
    "#         output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{dataset_name}_{prompt_type[idx]}.jsonl\"\n",
    "\n",
    "#         # Save the paired list to a JSON file\n",
    "#         with open(output_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "#             for pair in output:\n",
    "#                 jsonl_file.write(json.dumps(pair) + '\\n')\n",
    "    \n",
    "#     # evaluate\n",
    "#     score = pb.Eval.compute_cls_accuracy(preds, labels)\n",
    "#     print(f\"{score:.3f}, {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [04:08<00:00,  8.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [04:58<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:43<00:00, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.000\n",
      "Mean: 0.100, Median: 0.133, Std. Dev.: 0.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def async_process(data, model, prompt):\n",
    "    # Process input\n",
    "    input_text = pb.InputProcess.basic_format(prompt, data)\n",
    "    label = data['label']\n",
    "    try:\n",
    "        # Set individual timeout for each LLM call\n",
    "        raw_pred = await asyncio.wait_for(model(input_text), timeout=60)\n",
    "        if isinstance(raw_pred, AIMessage):\n",
    "            raw_pred = raw_pred.content\n",
    "    except asyncio.TimeoutError:\n",
    "        return (data['content'], \"Timeout\", -1, label)\n",
    "    \n",
    "    # Process output\n",
    "    if dataset_name == \"gsm8k\":\n",
    "        pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(\\d+| \\d+)\")\n",
    "    elif dataset_name == \"sst2\":\n",
    "        pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(positive|negative|Positive|Negative| positive| negative| Positive| Negative)\")\n",
    "        pred = {\"positive\": 1, \"negative\": 0}.get(pred.lower(), -1)\n",
    "    \n",
    "    return (data['content'], raw_pred, pred, label)\n",
    "\n",
    "async def process_data(samples, model, prompt):\n",
    "    results = []\n",
    "    for data in tqdm(samples):\n",
    "        result = await async_process(data, model, prompt)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "async def main():\n",
    "    main_dict = {\n",
    "        \"results\": {}\n",
    "    }\n",
    "    mean_scores = []\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        prompt_scores = []\n",
    "        result_dict = {\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        for i in range(1, iters + 1):\n",
    "            # Process data sequentially\n",
    "            results = await process_data(samples, llm, prompt)\n",
    "\n",
    "            # Initialize lists to store results\n",
    "            questions = []\n",
    "            raw_responses = []\n",
    "            preds = []\n",
    "            labels = []\n",
    "\n",
    "            for data_content, raw, pred, label in results:\n",
    "                questions.append(data_content)\n",
    "                raw_responses.append(raw)\n",
    "                preds.append(pred)\n",
    "                labels.append(label)\n",
    "\n",
    "            # Create the results section for the current iteration\n",
    "            results_section = {}\n",
    "            for j, (content, raw, pred, label) in enumerate(zip(questions, raw_responses, preds, labels), start=1):\n",
    "                # print(\"Prediction:\", pred, \"Label:\", label)\n",
    "                results_section[j] = {\n",
    "                    \"content\": content,\n",
    "                    \"response\": raw,\n",
    "                    \"pred\": pred,\n",
    "                    \"label\": label\n",
    "                }\n",
    "\n",
    "            # Compute the score for the current results section\n",
    "            score = pb.Eval.compute_cls_accuracy(preds, labels)\n",
    "            print(f\"Score: {score:.3f}\")\n",
    "            results_section[\"score\"] = score\n",
    "            prompt_scores.append(score)\n",
    "\n",
    "            # Add the results section to the result dictionary\n",
    "            result_dict[f\"results_{i}\"] = results_section\n",
    "\n",
    "        # Compute the average score for the current prompt\n",
    "        prompt_mean_score = sum(prompt_scores) / len(prompt_scores)\n",
    "        prompt_std_dev = (sum([(score - prompt_mean_score) ** 2 for score in prompt_scores]) / len(prompt_scores)) ** 0.5\n",
    "        result_dict[\"prompt_mean_score\"] = prompt_mean_score\n",
    "\n",
    "        # Add the result dictionary to the main dictionary\n",
    "        main_dict[\"results\"][idx + 1] = result_dict\n",
    "\n",
    "        # Append the average score to the avg_scores list for overall calculations\n",
    "        mean_scores.append(prompt_mean_score)\n",
    "\n",
    "        # Write the result dictionary to the JSONL file after each prompt\n",
    "        if agent_model == \"\":\n",
    "            output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/{dataset_name}/{prompt_type}.jsonl\"\n",
    "        else:\n",
    "            output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/{dataset_name}/{agent_model}_{prompt_type}.jsonl\"\n",
    "        with open(output_file_path, 'a', encoding='utf-8') as jsonl_file:\n",
    "            jsonl_file.write(json.dumps(result_dict, indent=4) + '\\n')\n",
    "\n",
    "    # Compute the overall average score, median, and standard deviation\n",
    "    mean_score = sum(mean_scores) / len(mean_scores)\n",
    "    median_score = sorted(mean_scores)[len(mean_scores) // 2]\n",
    "    std_dev = (sum([(score - mean_score) ** 2 for score in mean_scores]) / len(mean_scores)) ** 0.5\n",
    "    print(f\"Mean: {mean_score:.3f}, Median: {median_score:.3f}, Std. Dev.: {std_dev:.3f}\")\n",
    "\n",
    "    main_dict[\"architecture_mean_score\"] = mean_score\n",
    "    main_dict[\"architecture_median_score\"] = median_score\n",
    "    main_dict[\"architecture_std_dev\"] = std_dev\n",
    "\n",
    "    # Write the final summary to the JSONL file\n",
    "    with open(output_file_path, 'a', encoding='utf-8') as jsonl_file:\n",
    "        jsonl_file.write(json.dumps(main_dict, indent=4) + '\\n')\n",
    "\n",
    "# Run the main function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.044, Median: 0.033, Std. Dev.: 0.016028\n"
     ]
    }
   ],
   "source": [
    "mean_scores = [0.033, 0.033, 0.067 ]\n",
    "\n",
    "mean_score = sum(mean_scores) / len(mean_scores)\n",
    "median_score = sorted(mean_scores)[len(mean_scores) // 2]\n",
    "std_dev = (sum([(score - mean_score) ** 2 for score in mean_scores]) / len(mean_scores)) ** 0.5\n",
    "print(f\"Mean: {mean_score:.3f}, Median: {median_score:.3f}, Std. Dev.: {std_dev:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def process_data(data):\n",
    "#     # process input\n",
    "#     input_text = pb.InputProcess.basic_format(prompt, data)\n",
    "#     label = data['label']\n",
    "#     if data[\"content\"] == \"##\":\n",
    "#         return (data['content'], \"Timeout\", -1, label)\n",
    "#     else:\n",
    "#         raw_pred = llm(input_text)\n",
    "#     if (infer_model == \"llama3.1\") or (infer_model == \"mistral:v0.3\"):\n",
    "#         raw_pred = raw_pred.content\n",
    "#     # print(\"Output generated\")\n",
    "#     # process output\n",
    "#     if dataset_name == \"gsm8k\":\n",
    "#         # pred = extract_and_map_number(raw_pred)\n",
    "#         pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(\\d+| \\d+)\")\n",
    "#     elif dataset_name == \"sst2\":\n",
    "#         # pred = extract_and_map_sentiment(raw_pred)\n",
    "#         # regex to find extract ##positive or ##negative sentiment\n",
    "#         pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(positive|negative|Positive|Negative| positive| negative| Positive| Negative)\")\n",
    "#         pred = {\"positive\": 1, \"negative\": 0}.get(pred.lower(), -1)\n",
    "#     # print(f\"Raw: {raw_pred}, Pred: {pred}, Label: {label}\")\n",
    "#     # Collect the necessary information for output\n",
    "#     # print(\"Output processed\")\n",
    "#     return (data['content'], raw_pred, pred, label)\n",
    "\n",
    "# main_dict = {\n",
    "#     \"results\": {}\n",
    "# }\n",
    "# mean_scores = []\n",
    "# for idx, prompt in enumerate(prompts):\n",
    "#     prompt_scores = []\n",
    "#     result_dict = {\n",
    "#         \"prompt\": prompt\n",
    "#     }\n",
    "#     for i in range(1, iters+1):\n",
    "#         # Initialize lists to store results\n",
    "#         questions = []\n",
    "#         raw_responses = []\n",
    "#         preds = []\n",
    "#         labels = []\n",
    "\n",
    "#         # Use ThreadPoolExecutor to process data in parallel\n",
    "#         with ThreadPoolExecutor() as executor:\n",
    "#             # Map process_data function to each item in samples\n",
    "#             future_to_data = {executor.submit(process_data, data): data for data in samples}\n",
    "            \n",
    "#             for future in tqdm(as_completed(future_to_data), total=len(samples)):\n",
    "#                 data_content, raw, pred, label = future.result()\n",
    "#                 questions.append(data_content)\n",
    "#                 raw_responses.append(raw)\n",
    "#                 preds.append(pred)\n",
    "#                 labels.append(label)\n",
    "\n",
    "#         # Create the results section for the current iteration\n",
    "#         results_section = {}\n",
    "#         for j, (content, raw, pred, label) in enumerate(zip(questions, raw_responses, preds, labels), start=1):\n",
    "#             results_section[j] = {\n",
    "#                 \"content\": content,\n",
    "#                 \"response\": raw, \n",
    "#                 \"pred\": pred,\n",
    "#                 \"label\": label\n",
    "#             }\n",
    "\n",
    "#         # Compute the score for the current results section\n",
    "#         score = pb.Eval.compute_cls_accuracy(preds, labels)\n",
    "#         print(f\"Score: {score:.3f}\")\n",
    "#         results_section[\"score\"] = score\n",
    "#         prompt_scores.append(score)\n",
    "\n",
    "#         # Add the results section to the result dictionary\n",
    "#         result_dict[f\"results_{i}\"] = results_section\n",
    "\n",
    "#     # Compute the average score for the current prompt\n",
    "#     prompt_mean_score = sum(prompt_scores) / len(prompt_scores)\n",
    "#     prompt_std_dev = (sum([(score - prompt_mean_score) ** 2 for score in prompt_scores]) / len(prompt_scores)) ** 0.5\n",
    "#     result_dict[\"prompt_mean_score\"] = prompt_mean_score\n",
    "\n",
    "#     # Add the result dictionary to the main dictionary\n",
    "#     main_dict[\"results\"][idx + 1] = result_dict\n",
    "\n",
    "#     # Append the average score to the avg_scores list for overall calculations\n",
    "#     mean_scores.append(prompt_mean_score)\n",
    "\n",
    "#     # Write the result dictionary to the JSONL file after each prompt\n",
    "#     if agent_model == \"\":\n",
    "#         output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/{dataset_name}/{prompt_type}.jsonl\"\n",
    "#     else:\n",
    "#         output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{infer_model}/{dataset_name}/{agent_model}_{prompt_type}.jsonl\"\n",
    "#     with open(output_file_path, 'a', encoding='utf-8') as jsonl_file:\n",
    "#         jsonl_file.write(json.dumps(result_dict, indent=4) + '\\n')\n",
    "\n",
    "# # Compute the overall average score, median, and standard deviation\n",
    "# mean_score = sum(mean_scores) / len(mean_scores)\n",
    "# median_score = sorted(mean_scores)[len(mean_scores) // 2]\n",
    "# std_dev = (sum([(score - mean_score) ** 2 for score in mean_scores]) / len(mean_scores)) ** 0.5\n",
    "# print(f\"Mean: {mean_score:.3f}, Median: {median_score:.3f}, Std. Dev.: {std_dev:.3f}\")\n",
    "\n",
    "# main_dict[\"architecture_mean_score\"] = mean_score\n",
    "# main_dict[\"architecture_median_score\"] = median_score\n",
    "# main_dict[\"architecture_std_dev\"] = std_dev\n",
    "\n",
    "# # Write the final summary to the JSONL file\n",
    "# with open(output_file_path, 'a', encoding='utf-8') as jsonl_file:\n",
    "#     jsonl_file.write(json.dumps(main_dict, indent=4) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pre-defined Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported methods: \n",
      "['CoT', 'ZSCoT', 'least_to_most', 'generated_knowledge', 'expert_prompting', 'emotion_prompt', 'baseline']\n",
      "Supported datasets for each method: \n",
      "{'CoT': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'ZSCoT': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'expert_prompting': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'emotion_prompt': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'least_to_most': ['gsm8k', 'last_letter_concat'], 'generated_knowledge': ['csqa', 'numersense', 'qasc'], 'baseline': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking', 'last_letter_concat', 'numersense', 'qasc']}\n"
     ]
    }
   ],
   "source": [
    "# load method\n",
    "# print all methods and their supported datasets\n",
    "print('All supported methods: ')\n",
    "print(pb.SUPPORTED_METHODS)\n",
    "print('Supported datasets for each method: ')\n",
    "print(pb.METHOD_SUPPORT_DATASET)\n",
    "\n",
    "method = pb.PEMethod(method='baseline', \n",
    "                    dataset=dataset_name,\n",
    "                    verbose=True,  # if True, print the detailed prompt and response\n",
    "                    prompt_id = 1  # for emotion_prompt \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'convert_text_to_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m results\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/promptbench/promptbench/prompt_engineering/__init__.py:77\u001b[0m, in \u001b[0;36mPEMethod.test\u001b[0;34m(self, dataset, model, num_samples)\u001b[0m\n\u001b[1;32m     74\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     76\u001b[0m input_text \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 77\u001b[0m ouput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m res \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m##(.*)\u001b[39m\u001b[38;5;124m'\u001b[39m, ouput)\n\u001b[1;32m     79\u001b[0m pred \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;28;01melse\u001b[39;00m ouput\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/promptbench/promptbench/prompt_engineering/base.py:30\u001b[0m, in \u001b[0;36mBase.query\u001b[0;34m(self, input_text, model)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_text, model):\n\u001b[1;32m     28\u001b[0m     instr_get_answer \u001b[38;5;241m=\u001b[39m input_text  \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     29\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease output your answer at the end as ##<your answer (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_range\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m     prompt_get_answer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_text_to_prompt\u001b[49m(instr_get_answer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m     answer \u001b[38;5;241m=\u001b[39m model(prompt_get_answer)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# print(instr_get_answer)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# print(answer)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'convert_text_to_prompt'"
     ]
    }
   ],
   "source": [
    "results = method.test(dataset, \n",
    "                      model, \n",
    "                      num_samples=2,\n",
    "                      )\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
