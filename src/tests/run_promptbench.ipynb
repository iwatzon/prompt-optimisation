{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iwatson/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import promptbench as pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported datasets: \n",
      "  sst2\n",
      "  cola\n",
      "  qqp\n",
      "  mnli\n",
      "  mnli_matched\n",
      "  mnli_mismatched\n",
      "  qnli\n",
      "  wnli\n",
      "  rte\n",
      "  mrpc\n",
      "  mmlu\n",
      "  squad_v2\n",
      "  un_multi\n",
      "  iwslt2017\n",
      "  math\n",
      "  bool_logic\n",
      "  valid_parentheses\n",
      "  gsm8k\n",
      "  csqa\n",
      "  bigbench_date\n",
      "  bigbench_object_tracking\n",
      "  last_letter_concat\n",
      "  numersense\n",
      "  qasc\n",
      "  bbh\n",
      "  drop\n",
      "  arc-easy\n",
      "  arc-challenge\n"
     ]
    }
   ],
   "source": [
    "# print all supported datasets in promptbench\n",
    "print('All supported datasets: ')\n",
    "for dataset in pb.SUPPORTED_DATASETS:\n",
    "    print(f'  {dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported models: \n",
      "  google/flan-t5-large\n",
      "  llama2-7b\n",
      "  llama2-7b-chat\n",
      "  llama2-13b\n",
      "  llama2-13b-chat\n",
      "  llama2-70b\n",
      "  llama2-70b-chat\n",
      "  phi-1.5\n",
      "  phi-2\n",
      "  palm\n",
      "  gpt-3.5-turbo\n",
      "  gpt-4\n",
      "  gpt-4-1106-preview\n",
      "  gpt-3.5-turbo-1106\n",
      "  gpt-4-0125-preview\n",
      "  gpt-3.5-turbo-0125\n",
      "  gpt-4-turbo\n",
      "  gpt-4o\n",
      "  vicuna-7b\n",
      "  vicuna-13b\n",
      "  vicuna-13b-v1.3\n",
      "  google/flan-ul2\n",
      "  gemini-pro\n",
      "  mistralai/Mistral-7B-v0.1\n",
      "  mistralai/Mistral-7B-Instruct-v0.1\n",
      "  mistralai/Mixtral-8x7B-v0.1\n",
      "  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "  01-ai/Yi-6B\n",
      "  01-ai/Yi-34B\n",
      "  01-ai/Yi-6B-Chat\n",
      "  01-ai/Yi-34B-Chat\n",
      "  baichuan-inc/Baichuan2-7B-Base\n",
      "  baichuan-inc/Baichuan2-13B-Base\n",
      "  baichuan-inc/Baichuan2-7B-Chat\n",
      "  baichuan-inc/Baichuan2-13B-Chat\n"
     ]
    }
   ],
   "source": [
    "# print all supported models in promptbench\n",
    "print('All supported models: ')\n",
    "for model in pb.SUPPORTED_MODELS:\n",
    "    print(f'  {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"sst2\"\n",
    "dataset = pb.DatasetLoader.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"it 's a charming and often affecting journey . \", 'label': 1}, {'content': 'unflinchingly bleak and desperate ', 'label': 0}, {'content': 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ', 'label': 1}, {'content': \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \", 'label': 1}, {'content': \"it 's slow -- very , very slow . \", 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[654, 114, 25, 759, 281, 250, 228, 142, 754, 104, 692, 758, 558, 89, 604, 432, 32, 30, 95, 223, 238, 517, 616, 27, 574, 203, 733, 665, 718, 429]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "length = len(dataset)\n",
    "sample_indices = random.sample(range(length), 30)\n",
    "print(sample_indices)\n",
    "# extract 50 samples from the dataset\n",
    "samples = [dataset[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LLMModel():\n",
    "#     def __init__(self, model: str, max_new_tokens: int=20, temperature: float=0.0, device: str=\"cuda\", dtype: str=\"auto\", model_dir: str=None, system_prompt: str=None, api_key:str =None, **kwargs):\n",
    "#         self.model_name = model\n",
    "#         self.model = self._create_model(max_new_tokens, temperature, device, dtype, model_dir, system_prompt, api_key, **kwargs)\n",
    "\n",
    "#     def _create_model(self, max_new_tokens, temperature, device, dtype, model_dir, system_prompt, api_key, **kwargs):\n",
    "#         \"\"\"Uses ChatOllama to create a model and returns it.\"\"\"\n",
    "\n",
    "#         llm = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "model = pb.LLMModel(model=model_name, max_new_tokens=4096, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsm8k_prompts import GSM8KPrompts\n",
    "from sst2_prompts import SST2Prompts\n",
    "\n",
    "gsm8k_prompts = GSM8KPrompts()\n",
    "\n",
    "gsm8k_baseline_prompt = gsm8k_prompts.get_baseline_prompt()\n",
    "gmk9k_emotive_prompt = gsm8k_prompts.get_emotive_prompt()\n",
    "gsm8k_CoT_prompt = gsm8k_prompts.get_zero_shot_CoT_prompt()\n",
    "# reflection_prompts = gsm8k_prompts.get_reflection_prompts() \n",
    "gsm8k_authoritarian_prompts_0 = gsm8k_prompts.get_authoritarian_prompts_0()\n",
    "gsm8k_authoritarian_prompts_1 = gsm8k_prompts.get_authoritarian_prompts_1()\n",
    "gsm8k_market_prompts_0 = gsm8k_prompts.get_market_prompts_0()\n",
    "gsm8k_market_prompts_1 = gsm8k_prompts.get_market_prompts_1()\n",
    "gsm8k_hierarchical_prompts_0 = gsm8k_prompts.get_hierarchical_prompts_0()\n",
    "gsm8k_hierarchical_prompts_1 = gsm8k_prompts.get_hierarchical_prompts_1()\n",
    "\n",
    "sst2_prompts = SST2Prompts()\n",
    "\n",
    "sst2_baseline_prompt = sst2_prompts.get_baseline_prompt()\n",
    "sst2_emotive_prompt = sst2_prompts.get_emotive_prompt()\n",
    "sst2_CoT_prompt = sst2_prompts.get_zero_shot_CoT_prompt()\n",
    "# reflection_prompts = sst2_prompts.get_reflection_prompts()\n",
    "sst2_authoritarian_prompts_0 = sst2_prompts.get_authoritarian_prompts_0()\n",
    "sst2_authoritarian_prompts_1 = sst2_prompts.get_authoritarian_prompts_1()\n",
    "sst2_market_prompts_0 = sst2_prompts.get_market_prompts_0()\n",
    "sst2_market_prompts_1 = sst2_prompts.get_market_prompts_1()\n",
    "sst2_hierarchical_prompts_0 = sst2_prompts.get_hierarchical_prompts_0()\n",
    "sst2_hierarchical_prompts_1 = sst2_prompts.get_hierarchical_prompts_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pb.Prompt(sst2_hierarchical_prompts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentiment of the following text as either positive or negative, paying attention to emotive language and the overall emotional tone. Output your answer clearly as ##positive or ##negative without any spaces.\n",
      "\n",
      "{content}\n",
      "Classify the sentiment of the following text as either positive or negative: {content}\n",
      "Please output your answer at the end as ##<your answer (No format restrictions)>\n",
      "Please classify the sentiment of the following text as either positive or negative by breaking it down into aspects. Identify key aspects within the text, assess the sentiment of each aspect considering the emotional impact, emotive language, and lexical choices, and then determine the overall sentiment based on how these aspects contribute to the general tone. Here is an example of breaking down into aspects, evaluating emotional impact, and assessing:\n",
      "\n",
      "Example:\n",
      "Text: \"The device has an excellent display, but the battery life is disappointing.\"\n",
      "- Aspect 1: Display - Sentiment: Positive (excellent), Emotional Impact: High Positive\n",
      "- Aspect 2: Battery life - Sentiment: Negative (disappointing), Emotional Impact: High Negative\n",
      "\n",
      "Overall Sentiment: Negative (due to the significant negative impact of battery life on user experience)\n",
      "\n",
      "Output your answer at the end as ##positive or ##negative with no spaces.\n",
      "\n",
      "{content}\n",
      "Carefully analyze the given text for sentiment. Break the text down into specific aspects or topics, and consider the sentiment of each identified aspect. Pay attention to how these sentiments interact and contribute to the overall sentiment. Consider nuances of emotions, emotional intensity, lexical choices, and emotive language markers, such as adjectives, adverbs, punctuation (e.g., exclamation points, ellipses), and capitalization. Classify the sentiment of the overall text as either positive or negative:\n",
      "\n",
      "{content}\n",
      "\n",
      "Please output your answer at the end as ##positive or ##negative with no spaces.\n",
      "> Classify the sentiment of the following text as either positive or negative:\n",
      "> \n",
      "> {content}\n",
      "> \n",
      "> Output the sentiment classification as ##positive or ##negative with no spaces.\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type = [\n",
    "    \"hierarchical_1_0\",\n",
    "    \"hierarchical_1_1\",\n",
    "    \"hierarchical_1_2\",\n",
    "    \"hierarchical_1_3\",\n",
    "    \"hierarchical_1_4\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, prompt in enumerate(prompts):\n",
    "#     preds = []\n",
    "#     labels = []\n",
    "#     questions = []\n",
    "#     for data in tqdm(samples):\n",
    "#         # process input\n",
    "#         input_text = pb.InputProcess.basic_format(prompt, data)\n",
    "#         label = data['label']\n",
    "#         raw_pred = model(input_text)\n",
    "#         # process output\n",
    "#         pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(\\d+)\")\n",
    "#         # print(f\"Pred: {pred}, Label: {label}\")\n",
    "#         questions.append(data['content'])\n",
    "#         preds.append(pred)\n",
    "#         labels.append(label)\n",
    "\n",
    "#         output = list(zip(dataset, pred, label))\n",
    "\n",
    "#         # Define the output file path\n",
    "#         output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{dataset_name}_{prompt_type[idx]}.jsonl\"\n",
    "\n",
    "#         # Save the paired list to a JSON file\n",
    "#         with open(output_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "#             for pair in output:\n",
    "#                 jsonl_file.write(json.dumps(pair) + '\\n')\n",
    "    \n",
    "#     # evaluate\n",
    "#     score = pb.Eval.compute_cls_accuracy(preds, labels)\n",
    "#     print(f\"{score:.3f}, {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:05<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:48<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933\n",
      "Mean: 0.880, Median: 0.933, Std. Dev.: 0.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_map_sentiment(text):\n",
    "    # Attempt to find the term following \"##\"\n",
    "    try:\n",
    "        # Split the text around \"##\" and take the part right after it\n",
    "        term = text.split('##')[1].split()[0]  # This assumes the term is the first word following \"##\"\n",
    "    except IndexError:\n",
    "        # If \"##\" isn't found or there's no term after it, return None\n",
    "        return -1\n",
    "    \n",
    "    # Map the extracted term to a numerical value\n",
    "    if term.lower() == 'positive':\n",
    "        return 1\n",
    "    elif term.lower() == 'negative':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def process_data(data):\n",
    "    # process input\n",
    "    input_text = pb.InputProcess.basic_format(prompt, data)\n",
    "    label = data['label']\n",
    "    raw_pred = model(input_text)\n",
    "    # process output\n",
    "    # pred = pb.OutputProcess.pattern_re(raw_pred, r\"##(\\d+)\")\n",
    "    pred = extract_and_map_sentiment(raw_pred)\n",
    "    # pred = pb.OutputProcess.cls(cls)\n",
    "    # print(f\"Raw: {raw_pred}, Pred: {pred}, Label: {label}\")\n",
    "    # Collect the necessary information for output\n",
    "    return (data['content'], pred, label)\n",
    "\n",
    "scores = []\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    # Initialize lists to store results\n",
    "    questions = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to process data in parallel\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Map process_data function to each item in samples\n",
    "        future_to_data = {executor.submit(process_data, data): data for data in samples}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_data), total=len(samples)):\n",
    "            data_content, pred, label = future.result()\n",
    "            questions.append(data_content)\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Assuming dataset, dataset_name, and prompt_type[idx] are defined elsewhere\n",
    "    output = list(zip(dataset, preds, labels))\n",
    "\n",
    "    # Define the output file path\n",
    "    output_file_path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/outputs/{dataset_name}_{prompt_type[idx]}.jsonl\"\n",
    "\n",
    "    # Save the paired list to a JSON file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for pair in output:\n",
    "            jsonl_file.write(json.dumps(pair) + '\\n')\n",
    "    \n",
    "    score = pb.Eval.compute_cls_accuracy(preds, labels)\n",
    "    scores.append(score)\n",
    "    print(f\"{score:.3f}\")\n",
    "\n",
    "mean_score = sum(scores) / len(scores)\n",
    "median_score = sorted(scores)[len(scores) // 2]\n",
    "std_dev = (sum([(score - mean_score) ** 2 for score in scores]) / len(scores)) ** 0.5\n",
    "print(f\"Mean: {mean_score:.3f}, Median: {median_score:.3f}, Std. Dev.: {std_dev:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pre-defined Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All supported methods: \n",
      "['CoT', 'ZSCoT', 'least_to_most', 'generated_knowledge', 'expert_prompting', 'emotion_prompt', 'baseline']\n",
      "Supported datasets for each method: \n",
      "{'CoT': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'ZSCoT': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'expert_prompting': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'emotion_prompt': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking'], 'least_to_most': ['gsm8k', 'last_letter_concat'], 'generated_knowledge': ['csqa', 'numersense', 'qasc'], 'baseline': ['gsm8k', 'csqa', 'bigbench_date', 'bigbench_object_tracking', 'last_letter_concat', 'numersense', 'qasc']}\n"
     ]
    }
   ],
   "source": [
    "# load method\n",
    "# print all methods and their supported datasets\n",
    "print('All supported methods: ')\n",
    "print(pb.SUPPORTED_METHODS)\n",
    "print('Supported datasets for each method: ')\n",
    "print(pb.METHOD_SUPPORT_DATASET)\n",
    "\n",
    "method = pb.PEMethod(method='baseline', \n",
    "                    dataset=dataset_name,\n",
    "                    verbose=True,  # if True, print the detailed prompt and response\n",
    "                    prompt_id = 1  # for emotion_prompt \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/872 [00:07<51:43,  3.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = method.test(dataset, \n",
    "                      model, \n",
    "                      num_samples=2,\n",
    "                      )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
