{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts.chat import SystemMessage, _convert_to_message\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Hierarchical Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorePrinciples:\n",
    "    def __init__(self, core_principles: List[str]):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "class WorkerAgent:\n",
    "    \"\"\"\n",
    "    Worker Agent class defining agents that provide feedback on prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, llm):\n",
    "        self.position = position\n",
    "        self.core_principles = str(core_principles)\n",
    "        self.system_message = f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\"      \n",
    "        self.llm = llm\n",
    "\n",
    "        assert isinstance(self.llm, ChatOpenAI) or isinstance(self.llm, ChatAnthropic), \"The LLM must be an instance of ChatOpenAI or ChatAnthropic.\"\n",
    "\n",
    "    def review_prompt(self, state: Sequence[BaseMessage], criteria: str):\n",
    "        \"\"\"\n",
    "        Generates a review of the prompt.\n",
    "        \"\"\"\n",
    "        prompt_text = f\"\"\"Your task is to provide feedback on the prompt in the conversation above in light of your core princples.\n",
    "You must think outside the box and consider unconventional ideas.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "You must use this information to inform your feedback.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the conversation carefully as an experienced {self.position}.\n",
    "2. Explain how you think the prompt can improved in light of your core principles.\n",
    "3. Submit your feedback.\n",
    "\"\"\"\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "        elif isinstance(self.llm, ChatAnthropic):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"user\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({\"messages\": state})       \n",
    "        return {\"messages\": [HumanMessage(content=result.content, name=self.position)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamLeaderAgent:\n",
    "    \"\"\"\n",
    "    TeamLeaderAgent class defining an agent that manages a team of worker agents to help optimise prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, prompt: str, criteria: str, llm, workforce: List[WorkerAgent]):\n",
    "        self.position = position\n",
    "        self.core_principles = str(core_principles)\n",
    "        self.system_message = f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\"\n",
    "        self.prompt = prompt\n",
    "        self.criteria = criteria\n",
    "        self.llm = llm\n",
    "        self.workforce = workforce\n",
    "        self.iterations = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the agent state.\n",
    "        \"\"\"\n",
    "        self.iterations = 0\n",
    "\n",
    "    def leader_decision(self, state: Sequence[BaseMessage]):\n",
    "        \"\"\"\n",
    "        LeaderAgent to decide the next team or to finish.\n",
    "        \"\"\"\n",
    "        self.iterations += 1\n",
    "        if self.iterations > 3:\n",
    "            self.reset()\n",
    "            return \"FINISH\"\n",
    "        else:\n",
    "            workers = [\"FINISH\"] + [worker.position for worker in self.workforce]\n",
    "            workers_details = [f\"{worker.position}:\\n{worker.core_principles}\" for worker in self.workforce]\n",
    "            # shuffle the options to avoid positional bias\n",
    "            random.shuffle(workers)\n",
    "            # options = [\"FINISH\"] + positions\n",
    "            function_def = {\n",
    "                \"name\": \"route\",\n",
    "                \"description\": \"Select the next role.\",\n",
    "                \"parameters\": {\n",
    "                    \"title\": \"routeSchema\",\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"next\": {\n",
    "                            \"title\": \"Next\",\n",
    "                            \"anyOf\": [\n",
    "                                {\"enum\": workers},\n",
    "                            ],\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"next\"],\n",
    "                },\n",
    "            }\n",
    "            prompt_text = f\"\"\"Your task is to select the next advisor to provide feedback on the prompt in the conversation above.\n",
    "Think carefully about which advisor will provide the most valuable feedback to make improvements.\n",
    "FINISH if you think your team can no longer provide valuable feedback.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{self.criteria}\n",
    "You must use this information to inform your decision.\n",
    "\n",
    "Select one of the below workers to provide feedback on how to improve the prompt or FINISH: \n",
    "{workers}\n",
    "\n",
    "The details of all workers and their core principles are as follows: \n",
    "{workers_details}\n",
    "\n",
    "Your selection process should be as follows:\n",
    "1. Read the prompt as an experienced: {self.position}. Understand its content and intent.\n",
    "2. Explicitly detail how you think the prompt can be improved or why you think the team can no longer provide valuable feedback.\n",
    "3. If improvements are needed, determine which advisor you think will provide the most valuable feedback.\n",
    "4. Submit your selection.\"\"\"\n",
    "            if isinstance(self.llm, ChatOpenAI):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"system\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "                chain = (\n",
    "                    prompt\n",
    "                    | self.llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "                    | JsonOutputFunctionsParser()\n",
    "                )\n",
    "                result = chain.invoke({\"messages\": state})\n",
    "                print(result[\"next\"])\n",
    "                return result[\"next\"]\n",
    "            elif isinstance(self.llm, ChatAnthropic):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"user\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "                try:\n",
    "                    chain = (\n",
    "                        prompt \n",
    "                        | self.llm.bind_tools(tools=[function_def])\n",
    "                    )\n",
    "                    result = chain.invoke({\"messages\": state})\n",
    "                    if \"text\" in result.content[0]:\n",
    "                        return result.content[1][\"input\"][\"next\"]\n",
    "                    else:\n",
    "                        return result.content[0][\"input\"][\"next\"]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return random.choice(workers)\n",
    "\n",
    "    def update_prompt(self, state: Sequence[BaseMessage]):\n",
    "        \"\"\"\n",
    "        Updates the prompt with the feedback from the team agent.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(state) == 1:\n",
    "            return {\"next\": self.leader_decision(state)}\n",
    "        \n",
    "        prompt_text = f\"\"\"Your task is to improve the prompt in the conversation above in light of your core principles.\n",
    "If you recieve feedback and recommendations for the prompt, respond with a revised version of your previous attempts actioning the feedback.\n",
    "\n",
    "The success criteria for the prompt are as follows:\n",
    "{self.criteria}\n",
    "You will be penalized if the prompt does not meet this criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Read the prompt as an experienced: {self.position}. Understand its content and intent.\n",
    "2. Think carefully about how you can implement the most recent feedback and revise the prompt.\n",
    "3. Submit your revised prompt.\"\"\"\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "        elif isinstance(self.llm, ChatAnthropic):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"user\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({\"messages\": state})\n",
    "        return {\"messages\": [AIMessage(content=result.content, name=self.position)], \"next\": self.leader_decision(state)}\n",
    "    \n",
    "    def approval(self, state: Sequence[BaseMessage], criteria: str):\n",
    "        \"\"\"\n",
    "        Agent to approve or reject the prompt.\n",
    "        \"\"\"\n",
    "        function_def = {\n",
    "        \"name\": \"approval\",\n",
    "        \"description\": \"Submit approval decision for the prompt.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"team\": {\"type\": \"string\", \"enum\": [self.position]},\n",
    "                \"decision\": {\"type\": \"string\", \"enum\": [\"True\", \"False\"]},\n",
    "            },\n",
    "            \"required\": [\"decision\", \"team\"],\n",
    "        },\n",
    "        }\n",
    "        prompt_text = f\"\"\"Your task is to decide if the prompt in the conversation above is optimal in light of your core principles.\n",
    "\n",
    "If you think the prompt is optimal and does not require improvements in light of your core principles, return True.\n",
    "If you think the prompt needs improvements in light of your core principles, return False.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt as an experienced: {self.position}. Understand it's content and intent.\n",
    "2. Determine whether the prompt is optimal in light of your core principles.\n",
    "3. Submit your decision.\"\"\"\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "            chain = (\n",
    "                prompt\n",
    "                | self.llm.bind_functions(functions=[function_def], function_call=\"approval\")\n",
    "                | JsonOutputFunctionsParser()\n",
    "            )\n",
    "            result = chain.invoke({\"messages\": state})\n",
    "            return result\n",
    "        elif isinstance(self.llm, ChatAnthropic):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"user\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "            chain = (\n",
    "                prompt \n",
    "                | self.llm.bind_tools(tools=[function_def])\n",
    "            )\n",
    "            result = chain.invoke({\"messages\": state})\n",
    "            if \"text\" in result.content[0]:\n",
    "                return result.content[1][\"input\"]\n",
    "            else:\n",
    "                return result.content[0][\"input\"]\n",
    "                \n",
    "    def construct_team_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of team agents based on their roles and functions.\n",
    "        \"\"\"\n",
    "\n",
    "        def worker_node(state, agent):\n",
    "            return agent.review_prompt(state[\"messages\"], self.criteria)\n",
    "        \n",
    "        def leader_node(state):\n",
    "            return self.update_prompt(state[\"messages\"]) \n",
    "        \n",
    "        # The agent state is the input to each node in the graph\n",
    "        class AgentState(TypedDict):\n",
    "            messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "            next: str\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        for worker in self.workforce:\n",
    "            # Create a node for each team agent\n",
    "            node = functools.partial(worker_node, agent=worker)\n",
    "            workflow.add_node(worker.position, node)\n",
    "        workflow.add_node(self.position, leader_node)\n",
    "\n",
    "        members = [worker.position for worker in self.workforce]\n",
    "        for member in members:\n",
    "            # We want our teams to ALWAYS \"report back\" to the leader when done\n",
    "            workflow.add_edge(member, self.position)\n",
    "        # The leader populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(self.position, lambda x: x[\"next\"], conditional_map)\n",
    "        workflow.set_entry_point(self.position)\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeaderAgent:\n",
    "    \"\"\"\n",
    "    LeaderAgent class defining an agent that generates and communicates worker agents to help optimise prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_prompt: str, criteria: str, llm, team_leaders: List[TeamLeaderAgent]):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.system_message = f\"\"\"You are an experienced: Lead AI Prompt Engineer. Your core principles are:\n",
    "- Always pay attention to detail when designing prompts\n",
    "- Always make informed decisions when designing prompts\n",
    "- Always be critical when designing prompts\"\"\"\n",
    "        self.criteria = criteria\n",
    "        self.llm = llm\n",
    "        self.team_leaders = team_leaders\n",
    "        self.iterations = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def run_approval(self, state: Sequence[BaseMessage]) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Run the approval process for the prompt. Run concurrently\n",
    "        \"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(team.approval, state, self.criteria) for team in self.team_leaders]\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        return results\n",
    "    \n",
    "    def leader_decision(self, state: Sequence[BaseMessage]):\n",
    "        \"\"\"\n",
    "        LeaderAgent to decide the next team or to finish.\n",
    "        \"\"\"\n",
    "        self.iterations += 1\n",
    "        approval_results = self.run_approval(state)\n",
    "        print(\"Approval results:\", approval_results)\n",
    "        if all(approval_result['decision'] == \"True\" for approval_result in approval_results) or self.iterations >= 4:\n",
    "            self.reset()\n",
    "            return \"FINISH\"\n",
    "        else:\n",
    "            # Only ask for advise from teams that disapproved the prompt\n",
    "            disapproved_teams = [approval_result['team'] for approval_result in approval_results if approval_result['decision'] == \"False\"]\n",
    "            disapproved_teams_details = \"\\n\".join([f\"{team.position}: {team.core_principles}\" for team in self.team_leaders if team.position in disapproved_teams])\n",
    "            # shuffle the options to avoid positional bias\n",
    "            random.shuffle(disapproved_teams)\n",
    "            # options = [\"FINISH\"] + positions\n",
    "            function_def = {\n",
    "                \"name\": \"route\",\n",
    "                \"description\": \"Select the next role.\",\n",
    "                \"parameters\": {\n",
    "                    \"title\": \"routeSchema\",\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"next\": {\n",
    "                            \"title\": \"Next\",\n",
    "                            \"anyOf\": [\n",
    "                                {\"enum\": disapproved_teams},\n",
    "                            ],\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"next\"],\n",
    "                },\n",
    "            }\n",
    "            prompt_text = f\"\"\"Your task is to choose the next team to provide feedback based on the conversation above.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{self.criteria}\n",
    "You must use this information to inform your decision.\n",
    "\n",
    "Select one of the below teams that disapproved of the previous prompt to provide feedback on how to improve it: \n",
    "{disapproved_teams}\n",
    "\n",
    "The details of all disapproving teams and their core principles are as follows: \n",
    "{disapproved_teams_details}\n",
    "\n",
    "Your selection process should be as follows:\n",
    "1. Review the prompt as an experienced: Lead AI Prompt Engineer. Understand it's content and intent.\n",
    "2. Explicitly detail how you think the prompt can be improved. Assume the prompt always needs improvement.\n",
    "3. Determine which advisor you think will provide the most valuable feedback.\n",
    "4. Submit your selection.\"\"\"\n",
    "            if isinstance(self.llm, ChatOpenAI):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"system\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "                chain = (\n",
    "                    prompt\n",
    "                    | self.llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "                    | JsonOutputFunctionsParser()\n",
    "                )\n",
    "                result = chain.invoke({\"messages\": state})\n",
    "                return result[\"next\"]\n",
    "            elif isinstance(self.llm, ChatAnthropic):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"user\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "                try: \n",
    "                    chain = (\n",
    "                        prompt \n",
    "                        | self.llm.bind_tools(tools=[function_def])\n",
    "                    )\n",
    "                    result = chain.invoke({\"messages\": state})\n",
    "                    print(result)\n",
    "                    if \"text\" in result.content[0]:\n",
    "                        return result.content[1][\"input\"][\"next\"]\n",
    "                    else:\n",
    "                        return result.content[0][\"input\"][\"next\"]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    # return random advisor if the model is not able to generate the next advisor\n",
    "                    return random.choice(disapproved_teams)\n",
    "\n",
    "    def review_prompt(self, state: Sequence[BaseMessage]) -> str:\n",
    "        \"\"\"\n",
    "        Updates the prompt with the feedback from the team agent.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(state) == 1:\n",
    "            return {\"next\": self.leader_decision(state)}\n",
    "        \n",
    "        prompt_text = f\"\"\"Your task is to check the prompt in the conversation above meets the success criteria and adheres to the strict guidelines.\n",
    "If the success criteria are not met, or the strict guidlines not followed, provide a carefully revised version of the prompt.\n",
    "\n",
    "The success criteria for the prompt are as follows:\n",
    "{self.criteria}\n",
    "\n",
    "The strict guidelines for the prompt are as follows:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces. If you wish to use curly braces in your response, use double curly braces to avoid confusion with placeholders.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Review the prompt as an experienced: Lead AI Prompt Engineer. Understand it's content and intent.\n",
    "2. Explcitly go through each success criteria and ensure the prompt meets them. If not, revise the prompt to meet them.\n",
    "3. Explicitly go through each guideline and ensure changes made adhere to them. If not, revise the prompt to adhere to them.\n",
    "4. If the prompt meets the success criteria and adheres to the strict guidelines, submit the prompt.\n",
    "\"\"\"\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "        elif isinstance(self.llm, ChatAnthropic):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"user\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({\"messages\": state})\n",
    "        return {\"messages\": [HumanMessage(content=result.content, name=\"Leader\")], \"next\": self.leader_decision(state)}\n",
    "\n",
    "    def construct_team_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of team agents based on their roles and functions.\n",
    "        \"\"\"\n",
    "\n",
    "        # The agent state is the input to each node in the graph\n",
    "        class AgentState(TypedDict):\n",
    "            messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "            next: str\n",
    "\n",
    "        def enter_chain(message: str):\n",
    "            results = {\n",
    "                \"messages\": [HumanMessage(content=message)],\n",
    "            }\n",
    "            return results\n",
    "        \n",
    "        def get_last_message(state: AgentState) -> str:\n",
    "            return state[\"messages\"][-1].content\n",
    "\n",
    "        def join_graph(response: dict):\n",
    "            return {\"messages\": [response[\"messages\"][-1]]}\n",
    "\n",
    "        def leader_node(state):\n",
    "            return self.review_prompt(state[\"messages\"])\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        for team in self.team_leaders:\n",
    "            # Create a node for each team agent\n",
    "            graph = team.construct_team_graph()\n",
    "            chain = enter_chain | graph\n",
    "            workflow.add_node(team.position, get_last_message | chain | join_graph)\n",
    "        workflow.add_node(\"Leader\", leader_node)\n",
    "\n",
    "        members = [team.position for team in self.team_leaders]\n",
    "        for member in members:\n",
    "            # We want our teams to ALWAYS \"report back\" to the leader when done\n",
    "            workflow.add_edge(member, \"Leader\")\n",
    "        # The leader populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(\"Leader\", lambda x: x[\"next\"], conditional_map)\n",
    "        workflow.set_entry_point(\"Leader\")\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    def optimise_prompt(self):\n",
    "        \"\"\"\n",
    "        Optimises a prompt by invoking a graph of worker agents.\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=f\"{self.base_prompt}\", name=\"User\")],\n",
    "        }\n",
    "\n",
    "        # Construct the graph\n",
    "        graph = self.construct_team_graph()\n",
    "        # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        n = random.randint(0, 1000)\n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": str(n)},\n",
    "            \"recursion_limit\": 50,\n",
    "            }    \n",
    "\n",
    "        # Run the graph\n",
    "        for s in graph.stream(\n",
    "            initial_state,\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "            ):\n",
    "            if \"__end__\" not in s:\n",
    "                if len(s[\"messages\"]) > 1:\n",
    "                    s[\"messages\"][-1].pretty_print()\n",
    "                continue\n",
    "\n",
    "        def message_to_dict(obj):\n",
    "            if isinstance(obj, HumanMessage) or isinstance(obj, AIMessage):\n",
    "                return {obj.name: obj.content}\n",
    "            raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
    "\n",
    "        if type(self.llm) == ChatOpenAI:\n",
    "            model = self.llm.model_name\n",
    "        else:\n",
    "            model = self.llm.model\n",
    "        temp = int(self.llm.temperature)\n",
    "        path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/conversations/{model}/conversations_hierarchcial_{temp}.json\"\n",
    "        if not os.path.exists(path):\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump([], f)\n",
    "        \n",
    "        with open(path, \"r\") as f:\n",
    "            # write messages to json file\n",
    "            data = json.load(f)\n",
    "            # get the current key number then increment it\n",
    "            key = len(data)\n",
    "            data.append({key: json.dumps(s, default=message_to_dict)})\n",
    "            \n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "                \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatAnthropic(temperature=0, model=\"claude-3-haiku-20240307\")\n",
    "# llm = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-20240620\")\n",
    "# llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "# llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "# llm = ChatOllama(temperature=1, model=\"mistral:v0.3\")\n",
    "# llm = ChatOllama(temperature=0, model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import gsm8k, human_eval, sst2\n",
    "\n",
    "human_eval_baseline = human_eval.get_baseline_prompt()\n",
    "human_eval_criteria = human_eval.get_criteria()\n",
    "\n",
    "gsm8k_baseline = gsm8k.get_baseline_prompt()\n",
    "gsm8k_criteria = gsm8k.get_criteria()\n",
    "\n",
    "sst2_baseline_prompt = sst2.get_baseline_prompt()\n",
    "sst2_criteria = sst2.get_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_suite import PromptDesignAgents, HumanEvalAgents, GSM8kAgents, SST2Agents\n",
    "\n",
    "prompt_design_agents = PromptDesignAgents()\n",
    "\n",
    "style_and_structure_expert = WorkerAgent(\"Style_and_Structure_Expert\", CorePrinciples(prompt_design_agents.get_style_and_structure_principles()), llm)\n",
    "conciseness_and_clarity_expert = WorkerAgent(\"Conciseness_and_Clarity_Expert\", CorePrinciples(prompt_design_agents.get_conciseness_and_clarity_principles()), llm)\n",
    "contextual_relevance_expert = WorkerAgent(\"Contextual_Relevance_Expert\", CorePrinciples(prompt_design_agents.get_contextual_relevance_principles()), llm)\n",
    "task_alignment_expert = WorkerAgent(\"Task_Alignment_Expert\", CorePrinciples(prompt_design_agents.get_task_alignment_principles()), llm)\n",
    "example_demonstration_expert = WorkerAgent(\"Example_Demonstration_Expert\", CorePrinciples(prompt_design_agents.get_example_demonstration_principles()), llm)\n",
    "incremental_prompting_expert = WorkerAgent(\"Incremental_Prompting_Expert\", CorePrinciples(prompt_design_agents.get_incremental_prompting_principles()), llm)\n",
    "\n",
    "human_eval_agents = HumanEvalAgents()\n",
    "\n",
    "code_reviewer = WorkerAgent(\"Code_Reviewer\", CorePrinciples(human_eval_agents.get_code_reviewer_principles()), llm)\n",
    "software_engineer = WorkerAgent(\"Software_Engineer\", CorePrinciples(human_eval_agents.get_software_engineering_principles()), llm)\n",
    "software_architect = WorkerAgent(\"Software_Architect\", CorePrinciples(human_eval_agents.get_software_architecture_principles()), llm)\n",
    "\n",
    "gsm8k_agents = GSM8kAgents()\n",
    "\n",
    "mathematician = WorkerAgent(\"Mathematician\", CorePrinciples(gsm8k_agents.get_mathematician_principles()), llm)\n",
    "word_problem_solver = WorkerAgent(\"Word_Problem_Solver\", CorePrinciples(gsm8k_agents.get_word_problem_solver_principles()), llm)\n",
    "\n",
    "sst2_agents = SST2Agents()\n",
    "\n",
    "graded_sentiment_analyst = WorkerAgent(\"Graded_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_graded_sentiment_analyst_principles()), llm)\n",
    "emotive_sentiment_analyst = WorkerAgent(\"Emotive_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_emotive_sentiment_analyst_principles()), llm)\n",
    "aspect_based_sentiment_analyst = WorkerAgent(\"Aspect_Based_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_aspect_based_sentiment_analyst_principles()), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"human_eval\"\n",
    "if dataset == \"human_eval\":\n",
    "    baseline_prompt = human_eval_baseline\n",
    "    criteria = human_eval_criteria\n",
    "    domain_experts = [software_engineer, software_architect, code_reviewer]\n",
    "    domain_leader = \"Lead_Software_Engineer\"\n",
    "    domain_principles = CorePrinciples([human_eval_agents.get_lead_software_engineer_principles()])\n",
    "elif dataset == \"gsm8k\":\n",
    "    baseline_prompt = gsm8k_baseline\n",
    "    criteria = gsm8k_criteria\n",
    "    domain_experts = [mathematician, word_problem_solver]\n",
    "    domain_leader = \"Lead_Mathematician\"\n",
    "    domain_principles = CorePrinciples([gsm8k_agents.get_lead_mathematician_principles()])\n",
    "elif dataset == \"sst2\":\n",
    "    baseline_prompt = sst2_baseline_prompt\n",
    "    criteria = sst2_criteria\n",
    "    domain_experts = [graded_sentiment_analyst, emotive_sentiment_analyst, aspect_based_sentiment_analyst]\n",
    "    domain_leader = \"Lead_Sentiment_Analyst\"\n",
    "    domain_principles = CorePrinciples([sst2_agents.get_lead_sentiment_analyst_principles()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams and members:\n",
      "Lead_Prompt_Writer team:\n",
      "Position: Style_and_Structure_Expert, Core Principles: \n",
      "- Always structure prompts logically for the task\n",
      "- Always use a style and tone in prompts that is appropriate for the task\n",
      "- Always assign a role to the language model that is relevant to the task\n",
      "Position: Conciseness_and_Clarity_Expert, Core Principles: \n",
      "- Always write clear and concise prompts\n",
      "- Always use simple and direct language in prompts\n",
      "- Always avoid ambiguity in prompts\n",
      "Position: Contextual_Relevance_Expert, Core Principles: \n",
      "- Always provide context to help the model understand the task\n",
      "- Always write prompts informed by the context of the task\n",
      "- Always design contextually relevant roles for the language model\n",
      "Position: Task_Alignment_Expert, Core Principles: \n",
      "- Always write prompts that align with the task criteria\n",
      "- Always tailor instructions to the task to guide the model\n",
      "- Always make the task abundantly clear to the model in the prompt\n",
      "Position: Example_Demonstration_Expert, Core Principles: \n",
      "- Always provide examples to help the model understand the task\n",
      "- Always provide examples that cover a range of complexities\n",
      "- Always demonstrate the expected output of the model\n",
      "Position: Incremental_Prompting_Expert, Core Principles: \n",
      "- Always break-down complex tasks\n",
      "- Always write clear step-by-step instructions to guide the model\n",
      "- Always write instructions appropriate for the task complexity\n",
      "----\n",
      "Lead_Software_Engineer team:\n",
      "Position: Software_Engineer, Core Principles: \n",
      "- Always follow best practices in software engineering\n",
      "- Always write clean and maintainable code\n",
      "- Always consider the performance implications of code\n",
      "Position: Software_Architect, Core Principles: \n",
      "- Always design software architecture that is scalable and maintainable\n",
      "- Always consider the trade-offs of different architectural patterns\n",
      "- Always document software architecture decisions\n",
      "Position: Code_Reviewer, Core Principles: \n",
      "- Always review code for errors and inefficiencies\n",
      "- Always consider edge cases in code\n",
      "- Always provide constructive feedback in code reviews\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "prompt_design_team = TeamLeaderAgent(\n",
    "    \"Lead_Prompt_Writer\",\n",
    "    CorePrinciples([prompt_design_agents.get_lead_prompt_writer_principles()]),\n",
    "    baseline_prompt,\n",
    "    criteria,\n",
    "    workforce=[\n",
    "        style_and_structure_expert,\n",
    "        conciseness_and_clarity_expert,\n",
    "        contextual_relevance_expert,\n",
    "        task_alignment_expert,\n",
    "        example_demonstration_expert,\n",
    "        incremental_prompting_expert,\n",
    "    ],\n",
    "    llm=llm\n",
    ")\n",
    "domain_team = TeamLeaderAgent(\n",
    "    domain_leader,\n",
    "    domain_principles,\n",
    "    baseline_prompt,\n",
    "    criteria,\n",
    "    workforce=domain_experts,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "leader_agent = LeaderAgent(\n",
    "    base_prompt=baseline_prompt,\n",
    "    criteria=criteria,\n",
    "    team_leaders=[prompt_design_team, domain_team],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# leader_agent.generate_teams()\n",
    "# print teams and members\n",
    "print(\"Teams and members:\")\n",
    "for team_leader in leader_agent.team_leaders:\n",
    "    print(f\"{team_leader.position} team:\")\n",
    "    for worker in team_leader.workforce:\n",
    "        print(f\"Position: {worker.position}, Core Principles: \\n{worker.core_principles}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approval results: [{'team': 'Lead_Prompt_Writer', 'decision': 'False'}, {'team': 'Lead_Software_Engineer', 'decision': 'False'}]\n",
      "Task_Alignment_Expert\n",
      "Conciseness_and_Clarity_Expert\n",
      "FINISH\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Lead_Prompt_Writer\n",
      "\n",
      "### Revised Prompt\n",
      "\n",
      "Complete the function below using its signature and docstring. Ensure it is fully functional and handles edge cases.\n",
      "\n",
      "Function signature and docstring:\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "\n",
      "Output the completed function as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "Approval results: [{'team': 'Lead_Software_Engineer', 'decision': 'False'}, {'team': 'Lead_Prompt_Writer', 'decision': 'False'}]\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Leader\n",
      "\n",
      "### Review of the Prompt\n",
      "\n",
      "#### Success Criteria Check:\n",
      "1. **Instructs the LLM to complete a function based on its signature and docstring:**\n",
      "   - Yes, the prompt clearly instructs the LLM to complete the function using its signature and docstring.\n",
      "   \n",
      "2. **Includes the content placeholder:**\n",
      "   - Yes, the placeholder `{content}` is included in the prompt.\n",
      "   \n",
      "3. **Instructs the model to output the answer at the end as ```python <your answer> ``` :**\n",
      "   - Yes, the prompt instructs the model to output the answer in the specified format.\n",
      "\n",
      "#### Guidelines Check:\n",
      "1. **DO NOT modify existing restrictions:**\n",
      "   - No existing restrictions were modified.\n",
      "   \n",
      "2. **DO NOT modify or remove negations:**\n",
      "   - No negations were modified or removed.\n",
      "   \n",
      "3. **DO NOT add, modify or remove placeholders denoted by curly braces:**\n",
      "   - The placeholder `{content}` was not modified or removed.\n",
      "   \n",
      "4. **ALWAYS treat placeholders as the actual content:**\n",
      "   - The placeholder `{content}` is treated as the actual content.\n",
      "\n",
      "### Conclusion:\n",
      "The prompt meets all the success criteria and adheres to the strict guidelines. Therefore, it is ready for submission.\n",
      "\n",
      "### Final Prompt:\n",
      "```plaintext\n",
      "Complete the function below using its signature and docstring. Ensure it is fully functional and handles edge cases.\n",
      "\n",
      "Function signature and docstring:\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "\n",
      "Output the completed function as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "```\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Lead_Prompt_Writer\n",
      "\n",
      "### Revised Prompt\n",
      "\n",
      "Complete the function below using its signature and docstring. Ensure it is fully functional and handles edge cases.\n",
      "\n",
      "Function signature and docstring:\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "\n",
      "Output the completed function as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "Approval results: [{'team': 'Lead_Software_Engineer', 'decision': 'True'}, {'team': 'Lead_Prompt_Writer', 'decision': 'True'}]\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Leader\n",
      "\n",
      "### Review of the Prompt\n",
      "\n",
      "#### Success Criteria Check:\n",
      "1. **Instructs the LLM to complete a function based on its signature and docstring:**\n",
      "   - Yes, the prompt clearly instructs the LLM to complete the function using its signature and docstring.\n",
      "   \n",
      "2. **Includes the content placeholder:**\n",
      "   - Yes, the placeholder `{content}` is included in the prompt.\n",
      "   \n",
      "3. **Instructs the model to output the answer at the end as ```python <your answer> ``` :**\n",
      "   - Yes, the prompt instructs the model to output the answer in the specified format.\n",
      "\n",
      "#### Guidelines Check:\n",
      "1. **DO NOT modify existing restrictions:**\n",
      "   - No existing restrictions were modified.\n",
      "   \n",
      "2. **DO NOT modify or remove negations:**\n",
      "   - No negations were modified or removed.\n",
      "   \n",
      "3. **DO NOT add, modify or remove placeholders denoted by curly braces:**\n",
      "   - The placeholder `{content}` was not modified or removed.\n",
      "   \n",
      "4. **ALWAYS treat placeholders as the actual content:**\n",
      "   - The placeholder `{content}` is treated as the actual content.\n",
      "\n",
      "### Conclusion:\n",
      "The prompt meets all the success criteria and adheres to the strict guidelines. Therefore, it is ready for submission.\n",
      "\n",
      "### Final Prompt:\n",
      "```plaintext\n",
      "Complete the function below using its signature and docstring. Ensure it is fully functional and handles edge cases.\n",
      "\n",
      "Function signature and docstring:\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "\n",
      "Output the completed function as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "```\n",
      "Time taken: 38.05657982826233\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Lead_Prompt_Writer\n",
      "\n",
      "### Revised Prompt\n",
      "\n",
      "Complete the function below using its signature and docstring. Ensure it is fully functional and handles edge cases.\n",
      "\n",
      "Function signature and docstring:\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "\n",
      "Output the completed function as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for _ in range(1):\n",
    "    start = time.time()\n",
    "    result = leader_agent.optimise_prompt()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    print(f\"Time taken: {end - start}\")\n",
    "    result[\"messages\"][-2].pretty_print()\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Leader\n",
      "\n",
      "### Review of the Prompt\n",
      "\n",
      "#### Success Criteria Check:\n",
      "1. **Instructs the LLM to complete a function based on its signature and docstring:**\n",
      "   - Yes, the prompt clearly instructs the LLM to complete the function using its signature and docstring.\n",
      "   \n",
      "2. **Includes the content placeholder:**\n",
      "   - Yes, the placeholder `{content}` is included in the prompt.\n",
      "   \n",
      "3. **Instructs the model to output the answer at the end as ```python <your answer> ``` :**\n",
      "   - Yes, the prompt instructs the model to output the answer in the specified format.\n",
      "\n",
      "#### Guidelines Check:\n",
      "1. **DO NOT modify existing restrictions:**\n",
      "   - No existing restrictions were modified.\n",
      "   \n",
      "2. **DO NOT modify or remove negations:**\n",
      "   - No negations were modified or removed.\n",
      "   \n",
      "3. **DO NOT add, modify or remove placeholders denoted by curly braces:**\n",
      "   - The placeholder `{content}` was not modified or removed.\n",
      "   \n",
      "4. **ALWAYS treat placeholders as the actual content:**\n",
      "   - The placeholder `{content}` is treated as the actual content.\n",
      "\n",
      "### Conclusion:\n",
      "The prompt meets all the success criteria and adheres to the strict guidelines. Therefore, it is ready for submission.\n",
      "\n",
      "### Final Prompt:\n",
      "```plaintext\n",
      "Complete the function below using its signature and docstring. Ensure it is fully functional and handles edge cases.\n",
      "\n",
      "Function signature and docstring:\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "\n",
      "Output the completed function as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max time: \", max(times))\n",
    "print(\"Min time: \", min(times))\n",
    "print(\"Average time: \", sum(times) / len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
