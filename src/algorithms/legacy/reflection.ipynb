{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Reflection Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessage, _convert_to_message\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Reflection Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o\")\n",
    "# llm = ChatAnthropic(temperature=1.0, model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "def self_reflection_graph(criteria) -> MessageGraph:\n",
    "    \"\"\"\n",
    "    Constructs a graph for self-reflection and improvement of prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    system_message = f\"\"\"You are an experienced: AI Prompt Engineer. Your core principles are:\n",
    "- Always write clear and concise prompts.\n",
    "- Always write contextually relevant prompts.\n",
    "- Always write task aligned prompts.\n",
    "- Always write example demonstrations in prompts.\n",
    "- Always format and structure prompts to be easily understood by the model.\"\"\"\n",
    "\n",
    "    def generation_node(state: Sequence[BaseMessage]):\n",
    "        prompt_text = f\"\"\"Your task is to improve the prompt in the conversation above in light of your core principles.\n",
    "If you recieve feedback and recommendations for the prompt, respond with a revised version of your previous attempts actioning the feedback.\n",
    "Always think outside the box and consider unconventional ideas on how to implement the feedback.\n",
    "\n",
    "The success criteria for the prompt are as follows:\n",
    "{criteria}\n",
    "You will be penalized if the prompt does not meet this criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Review the conversation carefully as an experienced AI Prompt Engineer.\n",
    "2. Think carefully about how you can implement the most recent feedback and revise the prompt.\n",
    "3. Explcitly go through each success criteria and ensure the prompt meets them. If not, revise the prompt to make sure it does.\n",
    "4. Explicitly go through each guideline and ensure the changes adhere to them. If not, revise the prompt to make sure it does.\n",
    "5. Submit your revised prompt.\"\"\"\n",
    "        messages = [\n",
    "            (\"system\", system_message),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\"system\", prompt_text),\n",
    "        ]\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        chain = prompt | llm\n",
    "        return chain.invoke({\"messages\": state})\n",
    "        \n",
    "    def reflection_node(state: Sequence[BaseMessage]):\n",
    "        prompt_text = f\"\"\"Your task is to provide feedback on the prompt in the conversation above in light of your core princples.\n",
    "Always think outside the box and consider unconventional ideas on how to enforce your core principles in the prompt.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "You must use this information to inform your feedback.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the conversation carefully as an experienced: AI Prompt Engineer.\n",
    "2. Explain how you think the prompt can improved in light of your core principles.\n",
    "3. Submit your feedback.\"\"\"\n",
    "        messages = [\n",
    "            (\"system\", system_message),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\"system\", prompt_text),\n",
    "        ]\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        chain = prompt | llm\n",
    "        result = chain.invoke({\"messages\": state})\n",
    "        return HumanMessage(content=result.content)\n",
    "\n",
    "    builder = MessageGraph()\n",
    "    builder.add_node(\"generate\", generation_node)\n",
    "    builder.add_node(\"reflect\", reflection_node)\n",
    "    builder.set_entry_point(\"generate\")\n",
    "\n",
    "    def approval(state: Sequence[BaseMessage], criteria: str):\n",
    "        \"\"\"\n",
    "        Agent to approve or reject the prompt.\n",
    "        \"\"\"\n",
    "        function_def = {\n",
    "        \"name\": \"approval\",\n",
    "        \"description\": \"Submit approval decision for the prompt.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"decision\": {\"type\": \"string\", \"enum\": [\"True\", \"False\"]},\n",
    "            },\n",
    "            \"required\": [\"decision\"],\n",
    "        },\n",
    "        }\n",
    "        prompt_text = f\"\"\"Your task is to review the conversation above and decide if the prompt is optimal in light of your core principles and the success criteria.\n",
    "\n",
    "The success criteria for the prompt are as follows:\n",
    "{criteria}\n",
    "You must use this information to inform your decision.\n",
    "\n",
    "If you think the prompt sufficiently meets the success criteria, return True. \n",
    "If you think the prompt needs improvements in light of your core principles to better meet the success criteria, return False.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt carefully as an expert AI Prompt Engineer.\n",
    "2. Determine whether the prompt needs improvements or meets the success criteria.\n",
    "3. Submit your decision.\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            (\"system\", system_message),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\"system\", prompt_text),\n",
    "        ]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "        chain = (\n",
    "            prompt\n",
    "            | llm.bind_functions(functions=[function_def], function_call=\"approval\")\n",
    "            | JsonOutputFunctionsParser()\n",
    "        )\n",
    "        result = chain.invoke({\"messages\": state})\n",
    "        return result\n",
    "\n",
    "    def should_continue(state: List[BaseMessage]):\n",
    "        approval_result = approval(state, criteria)\n",
    "        print(approval_result)\n",
    "        if approval_result[\"decision\"] == \"True\" or len(state) > 12:\n",
    "            return END\n",
    "        return \"reflect\"\n",
    "\n",
    "    builder.add_conditional_edges(\"generate\", should_continue)\n",
    "    builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "    memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def update_prompt(base_prompt: str, criteria: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses self_reflection_graph to iteratively act on feedback and update prompt\n",
    "    \"\"\"\n",
    "    graph = self_reflection_graph(criteria)\n",
    "    input = HumanMessage(content=base_prompt, name=\"User\")\n",
    "    n = random.randint(0, 1000)\n",
    "    config = {\n",
    "        \"configurable\": {\"thread_id\": n},\n",
    "        \"recursion_limit\": 50,\n",
    "        }    \n",
    "\n",
    "    # Run the graph\n",
    "    for s in graph.stream(\n",
    "        input,\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "        ):\n",
    "        if \"__end__\" not in s:\n",
    "            # if len(s) > 1:\n",
    "            #     s[-1].pretty_print()\n",
    "            continue\n",
    "        \n",
    "    def message_to_dict(obj):\n",
    "        if isinstance(obj, HumanMessage) or isinstance(obj, AIMessage):\n",
    "            return {obj.name: obj.content}\n",
    "        raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
    "    \n",
    "    if not os.path.exists(\"conversations_reflection.json\"):\n",
    "        with open(\"conversations_reflection.json\", \"w\") as f:\n",
    "            json.dump([], f)\n",
    "    \n",
    "    with open(\"conversations_reflection.json\", \"r\") as f:\n",
    "        # write messages to json file\n",
    "        data = json.load(f)\n",
    "        # get the current key number then increment it\n",
    "        key = len(data)\n",
    "        data.append({key: json.dumps(s, default=message_to_dict)})\n",
    "        \n",
    "    with open(\"conversations_reflection.json\", \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval_prompts import HumanEvalPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_prompts = HumanEvalPrompts()\n",
    "baseline_prompt = human_eval_prompts.get_baseline_prompt()\n",
    "criteria = human_eval_prompts.get_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented according to the given details.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented and follows the given specifications.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is the revised prompt based on the given criteria and guidelines:\n",
      "\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "Please complete the function based on its signature and docstring. Output your answer at the end as:\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented and follows the given specifications.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented according to the given details.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is the revised prompt based on the given criteria and guidelines:\n",
      "\n",
      "```python\n",
      "{content}\n",
      "```\n",
      "Please complete the function based on its signature and docstring. Output your answer at the end as ```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented and follows the given specifications.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented according to the given details.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented according to the given details.\n",
      "{content}\n",
      "\n",
      "# Output your answer at the end as\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n",
      "{'decision': 'True'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```python\n",
      "# Complete the function based on its signature and docstring provided below.\n",
      "# Ensure the function is correctly implemented according to the given details.\n",
      "# Output your answer at the end as ```python\n",
      "<your answer>\n",
      "```\n",
      "\n",
      "{content}\n",
      "```python\n",
      "<your answer>\n",
      "```\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "    result = update_prompt(baseline_prompt, criteria)    \n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    result[-1].pretty_print()\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
