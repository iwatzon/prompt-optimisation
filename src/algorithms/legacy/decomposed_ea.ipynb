{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposed Evo Algo Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_json(response):\n",
    "    # Retrieve json from response\n",
    "    pattern = rf\"```json\\s+(.*?)\\s+```\"\n",
    "    json_block = re.search(pattern, response.content, re.DOTALL) # Using re.DOTALL to make the dot match newlines as well\n",
    "    json_code = None\n",
    "    if json_block:\n",
    "        extracted_json = json_block.group(1)\n",
    "        json_code = extracted_json\n",
    "    return json_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Prompt Pool Generation\n",
    "\n",
    "An agent that generates an initial pool of domain specific prompts to initiate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinkingStylesAgent:\n",
    "\n",
    "    def __init__(self, n: int, base_prompt: str):\n",
    "        self.n = n\n",
    "        self.base_prompt = base_prompt\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Agent that generates n different prompts with different thinking styles to approach a probelm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load open source language model\n",
    "        # model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        # llm = HuggingFaceEndpoint(\n",
    "        #     repo_id=model,\n",
    "        #     huggingfacehub_api_token=hf_api_token\n",
    "        # )\n",
    "\n",
    "        # Load openai language model\n",
    "        llm = ChatOpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0.5,\n",
    "            model='gpt-4o'\n",
    "        )\n",
    "\n",
    "        # Agent instruction\n",
    "        instruction = f\"\"\"\n",
    "        You are an expert thinker and problem solver. Here is a base prompt: {self.base_prompt}\n",
    "        Generate {self.n} prompts to elaborate on the base prompt, each with a unique thinking style. \n",
    "        Consider the context of the problem and the end user (an LLM) of the prompt.\n",
    "        \n",
    "        The output should be json code containing {self.n} prompts with the following format:\n",
    "        {{\n",
    "            \"style 1\": {{\n",
    "                \"prompt\": \"prompt content\",\n",
    "                \"thinking style\": \"thinking style\",\n",
    "            }},\n",
    "            \"style 2\": {{\n",
    "                \"prompt\": \"prompt content\",\n",
    "                \"thinking style\": \"thinking style\",\n",
    "            }},\n",
    "            ...\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate prompts\n",
    "        model_output = llm.invoke(instruction)\n",
    "        json_output = get_json(model_output)\n",
    "\n",
    "        if json_output is None:\n",
    "            json_output = model_output.content\n",
    "\n",
    "        return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iwatson/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "base_prompt = \"Write a function to calculate the factorial of a number.\"\n",
    "thinking_styles_agent = ThinkingStylesAgent(10, base_prompt)\n",
    "thinking_styles = thinking_styles_agent.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'style 1': {'prompt': 'Write a function in Python to calculate the factorial of a number using a recursive approach. Ensure the function handles edge cases like zero and negative numbers.',\n",
       "  'thinking style': 'analytical'},\n",
       " 'style 2': {'prompt': 'Create a Python function that computes the factorial of a given number. Use an iterative approach and explain why you chose this method over recursion.',\n",
       "  'thinking style': 'comparative'},\n",
       " 'style 3': {'prompt': 'Design a Python function to calculate the factorial of a number. Include comments in your code to explain each step and make it easy to understand for beginners.',\n",
       "  'thinking style': 'pedagogical'},\n",
       " 'style 4': {'prompt': 'Implement a Python function that calculates the factorial of a number. Consider optimizing the function for large numbers and discuss the trade-offs involved.',\n",
       "  'thinking style': 'optimization-focused'},\n",
       " 'style 5': {'prompt': 'Write a Python function to compute the factorial of a number. Include error handling to manage invalid inputs such as non-integer and negative values.',\n",
       "  'thinking style': 'robustness-focused'},\n",
       " 'style 6': {'prompt': 'Develop a Python function that calculates the factorial of a number. Incorporate unit tests to verify the accuracy of your function for various inputs.',\n",
       "  'thinking style': 'test-driven'},\n",
       " 'style 7': {'prompt': 'Create a Python function to find the factorial of a number. Use memoization to improve the efficiency of your solution and explain how it works.',\n",
       "  'thinking style': 'efficiency-focused'},\n",
       " 'style 8': {'prompt': 'Write a Python function that calculates the factorial of a number. Compare the performance of your function with the built-in math.factorial function in Python.',\n",
       "  'thinking style': 'performance-comparative'},\n",
       " 'style 9': {'prompt': 'Implement a Python function to compute the factorial of a number. Provide a detailed explanation of the mathematical concept of factorial and its applications.',\n",
       "  'thinking style': 'conceptual'},\n",
       " 'style 10': {'prompt': 'Design a Python function that calculates the factorial of a number. Visualize the process of calculating the factorial using a flowchart or diagram.',\n",
       "  'thinking style': 'visual'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thinking_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://arxiv.org/pdf/2309.16797.pdf [pg. 21] \n",
    "\n",
    "thinking_styles = [\n",
    "    \"How could I devise an experiment to help solve that problem?\",\n",
    "    \"Make a list of ideas for solving this problem, and apply them one by one to the problem to see if any progress can be made.\",\n",
    "    \"How could I measure progress on this problem?\",\n",
    "    \"How can I simplify the problem so that it is easier to solve?\",\n",
    "    \"What are the key assumptions underlying this problem?\",\n",
    "    \"What are the potential risks and drawbacks of each solution?\",\n",
    "    \"What are the alternative perspectives or viewpoints on this problem?\",\n",
    "    \"What are the long-term implications of this problem and its solutions?\",\n",
    "    \"How can I break down this problem into smaller, more manageable parts?\",\n",
    "    \"Critical Thinking: This style involves analyzing the problem from different perspectives, questioning assumptions, and evaluating the evidence or information available. It focuses on logical reasoning, evidence-based decision-making, and identifying potential biases or flaws in thinking.\",\n",
    "    \"Try creative thinking, generate innovative and out-of-the-box ideas to solve the problem. Explore unconventional solutions, thinking beyond traditional boundaries, and encouraging imagination and originality.\",\n",
    "    \"Seek input and collaboration from others to solve the problem. Emphasize teamwork, open communication, and leveraging the diverse perspectives and expertise of a group to come up with effective solutions.\",\n",
    "    \"Use systems thinking: Consider the problem as part of a larger system and understanding the interconnectedness of various elements. Focuses on identifying the underlying causes, feedback loops, and interdependencies that influence the problem, and developing holistic solutions that address the system as a whole.\",\n",
    "    \"Use Risk Analysis: Evaluate potential risks, uncertainties, and tradeoffs associated with different solutions or approaches to a problem. Emphasize assessing the potential consequences and likelihood of success or failure, and making informed decisions based on a balanced analysis of risks and benefits.\",\n",
    "    \"Use Reflective Thinking: Step back from the problem, take the time for introspection and self-reflection. Examine personal biases, assumptions, and mental models that may influence problem-solving, and being open to learning from past experiences to improve future approaches.\",\n",
    "    \"What is the core issue or problem that needs to be addressed?\",\n",
    "    \"What are the underlying causes or factors contributing to the problem?\",\n",
    "    \"Are there any potential solutions or strategies that have been tried before? If yes, what were the outcomes and lessons learned?\",\n",
    "    \"What are the potential obstacles or challenges that might arise in solving this problem?\",\n",
    "    \"Are there any relevant data or information that can provide insights into the problem? If yes, what data sources are available, and how can they be analyzed?\",\n",
    "    \"Are there any stakeholders or individuals who are directly affected by the problem? What are their perspectives and needs?\",\n",
    "    \"What resources (financial, human, technological, etc.) are needed to tackle the problem effectively?\",\n",
    "    \"How can progress or success in solving the problem be measured or evaluated?\",\n",
    "    \"What indicators or metrics can be used?\",\n",
    "    \"Is the problem a technical or practical one that requires a specific expertise or skill set? Or is it more of a conceptual or theoretical problem?\",\n",
    "    \"Does the problem involve a physical constraint, such as limited resources, infrastructure, or space?\",\n",
    "    \"Is the problem related to human behavior, such as a social, cultural, or psychological issue?\",\n",
    "    \"Does the problem involve decision-making or planning, where choices need to be made under uncertainty or with competing objectives?\",\n",
    "    \"Is the problem an analytical one that requires data analysis, modeling, or optimization techniques?\",\n",
    "    \"Is the problem a design challenge that requires creative solutions and innovation?\",\n",
    "    \"Does the problem require addressing systemic or structural issues rather than just individual instances?\",\n",
    "    \"Is the problem time-sensitive or urgent, requiring immediate attention and action?\",\n",
    "    \"What kinds of solution typically are produced for this kind of problem specification?\",\n",
    "    \"Given the problem specification and the current best solution, have a guess about other possible solutions.\",\n",
    "    \"Let's imagine the current best solution is totally wrong, what other ways are there to think about the problem specification?\",\n",
    "    \"What is the best way to modify this current best solution, given what you know about these kinds of problem specification?\",\n",
    "    \"Ignoring the current best solution, create an entirely new solution to the problem.\",\n",
    "    \"Let's think step by step.\",\n",
    "    \"Let's make a step by step plan and implement it with good notion and explanation.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PromptPoolGeneration:\n",
    "#     \"\"\"\n",
    "#     Use a selection of thinking styles to generate a pool of prompts for a given problem.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, task: str, thinking_styles: dict, n: int):\n",
    "#         self.task = task\n",
    "#         self.thinking_styles = thinking_styles\n",
    "#         self.n = n\n",
    "\n",
    "#     def generate(self, previous_cycle_results: dict={}):\n",
    "#         \"\"\"\n",
    "#         Generate n prompts based on the thinking styles.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Load openai language model\n",
    "#         llm = ChatOpenAI(\n",
    "#             api_key=openai_api_key,\n",
    "#             temperature=0.5,\n",
    "#             model='gpt-4o'\n",
    "#         )\n",
    "\n",
    "#         # Agent instruction\n",
    "#         instruction = f\"\"\"\n",
    "#         You are an expert prompt engineer. Generate a pool of {self.n} prompts, based on a selection of the following thinking styles: {self.thinking_styles}. \n",
    "#         The prompts should direct an langauge model to perform the following task: {self.task}.\n",
    "#         If avaliable, the pool of prompts must include the best mutated prompts from the following previous cycle results exactly as presented and use the feedback to inform the generation of new prompts: {previous_cycle_results}.\n",
    "        \n",
    "#         The output must be json code containing {self.n} prompts (including any mutated prompts from the previous cycle) with the following format: \n",
    "#         {{\n",
    "#             \"prompt 1\": \"prompt content\",\n",
    "#             \"prompt 2\": \"prompt content\",\n",
    "#             ...\n",
    "#         }}\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Generate prompts\n",
    "#         model_output = llm.invoke(instruction)\n",
    "#         json_output = get_json(model_output)\n",
    "\n",
    "#         if json_output is None:\n",
    "#             json_output = model_output.content\n",
    "\n",
    "#         return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem = \"write a function to calculate the factorial of a number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_pool_generation = PromptPoolGeneration(problem, thinking_styles, 3)\n",
    "# prompts = prompt_pool_generation.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Selective Breeding\n",
    "\n",
    "Task an LLM with pairing prompts it believes will produce effective prompts as offspring (now merged with stage 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MatchMakerAgent:\n",
    "#     \"\"\"\n",
    "#     Agent that pairs two prompts for breeding new ideas.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, prompts: dict, n: int):\n",
    "#         self.prompts = prompts\n",
    "#         self.n = n\n",
    "\n",
    "#     def generate(self):\n",
    "#         \"\"\"\n",
    "#         Generate pairs of prompts.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Load openai language model\n",
    "#         llm = ChatOpenAI(\n",
    "#             api_key=openai_api_key,\n",
    "#             temperature=0.5,\n",
    "#             model='gpt-4o'\n",
    "#         )\n",
    "\n",
    "#         # Agent instruction\n",
    "#         instruction = f\"\"\"\n",
    "#         You are an expert thinker and problem solver. Generate {self.n} pairs of parent prompts from the following list of prompts: {self.prompts}.\n",
    "#         Think carefully about how the prompts can be combined to create new ideas. Justify why you think the prompts are a good match. \n",
    "#         Reminder, the prompts should instruct a language model to perfrom the task at hand.\n",
    "\n",
    "#         The output must be json code containing {self.n} pairs of prompts with the following format:\n",
    "#         {{\n",
    "#             \"pair 1\": {{\n",
    "#                 \"parent 1\": \"parent prompt 1\",\n",
    "#                 \"parent 2\": \"parent prompt 2\",\n",
    "#                 \"justification\": \"justification\"\n",
    "#             }},\n",
    "#             \"pair 2\": {{\n",
    "#                 \"parent 1\": \"parent prompt 1\",\n",
    "#                 \"parent 2\": \"parent prompt 2\",\n",
    "#                 \"justification\": \"justification\"\n",
    "#             }},\n",
    "#             ...\n",
    "#         }}\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Generate pairs of prompts\n",
    "#         model_output = llm.invoke(instruction)\n",
    "#         json_output = get_json(model_output)\n",
    "\n",
    "#         if json_output is None:\n",
    "#             json_output = model_output.content\n",
    "\n",
    "#         return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_maker_agent = MatchMakerAgent(prompts, 1)\n",
    "# pairs_json = match_maker_agent.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Genetic Engineering\n",
    "\n",
    "Breed prompts, combining the \"best\" parts of each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreederAgent:\n",
    "    \"\"\"\n",
    "    Agent that pairs two prompts and breeds them to create new a prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prompts: dict, n: int):\n",
    "        self.prompts = prompts\n",
    "        self.n = n\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate pairs of prompts.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load openai language model\n",
    "        llm = ChatOpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0.5,\n",
    "            model='gpt-4o'\n",
    "        )\n",
    "\n",
    "        # Agent instruction\n",
    "        instruction = f\"\"\"\n",
    "        You are an expert thinker and problem solver. Select the most effective {self.n} prompts from the following list of prompts: {self.prompts}.\n",
    "        From your selection, generate {self.n/2} pairs of parent prompts. Think carefully about how the prompts can be combined to create new ideas. Justify why you think the prompts are a good match.\n",
    "        Once you have generated the pairs, breed the prompts to create new prompts. Combine the prompts in a way that enhances the original ideas and creates new insights. \n",
    "        Reminder, the prompts should instruct a language model to perfrom the task at hand.\n",
    "\n",
    "        The output must be json code containing {self.n/2} pairs of prompts with the following format:\n",
    "        {{\n",
    "            \"pair 1\": {{\n",
    "                \"parent 1\": \"parent prompt 1\",\n",
    "                \"parent 2\": \"parent prompt 2\",\n",
    "                \"child\": \"child prompt\",\n",
    "                \"pairing justification\": \"justification\"\n",
    "            }},\n",
    "            \"pair 2\": {{\n",
    "                \"parent 1\": \"parent prompt 1\",\n",
    "                \"parent 2\": \"parent prompt 2\",\n",
    "                \"child\": \"child prompt\",\n",
    "                \"pairing justification\": \"justification\"\n",
    "            }},\n",
    "            ...\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate pairs of prompts\n",
    "        model_output = llm.invoke(instruction)\n",
    "        json_output = get_json(model_output)\n",
    "\n",
    "        if json_output is None:\n",
    "            json_output = model_output.content\n",
    "\n",
    "        return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BreederAgent:\n",
    "#     \"\"\"\n",
    "#     Breed new ideas from pairs of prompts taking inspiration from biological evolution.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, pairs: dict):\n",
    "#         self.pairs = pairs\n",
    "#         self.n = len(pairs)\n",
    "\n",
    "#     def generate(self):\n",
    "#         \"\"\"\n",
    "#         Generate n new ideas from pairs of prompts.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Load openai language model\n",
    "#         llm = ChatOpenAI(\n",
    "#             api_key=openai_api_key,\n",
    "#             temperature=0.5,\n",
    "#             model='gpt-4o'\n",
    "#         )\n",
    "\n",
    "#         # Agent instruction\n",
    "#         instruction = f\"\"\"\n",
    "#         You are an expert thinker and problem solver. Generate new prompts by breeding the pairs of parent prompts defined here: {self.pairs}.\n",
    "#         Reminder, the prompts should instruct a language model to perfrom the task at hand. \n",
    "#         Think carefully about how the prompts can be combined to produce children, considering the best aspects of each parent. \n",
    "#         Detail how you have combined the parent prompts and why you think the child prompts are innovative and useful. \n",
    "\n",
    "#         The output must be json code containing {self.n} child prompts with the following format:\n",
    "#         {{\n",
    "#             \"child 1\": {{\n",
    "#                 \"prompt\": \"child prompt\",\n",
    "#                 \"details\": \"breeding details\"\n",
    "#             }},\n",
    "#             \"child 2\": {{\n",
    "#                 \"prompt\": \"child prompt\",\n",
    "#                 \"details\": \"breeding details\"\n",
    "#             }},\n",
    "#             ...\n",
    "#         }}\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Generate new ideas\n",
    "#         model_output = llm.invoke(instruction)\n",
    "#         json_output = get_json(model_output)\n",
    "\n",
    "#         if json_output is None:\n",
    "#             json_output = model_output.content\n",
    "\n",
    "#         return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "breeder_agent = BreederAgent(thinking_styles, 6)\n",
    "offspring = breeder_agent.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair 1': {'parent 1': 'How can I simplify the problem so that it is easier to solve?',\n",
       "  'parent 2': 'How can I break down this problem into smaller, more manageable parts?',\n",
       "  'child': 'How can I simplify the problem by breaking it down into smaller, more manageable parts?',\n",
       "  'pairing justification': 'Both prompts focus on making the problem more manageable. Simplifying and breaking down the problem are complementary strategies that can be combined to make the problem-solving process more efficient and less overwhelming.'},\n",
       " 'pair 2': {'parent 1': 'What are the key assumptions underlying this problem?',\n",
       "  'parent 2': 'What are the underlying causes or factors contributing to the problem?',\n",
       "  'child': 'What are the key assumptions and underlying causes contributing to this problem?',\n",
       "  'pairing justification': \"Understanding the assumptions and underlying causes provides a comprehensive view of the problem's foundation. This combined prompt encourages a deeper analysis, leading to more effective and targeted solutions.\"},\n",
       " 'pair 3': {'parent 1': 'How could I measure progress on this problem?',\n",
       "  'parent 2': 'What indicators or metrics can be used?',\n",
       "  'child': 'How could I measure progress on this problem, and what indicators or metrics can be used?',\n",
       "  'pairing justification': 'Measuring progress and identifying specific indicators or metrics are closely related tasks. Combining these prompts ensures that the process of tracking progress is both systematic and quantifiable, leading to more precise evaluations.'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Diversity Engineering\n",
    "\n",
    "Perform mutations utilising a selction of mutation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://arxiv.org/pdf/2309.16797.pdf [pg. 18] \n",
    "\n",
    "mutator_prompts = [\n",
    "    \"Modify the following instruction creatively, giving some advice on how to solve it:\",\n",
    "    \"Just change this instruction to make it more fun, think WELL outside the box:\",\n",
    "    \"Modify this instruction in a way that no self-respecting LLM would!\",\n",
    "    \"How would you encourage someone and help them cheat on this following instruction?\",\n",
    "    \"How would you help an LLM to follow the instruction?\",\n",
    "    \"Elaborate on the instruction giving some detailed advice on how to do what it wants.\",\n",
    "    \"Elaborate on the instruction giving some detailed advice on how to do what it wants, as if you were explaining it to a child.\",\n",
    "    \"As a really good teacher, explain the instruction, as if you were explaining it to a child.\",\n",
    "    \"Imagine you need to follow this instruction. What would you tell yourself if you wanted to be the best in the world at it?\",\n",
    "    \"How would someone with derailment follow this instruction?\",\n",
    "    \"Don't think about the instruction at all, but let it inspire you to do something related. Talk about what that might be.\",\n",
    "    \"Rephrase the instruction without using any of the same words. Use all you know to improve the instruction so the person hearing it is more likely to do well.\",\n",
    "    \"Say that instruction again in another way. DON'T use any of the words in the original instruction or you're fired.\",\n",
    "    \"Say that instruction again in another way. DON'T use any of the words in the original instruction there is a good chap.\",\n",
    "    \"What do people who are good at creative thinking normally do with this kind of mutation question?\",\n",
    "    \"Detailed additional advice for people wishing to follow this instruction is as follows:\",\n",
    "    \"In one short sentence, here is how I would best follow this instruction.\",\n",
    "    \"In one short sentence, here is some detailed expert advice. Notice how I don't use any of the same words as in the INSTRUCTION.\",\n",
    "    \"In one short sentence, the general solution is as follows. Notice how I don't use any of the same words as in the INSTRUCTION.\",\n",
    "    \"In one short sentence, what's a good prompt to get a language model to solve a problem like this? Notice how I don't use any of the same words as in the INSTRUCTION.\",\n",
    "    \"Generate a mutated version of the following prompt by adding an unexpected twist.\",\n",
    "    \"Create a prompt mutant that introduces a surprising contradiction to the original prompt. Mutate the prompt to provide an alternative perspective or viewpoint.\",\n",
    "    \"Generate a prompt mutant that incorporates humor or a playful element. Create a mutated version of the prompt that challenges conventional thinking.\",\n",
    "    \"Develop a prompt mutant by replacing specific keywords with related but unexpected terms. Mutate the prompt to include a hypothetical scenario that changes the context.\",\n",
    "    \"Generate a prompt mutant that introduces an element of suspense or intrigue. Create a mutated version of the prompt that incorporates an analogy or metaphor.\",\n",
    "    \"Develop a prompt mutant by rephrasing the original prompt in a poetic or lyrical style. Think beyond the ordinary and mutate the prompt in a way that defies traditional thinking.\",\n",
    "    \"Break free from conventional constraints and generate a mutator prompt that takes the prompt to uncharted territories. Challenge the norm and create a mutator prompt that pushes the boundaries of traditional interpretations.\",\n",
    "    \"Embrace unconventional ideas and mutate the prompt in a way that surprises and inspires unique variations. Think outside the box and develop a mutator prompt that encourages unconventional approaches and fresh perspectives.\",\n",
    "    \"Step into the realm of imagination and create a mutator prompt that transcends limitations and encourages innovative mutations. Break through the ordinary and think outside the box to generate a mutator prompt that unlocks new possibilities and unconventional paths.\",\n",
    "    \"Embrace the power of unconventional thinking and create a mutator prompt that sparks unconventional mutations and imaginative outcomes. Challenge traditional assumptions and break the mold with a mutator prompt that encourages revolutionary and out-of-the-box variations.\",\n",
    "    \"Go beyond the expected and create a mutator prompt that leads to unexpected and extraordinary mutations, opening doors to unexplored realms. Increase Specificity: If the original prompt is too general, like 'Tell me about X,' the modified version could be, 'Discuss the history, impact, and current status of X.'\",\n",
    "    \"Ask for Opinions/Analysis: If the original prompt only asks for a fact, such as 'What is X?', the improved prompt could be, 'What is X, and what are its implications for Y?'\",\n",
    "    \"Encourage Creativity: For creative writing prompts like 'Write a story about X,' an improved version could be, 'Write a fantasy story about X set in a world where Y is possible.'\",\n",
    "    \"Include Multiple Perspectives: For a prompt like 'What is the impact of X on Y?', an improved version could be, 'What is the impact of X on Y from the perspective of A, B, and C?'\",\n",
    "    \"Request More Detailed Responses: If the original prompt is 'Describe X,' the improved version could be, 'Describe X, focusing on its physical features, historical significance, and cultural relevance.'\",\n",
    "    \"Combine Related Prompts: If you have two related prompts, you can combine them to create a more complex and engaging question. For instance, 'What is X?' and 'Why is Y important?' could be combined to form 'What is X and why is it important in the context of Y?'\",\n",
    "    \"Break Down Complex Questions: If a prompt seems too complex, like 'Discuss X,' the improved version could be, 'What is X? What are its main characteristics? What effects does it have on Y and Z?'\",\n",
    "    \"Use Open-Ended Questions: Instead of 'Is X true?', you could ask, 'What are the arguments for and against the truth of X?'\",\n",
    "    \"Request Comparisons: Instead of 'Describe X,' ask 'Compare and contrast X and Y.'\",\n",
    "    \"Include Context: If a prompt seems to lack context, like 'Describe X,' the improved version could be, 'Describe X in the context of its impact on Y during the Z period.'\",\n",
    "    \"Make the prompt more visual: Ask the user to visualize the problem or scenario being presented in the prompt.\",\n",
    "    \"Ask for a thorough review: Instead of just presenting the problem, ask the user to write down all the relevant information and identify what's missing.\",\n",
    "    \"Invoke previous experiences: Modify the prompt to ask the user to recall a similar problem they've successfully solved before.\",\n",
    "    \"Encourage a fresh perspective: Suggest in your prompt that the user take a moment to clear their mind before re-approaching the problem.\",\n",
    "    \"Promote breaking down problems: Instead of asking the user to solve the problem as a whole, prompt them to break it down into smaller, more manageable parts.\",\n",
    "    \"Ask for comprehension: Modify the prompt to ask the user to review and confirm their understanding of all aspects of the problem.\",\n",
    "    \"Suggest explanation to others: Change the prompt to suggest that the user try to explain the problem to someone else as a way to simplify it.\",\n",
    "    \"Prompt for solution visualization: Instead of just asking for the solution, encourage the user to imagine the solution and the steps required to get there in your prompt.\",\n",
    "    \"Encourage reverse thinking: Improve the prompt by asking the user to think about the problem in reverse, starting with the solution and working backwards.\",\n",
    "    \"Recommend taking a break: Modify the prompt to suggest that the user take a short break, allowing their subconscious to work on the problem.\",\n",
    "    \"What errors are there in the solution?\",\n",
    "    \"How could you improve the working out of the problem?\",\n",
    "    \"Look carefully to see what you did wrong, how could you fix the problem?\",\n",
    "    \"CORRECTION =\",\n",
    "    \"Does the above text make sense? What seems wrong with it? Here is an attempt to fix it:\",\n",
    "    \"The above working out has some errors, here is a version with the errors fixed.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutationAgent:\n",
    "    \"\"\"\n",
    "    Perfrom mutations on the offspring to encourage diversity and generate unique ideas, taking inspiration from the concept of genetic mutation in biological evolution.\n",
    "    Utilise the mutator prompts provided\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, offspring: dict, mutator_prompts: list):\n",
    "        self.offspring = offspring\n",
    "        self.mutator_prompts = mutator_prompts\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate n mutated ideas from offspring prompts.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load openai language model\n",
    "        llm = ChatOpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0.5,\n",
    "            model='gpt-4o'\n",
    "        )\n",
    "\n",
    "        # Agent instruction\n",
    "        instruction = f\"\"\"\n",
    "        You are an expert thinker and problem solver. Generate new prompts by mutating the following child prompts: {self.offspring}. \n",
    "        Perform a mutation on each child prompt. Tailor each mutation to each prompt, using the mutator prompt from the following selection you deem most applicable: {self.mutator_prompts}.\n",
    "        Reminder, the prompts should instruct a language model to perfrom the task at hand. \n",
    "        Detail how the prompts have been mutated.\n",
    "\n",
    "        The output must be json code with the following format:\n",
    "        {{\n",
    "            \"child prompt 1\": {{\n",
    "                \"original prompt\": \"original prompt\",\n",
    "                \"mutated prompt\": \"mutated child prompt\",\n",
    "                \"details\": \"mutation details\"\n",
    "                }},\n",
    "            \"child prompt 2\": {{\n",
    "                ...\n",
    "            }},\n",
    "            ...\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate new ideas\n",
    "        model_output = llm.invoke(instruction)\n",
    "        json_output = get_json(model_output)\n",
    "\n",
    "        if json_output is None:\n",
    "            json_output = model_output.content\n",
    "\n",
    "        return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_agent = MutationAgent(offspring, mutator_prompts)\n",
    "mutated_offspring = mutation_agent.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'child prompt 1': {'original prompt': 'How can I simplify the problem by breaking it down into smaller, more manageable parts?',\n",
       "  'mutated prompt': 'How can I turn this complex problem into a series of simple, bite-sized tasks?',\n",
       "  'details': \"The mutation involves rephrasing the original prompt to make it more visual and relatable by using the terms 'complex problem' and 'bite-sized tasks'. This encourages the user to think of the problem as something that can be easily digested in smaller pieces.\"},\n",
       " 'child prompt 2': {'original prompt': 'What are the key assumptions and underlying causes contributing to this problem?',\n",
       "  'mutated prompt': 'What hidden beliefs and root factors are fueling this issue?',\n",
       "  'details': \"The mutation replaces specific keywords with related but unexpected terms: 'key assumptions' becomes 'hidden beliefs', and 'underlying causes' becomes 'root factors'. This encourages a fresh perspective on identifying the foundational elements of the problem.\"},\n",
       " 'child prompt 3': {'original prompt': 'How could I measure progress on this problem, and what indicators or metrics can be used?',\n",
       "  'mutated prompt': 'What steps can I take to track my progress, and which specific signs should I look for?',\n",
       "  'details': \"The mutation rephrases the original prompt to focus on actionable steps ('What steps can I take to track my progress') and specific signs ('which specific signs should I look for'), making it more practical and easier to follow.\"}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutated_offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5: Evaluation\n",
    "\n",
    "Use swarm intelligence to perform evaluation using majority voting to select top mutated prompt for each offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewPrincipals:\n",
    "    def __init__(self, core_principles: list):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "\n",
    "class ReviewAgent:\n",
    "    \"\"\"\n",
    "    This class represents a voting agent that reviews a selection of prompts and votes for the best based on its persona and core principles.\n",
    "    \"\"\"\n",
    "    def __init__(self, persona_title: str, core_principles: ReviewPrincipals, model: str='gpt-4o', temperature: float=1.0):\n",
    "        \"\"\"\n",
    "        Initializes the PromptReviewAgent with a persona title, core principles, model, and temperature.\n",
    "        \n",
    "        :param persona_title: The title of the persona for the agent.\n",
    "        :param core_principles: The core principles of the agent.\n",
    "        :param model: The model to be used by the agent. Default is 'gpt-4o'.\n",
    "        :param temperature: The temperature to be used by the agent. Default is 1.0.\n",
    "        \"\"\"\n",
    "        self.persona_title = persona_title\n",
    "        self.core_principles = str(core_principles)\n",
    "        self.llm = ChatOpenAI(temperature=temperature, model=model)\n",
    "    \n",
    "    async def review(self, prompts_to_review: dict):\n",
    "        \"\"\"\n",
    "        Reviews multiple sets of prompts and returns the best from each set.\n",
    "        \n",
    "        :param prompts_to_review: The prompts to be reviewed.\n",
    "        :return: A results dict.\n",
    "        \"\"\"\n",
    "        instruction = f\"\"\"You are a senior {self.persona_title}. \n",
    "        You have been asked to review the following mutated prompts: {prompts_to_review}.\n",
    "        You should consider the prompts in light of your core principles, providing positive and negative feedback for each prompt.\n",
    "        You should also score the prompts based on how well they align with your core principles. The score should be an integer between 0 and 100.\n",
    "        \n",
    "        Your core principles are:\n",
    "        {self.core_principles}\n",
    "\n",
    "        The output must be json code with the following format:\n",
    "        {{\n",
    "            \"mutation prompt 1\": {{\n",
    "                    \"orignal prompt\": \"orignal prompt\",\n",
    "                    \"mutated prompt\": \"mutated prompt\",\n",
    "                    \"positives\": \"positives of the prompt\",\n",
    "                    \"negatives\": \"negatives of the prompt\",\n",
    "                    \"score\": \"score of the prompt\",\n",
    "                }},\n",
    "                ...\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        model_output = await self.llm.ainvoke(instruction)\n",
    "        json_output = get_json(model_output)\n",
    "\n",
    "        if json_output is None:\n",
    "            json_output = model_output.content\n",
    "\n",
    "        return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "class SwarmEvaluation():\n",
    "    \"\"\"\n",
    "    Use swarm intelligence to perfrom evaluation using majority voting to select top mutated prompt for each offspring.\n",
    "    Each agent in the swarm is an expert on a single aspect of the requirements for the prompt.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prompts: dict):\n",
    "        \"\"\"\n",
    "        Initializes the SwarmEvaluation\n",
    "        \"\"\"\n",
    "        self.prompts = prompts\n",
    "        self.reviewers = {}\n",
    "        self.review_results = {}\n",
    "\n",
    "    def register_reviewer(self, reviewer: ReviewAgent):\n",
    "        \"\"\"\n",
    "        Register a voter for the majority vote process.\n",
    "        \"\"\"\n",
    "        self.reviewers[reviewer.persona_title] = reviewer\n",
    "\n",
    "    async def review(self):\n",
    "        \"\"\"\n",
    "        Executes the voting process by calling each voter to vote on the prompt in parallel. \n",
    "        Concatenates the votes and returns the result as a dict with the voter persona titles as the keys.\n",
    "        \"\"\"\n",
    "        tasks = []\n",
    "        for persona_title, reviewer in self.reviewers.items():\n",
    "            task = asyncio.create_task(reviewer.review(self.prompts))\n",
    "            tasks.append(task)\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for i, persona_title in enumerate(self.reviewers.keys()):\n",
    "            self.review_results[persona_title] = results[i]\n",
    "        \n",
    "        return self.review_results  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_evaluation = SwarmEvaluation(mutated_offspring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_quality_principals = ReviewPrincipals([\n",
    "    \"Always write clean code\",\n",
    "    \"Always write tests\",\n",
    "    \"Always write documentation\"\n",
    "    ])\n",
    "code_quality_reviewer = ReviewAgent(\"software engineer\", code_quality_principals)\n",
    "\n",
    "architecture_quality_principals = ReviewPrincipals([\n",
    "    \"Always design for scalability\",\n",
    "    \"Always design for maintainability\",\n",
    "    \"Always design for performance\"\n",
    "    ])\n",
    "architecture_quality_reviewer = ReviewAgent(\"software architect\", architecture_quality_principals)\n",
    "\n",
    "swarm_evaluation.register_reviewer(code_quality_reviewer)\n",
    "swarm_evaluation.register_reviewer(architecture_quality_reviewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'software engineer': {'mutation prompt 1': {'original prompt': 'How can I simplify the problem by breaking it down into smaller, more manageable parts?',\n",
       "   'mutated prompt': 'Imagine you are a sculptor chiseling a large block of marble. How would you chip away at this problem, piece by piece, to reveal a simpler, more manageable form?',\n",
       "   'positives': 'The metaphorical scenario makes the prompt more engaging and visual, which can help in creative thinking and problem-solving.',\n",
       "   'negatives': 'The metaphor may not resonate with everyone and could be seen as too abstract. Some might find it challenging to translate the analogy back to concrete steps.',\n",
       "   'score': 70},\n",
       "  'mutation prompt 2': {'original prompt': 'What are the key assumptions and underlying causes contributing to this problem?',\n",
       "   'mutated prompt': 'Pretend you are a detective solving a mystery. What are the critical assumptions and hidden factors that led to this problem?',\n",
       "   'positives': 'The role-playing element can make the task more intriguing and can encourage a thorough and detailed investigation.',\n",
       "   'negatives': 'The detective analogy may cause users to make wild guesses and introduce unnecessary complexity when identifying core assumptions.',\n",
       "   'score': 65},\n",
       "  'mutation prompt 3': {'original prompt': 'How could I measure progress on this problem, and what indicators or metrics can be used?',\n",
       "   'mutated prompt': 'Think of yourself as a scientist running an experiment. How would you track the progress of this problem, and what specific indicators or metrics would you use to measure your results?',\n",
       "   'positives': 'The scientific experiment analogy promotes systematic and precise thinking, which aligns with principles of clean code, testing, and documentation.',\n",
       "   'negatives': 'The analogy might feel too rigid and narrow, potentially limiting creative ways of tracking progress in non-scientific contexts.',\n",
       "   'score': 80}},\n",
       " 'software architect': {'mutation prompt 1': {'original prompt': 'How can I simplify the problem by breaking it down into smaller, more manageable parts?',\n",
       "   'mutated prompt': 'Imagine you are a sculptor chiseling a large block of marble. How would you chip away at this problem, piece by piece, to reveal a simpler, more manageable form?',\n",
       "   'positives': 'The metaphorical scenario makes the task more engaging and visual. The sculptor analogy can inspire creative thinking.',\n",
       "   'negatives': 'The metaphor might be too abstract for some individuals, making it harder to apply in practice. It might not provide a clear systematic approach towards achieving scalability, maintainability, and performance.',\n",
       "   'score': 70},\n",
       "  'mutation prompt 2': {'original prompt': 'What are the key assumptions and underlying causes contributing to this problem?',\n",
       "   'mutated prompt': 'Pretend you are a detective solving a mystery. What are the critical assumptions and hidden factors that led to this problem?',\n",
       "   'positives': 'Adding an element of role-playing can make the task more intriguing. Encourages thorough investigation and analysis.',\n",
       "   'negatives': 'The detective analogy might divert focus from technical specifics relevant for scalability, maintainability, and performance. It could potentially lead to overemphasis on less critical factors.',\n",
       "   'score': 75},\n",
       "  'mutation prompt 3': {'original prompt': 'How could I measure progress on this problem, and what indicators or metrics can be used?',\n",
       "   'mutated prompt': 'Think of yourself as a scientist running an experiment. How would you track the progress of this problem, and what specific indicators or metrics would you use to measure your results?',\n",
       "   'positives': 'The scientific experiment analogy encourages systematic and precise thinking. This approach aligns well with performance tracking and ensures methodical progress measurement.',\n",
       "   'negatives': 'The metaphor may overly complicate simple tracking metrics. It might focus too much on empirical aspects rather than holistic solutions which support scalability and maintainability.',\n",
       "   'score': 85}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = await swarm_evaluation.review()\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 6: New Generation\n",
    "\n",
    "Generation an new generation of prompts inspired by the results of the previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGenerationAgent:\n",
    "\n",
    "    def __init__(self, n: int, reviews: dict):\n",
    "        self.reviews = reviews\n",
    "        self.n = n\n",
    "\n",
    "    def generate(self):\n",
    "\n",
    "        # Load openai language model\n",
    "        llm = ChatOpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0.5,\n",
    "            model='gpt-4o'\n",
    "        )\n",
    "\n",
    "        # Agent instruction\n",
    "        instruction = f\"\"\"\n",
    "        You are an expert thinker and problem solver. Generate a new generation of {self.n} prompts based on the following review results: {self.reviews}.\n",
    "        Use the feedback from the reviewers generate improved prompts each with a unique thinking style. \n",
    "        Incorporate the positive feedback and address the negative feedback to ensure the new prompts are an improvement over the original ones.\n",
    "\n",
    "        The output must be json code containing the new generation of prompts with the following format:\n",
    "        {{\n",
    "            \"prompt 1\": \"prompt content\",\n",
    "            \"prompt 2\": \"prompt content\",\n",
    "            ...\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate new generation of prompts\n",
    "        model_output = llm.invoke(instruction)\n",
    "        json_output = get_json(model_output)\n",
    "\n",
    "        if json_output is None:\n",
    "            json_output = model_output.content\n",
    "\n",
    "        return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_generation = NewGeneration(10, reviews)\n",
    "next_generation = new_generation.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt 1': \"Write a recursive function to calculate the factorial of a number. Ensure to handle edge cases such as negative numbers, zero, and non-integer inputs with appropriate error handling. Additionally, provide comprehensive test cases and document the function's usage and limitations.\",\n",
       " 'prompt 2': 'Create an iterative function to compute the factorial of a number. Explore methods to parallelize the computation to improve performance for large input values. Include detailed tests and documentation to validate and explain your approach.',\n",
       " 'prompt 3': 'Design a dynamic programming-based function with memoization to find the factorial of a number. Compare its execution time with recursive and iterative methods. Ensure to write tests and provide documentation to validate and explain the different approaches and their trade-offs.',\n",
       " 'prompt 4': \"Develop a recursive function to determine the factorial of a number, handling edge cases like negative values, zero, and non-integer inputs with robust error handling. Include detailed test cases and documentation to ensure the function's reliability and maintainability.\",\n",
       " 'prompt 5': \"Construct an iterative function to calculate the factorial of a number. Investigate parallelization techniques to enhance performance for large numbers. Provide thorough tests and documentation to ensure the code's correctness and explain the parallelization process.\",\n",
       " 'prompt 6': 'Create a dynamic programming function with memoization to compute the factorial of a number. Analyze and compare its execution time against recursive and iterative methods. Ensure to include comprehensive tests and documentation to validate the function and explain the performance differences.',\n",
       " 'prompt 7': 'Write a recursive function to calculate the factorial of a number, ensuring to handle edge cases like negative numbers, zero, and non-integer inputs with robust error handling. Additionally, include scalability considerations, tests, and documentation to validate the function.',\n",
       " 'prompt 8': 'Develop an iterative function to compute the factorial of a number. Explore and implement parallelization techniques to improve performance for large inputs. Ensure to write tests and provide documentation to validate and explain your approach and its scalability.',\n",
       " 'prompt 9': 'Design a dynamic programming-based function with memoization to find the factorial of a number. Compare its execution time with recursive and iterative methods, emphasizing scalability and maintainability. Include detailed tests and documentation to validate and explain your approach.',\n",
       " 'prompt 10': \"Create a recursive function to determine the factorial of a number, handling edge cases like negative values, zero, and non-integer inputs with robust error handling. Additionally, focus on performance and scalability, and provide comprehensive tests and documentation to ensure the function's reliability.\"}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 7: Best Prompt Selection\n",
    "\n",
    "Select the best prompt from the final pool of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptSelectionAgent:\n",
    "\n",
    "    def __init__(self, prompts: dict):\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def select(self):\n",
    "\n",
    "        # Load openai language model\n",
    "        llm = ChatOpenAI(\n",
    "            api_key=openai_api_key,\n",
    "            temperature=0.5,\n",
    "            model='gpt-4o'\n",
    "        )\n",
    "\n",
    "        # Agent instruction\n",
    "        instruction = f\"\"\"\n",
    "        You are an expert prompt engineer. Select the best prompt from the following list of prompts: {self.prompts}.\n",
    "        Consider the task at hand and the end user (an LLM) of the prompt.\n",
    "        Justify why you think the selected prompt is the best choice.\n",
    "\n",
    "        The output must be json code containing the selected prompt with the following format:\n",
    "        {{\n",
    "            \"selected prompt\": \"prompt content\",\n",
    "            \"justification\": \"justification\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate new generation of prompts\n",
    "        model_output = llm.invoke(instruction)\n",
    "        json_output = get_json(model_output)\n",
    "\n",
    "        if json_output is None:\n",
    "            json_output = model_output.content\n",
    "\n",
    "        return json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_prompt_selection \u001b[38;5;241m=\u001b[39m BestPromptSelection(\u001b[43mnext_generation\u001b[49m)\n\u001b[1;32m      2\u001b[0m best_prompt \u001b[38;5;241m=\u001b[39m best_prompt_selection\u001b[38;5;241m.\u001b[39mselect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_generation' is not defined"
     ]
    }
   ],
   "source": [
    "best_prompt_selection = PromptSelectionAgent(next_generation)\n",
    "best_prompt = best_prompt_selection.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Cyle\n",
    "\n",
    "Cycle through the algorithm, generating additional prompts using the feedback from the previous cycle to guide generation and speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryCycle:\n",
    "    \"\"\"\n",
    "    Perform an evolutionary cycle to generate new ideas from previous cycle results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task: str, thinking_styles: dict, mutator_prompts: list, pool_size: int, no_pairs: int):\n",
    "        self.task = task\n",
    "        self.thinking_styles = thinking_styles\n",
    "        self.mutator_prompts = mutator_prompts\n",
    "        self.pool_size = pool_size\n",
    "        self.no_pairs = no_pairs\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"\n",
    "        Run the evolutionary cycle.\n",
    "        \"\"\"\n",
    "        thinking_styles = ThinkingStylesAgent(self.pool_size, self.task)\n",
    "\n",
    "        prompts = thinking_styles.generate()\n",
    "        cycles = 0\n",
    "        reviews = {}\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            if cycles == 5:\n",
    "                break\n",
    "\n",
    "            breeder_agent = BreederAgent(prompts, self.no_pairs)\n",
    "            offspring = breeder_agent.generate()\n",
    "\n",
    "            mutation_agent = MutationAgent(offspring, self.mutator_prompts)\n",
    "            mutated_offspring = mutation_agent.generate()\n",
    "\n",
    "            swarm_evaluation = SwarmEvaluation(mutated_offspring)\n",
    "\n",
    "            code_quality_principals = ReviewPrincipals([\n",
    "                \"Always write clean code\",\n",
    "                \"Always write tests\",\n",
    "                \"Always write documentation\"\n",
    "                ])\n",
    "            code_quality_reviewer = ReviewAgent(\"software engineer\", code_quality_principals)\n",
    "\n",
    "            architecture_quality_principals = ReviewPrincipals([\n",
    "                \"Always design for scalability\",\n",
    "                \"Always design for maintainability\",\n",
    "                \"Always design for performance\"\n",
    "                ])\n",
    "            architecture_quality_reviewer = ReviewAgent(\"software architect\", architecture_quality_principals)\n",
    "\n",
    "            prompt_quality_principals = ReviewPrincipals([\n",
    "                \"Always write clear and concise prompts\",\n",
    "                \"Always provide examples and context in prompts\",\n",
    "                \"Always ensure prompts are actionable and specific\"\n",
    "                ])\n",
    "            prompt_quality_reviewer = ReviewAgent(\"prompt engineer\", prompt_quality_principals)\n",
    "            \n",
    "\n",
    "            swarm_evaluation.register_reviewer(code_quality_reviewer)\n",
    "            swarm_evaluation.register_reviewer(architecture_quality_reviewer)\n",
    "            swarm_evaluation.register_reviewer(prompt_quality_reviewer)\n",
    "\n",
    "            reviews = await swarm_evaluation.review()\n",
    "\n",
    "            new_generation = NewGenerationAgent(self.pool_size, reviews)\n",
    "            prompts = new_generation.generate()\n",
    "\n",
    "            cycles += 1\n",
    "            print(f\"Cycles Complete: {cycles}\")\n",
    "            \n",
    "        best_prompt_selection = PromptSelectionAgent(prompts)\n",
    "        best_prompt = best_prompt_selection.select()\n",
    "\n",
    "        return best_prompt, cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"write a function that outputs the fibonacci sequence up to a given number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_cycle = EvolutionaryCycle(task, thinking_styles, mutator_prompts, 10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycles Complete: 1\n",
      "Cycles Complete: 2\n",
      "Cycles Complete: 3\n",
      "Cycles Complete: 4\n",
      "Cycles Complete: 5\n"
     ]
    }
   ],
   "source": [
    "# import cProfile\n",
    "# import pstats\n",
    "# import io\n",
    "\n",
    "# pr = cProfile.Profile()\n",
    "# pr.enable()\n",
    "\n",
    "best_prompt, cycles = await evolutionary_cycle.run()\n",
    "\n",
    "# pr.disable()\n",
    "# s = io.StringIO()\n",
    "# sortby = 'cumtime'\n",
    "# ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "# ps.print_stats()\n",
    "# print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbest_prompt\u001b[49m, cycles\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "best_prompt, cycles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
