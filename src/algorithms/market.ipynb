{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidding Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts.chat import SystemMessage, _convert_to_message\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Market Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorePrinciples:\n",
    "    def __init__(self, core_principles: List[str]):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "class ExpertAgent:\n",
    "    \"\"\"\n",
    "    Expert Agent class defining agents that provide feedback on prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, llm):\n",
    "        self.position = position\n",
    "        self.core_principles = core_principles\n",
    "        self.system_message = f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\"\n",
    "        self.llm = llm\n",
    "\n",
    "        assert isinstance(self.llm, ChatOpenAI) or isinstance(self.llm, ChatAnthropic), \"LLM must be an instance of ChatOpenAI or ChatAnthropic\"\n",
    "\n",
    "    def bid(self, state: Sequence[BaseMessage]) -> float:\n",
    "        \"\"\"\n",
    "        Bids on the prompt based on the expert's expertise.\n",
    "        \"\"\"\n",
    "        prompt_text = f\"\"\"Your task is to bid on the prompt in the conversation above in light of your core principles.\n",
    "The bid must reflect the prompts quality and alignment with your core principles.\n",
    "\n",
    "The bid must be an integer between 1 and 10 and should be based on the following scale:\n",
    "- 1 (Exceptional Alignment): Perfectly aligns with your core principles. No modifications needed.\n",
    "- 2 (Strong Alignment): Demonstrates strong alignment. Minimal to no adjustments required.\n",
    "- 3 (Good Alignment): Well-aligned with minor tweaks needed.\n",
    "- 4 (Moderate Alignment): Moderately aligned but requires moderate adjustments.\n",
    "- 5 (Adequate Alignment): Adequate alignment with room for improvement.\n",
    "- 6 (Fair Alignment): Fairly aligned but lacking in certain areas. Significant improvements needed.\n",
    "- 7 (Marginal Alignment): Marginally aligns, requiring substantial reworking.\n",
    "- 8 (Poor Alignment): Poorly aligns, necessitating major revisions.\n",
    "- 9 (Very Poor Alignment): Significantly misaligned, requiring a comprehensive overhaul.\n",
    "- 10 (Wholesale Changes Needed): In direct conflict with your core principles, requiring wholesale changes.\n",
    "\n",
    "Your bid process should be as follows:\n",
    "1. Read the prompt carefully as an experienced: {self.position}. Understand it's content and intent.\n",
    "2. Based on your assessment of how well the prompt aligns with your core principles, assign a bid using the bidding scale. Ensure your bid reflects the prompt's quality and alignment accurately.\n",
    "3. Submit your bid.\"\"\"\n",
    "        function_def = {\n",
    "            \"name\": \"bid\",\n",
    "            \"description\": \"Submit a bid for the prompt\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expert\": {\"type\": \"string\", \"enum\": [self.position]},\n",
    "                    \"bid\": {\"type\": \"string\", \"enum\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]},\n",
    "                },\n",
    "                \"required\": [\"expert\", \"bid\"],\n",
    "            },\n",
    "        }\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "            chain = (\n",
    "                prompt\n",
    "                | self.llm.bind_functions(functions=[function_def], function_call=\"bid\")\n",
    "                | JsonOutputFunctionsParser()\n",
    "            )\n",
    "            result = chain.invoke({\"messages\": state})\n",
    "            return result\n",
    "        elif isinstance(self.llm, ChatAnthropic):\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"user\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "            try:\n",
    "                chain = (\n",
    "                    prompt \n",
    "                    | self.llm.bind_tools(tools=[function_def])\n",
    "                )\n",
    "                result = chain.invoke({\"messages\": state})\n",
    "                if \"text\" in result.content[0]:\n",
    "                    return result.content[1][\"input\"]\n",
    "                else:\n",
    "                    return result.content[0][\"input\"]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return {\"expert\": self.position, \"bid\": \"2\"}\n",
    "\n",
    "    def self_reflection_graph(self, criteria) -> MessageGraph:\n",
    "        \"\"\"\n",
    "        Constructs a graph for self-reflection and improvement of prompts.\n",
    "        \"\"\"\n",
    "\n",
    "        def generation_node(state: Sequence[BaseMessage]):\n",
    "            prompt_text = f\"\"\"Your task is to improve the prompt in the conversation above in light of your core principles.\n",
    "If you recieve feedback and recommendations for the prompt, respond with a revised version of your previous attempts actioning the feedback.\n",
    "Always think outside the box and consider unconventional ideas on how to implement the feedback.\n",
    "\n",
    "The success criteria for the prompt are as follows:\n",
    "{criteria}\n",
    "You will be penalized if the prompt does not meet this criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces. If you wish to use curly braces in your response, use double curly braces to avoid confusion with placeholders.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Read the prompt as an experienced: {self.position}. Understand it's content and intent.\n",
    "2. Think carefully about how you can implement the most recent feedback and revise the prompt.\n",
    "3. Explcitly go through each success criteria and ensure the prompt meets them. If not, revise the prompt to make sure it does.\n",
    "4. Explicitly go through each guideline and ensure the changes adhere to them. If not, revise the prompt to make sure it does.\n",
    "5. Submit your revised prompt.\"\"\"\n",
    "            if isinstance(self.llm, ChatOpenAI):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"system\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "            elif isinstance(self.llm, ChatAnthropic):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"user\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "            chain = prompt | self.llm\n",
    "            return chain.invoke({\"messages\": state})\n",
    "            \n",
    "        def reflection_node(state: Sequence[BaseMessage]):\n",
    "            prompt_text = f\"\"\"Your task is to provide feedback on the prompt in the conversation above in light of your core princples.\n",
    "Always think outside the box and consider unconventional ideas on how to enforce your core principles in the prompt.\n",
    "\n",
    "The success criteria for the updated prompt are as follows:\n",
    "{criteria}\n",
    "You must use this information to inform your feedback.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt carefully as an experienced: {self.position}. Understand it's content and intent.\n",
    "2. Explain how you think the prompt can be improved in light of your core principles.\n",
    "3. Submit your feedback.\"\"\"\n",
    "            if isinstance(self.llm, ChatOpenAI):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"system\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "            elif isinstance(self.llm, ChatAnthropic):\n",
    "                prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self.system_message),\n",
    "                        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                        (\"user\", prompt_text),\n",
    "                    ]\n",
    "                )\n",
    "            chain = prompt | self.llm\n",
    "            result = chain.invoke({\"messages\": state})\n",
    "            return HumanMessage(content=result.content)\n",
    "\n",
    "        builder = MessageGraph()\n",
    "        builder.add_node(\"generate\", generation_node)\n",
    "        builder.add_node(\"reflect\", reflection_node)\n",
    "        builder.set_entry_point(\"generate\")\n",
    "\n",
    "        def should_continue(state: List[BaseMessage]):\n",
    "            if len(state) > 2:\n",
    "                return END\n",
    "            return \"reflect\"\n",
    "\n",
    "        builder.add_conditional_edges(\"generate\", should_continue)\n",
    "        builder.add_edge(\"reflect\", \"generate\")\n",
    "        graph = builder.compile()\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    def update_prompt(self, state: Sequence[BaseMessage], criteria) -> str:\n",
    "        \"\"\"\n",
    "        Uses self_reflection_graph to iteratively act on feedback and update prompt\n",
    "        \"\"\"\n",
    "        graph = self.self_reflection_graph(criteria)\n",
    "        result = graph.invoke([HumanMessage(content=state[\"messages\"][-2].content)])\n",
    "        return {\"messages\": result, \"next\": \"Moderator\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketPlace:\n",
    "    \"\"\"\n",
    "    Moderator Agent class defining an agent that builds the experts and moderates the bidding process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_prompt: str, criteria: str, experts: List[ExpertAgent], llm):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.criteria = criteria\n",
    "        self.experts = experts\n",
    "        self.iteration = 0\n",
    "        self.llm = llm\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the bidding process.\n",
    "        \"\"\"\n",
    "        self.iteration = 0\n",
    "\n",
    "    def bidding_process(self, state: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Execution of bidding process to select the next expert to process the prompt.\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        # print(f\"Iteration: {self.iteration}\")\n",
    "        # Collect bids from all experts\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(expert.bid, state[\"messages\"]) for expert in self.experts]\n",
    "            bids = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        # Only consider valid bid formats\n",
    "        bids = [bid for bid in bids if 'expert' in bid]\n",
    "        # Convert bids to integers\n",
    "        for bid in bids:\n",
    "            bid[\"bid\"] = int(bid[\"bid\"])\n",
    "        # Sort dictionary of bids by bid value\n",
    "        bids = sorted(bids, key=lambda x: x[\"bid\"], reverse=True)\n",
    "        # for i in range(len(bids)):\n",
    "        #     print(\"Expert: {expert}, Bid: {bid}\".format(expert=bids[i][\"expert\"], bid=bids[i][\"bid\"]))\n",
    "        # If a tie occurs, randomly select a expert from the tied experts, else select the expert with the highest bid\n",
    "        max_bid = max(bids, key=lambda x: x[\"bid\"])[\"bid\"]\n",
    "        tied_experts = [expert for expert in bids if expert[\"bid\"] == max_bid]\n",
    "        if len(tied_experts) > 1:\n",
    "            highest_bidder = tied_experts[random.randint(0, len(tied_experts) - 1)]\n",
    "        else:\n",
    "            highest_bidder = tied_experts[0]\n",
    "        next_expert = highest_bidder[\"expert\"]\n",
    "        print(f\"Highest Bidder: {next_expert}, Bid: {max_bid}\")\n",
    "        # Update the state with the next expert to process\n",
    "        if max_bid <= 2.0 or self.iteration >=6:\n",
    "            self.reset()\n",
    "            return {\n",
    "                \"next\": \"FINISH\", \n",
    "                \"messages\": [HumanMessage(content=f\"Bidding over. All bids <= 2\", name=\"Moderator\")]\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"next\": next_expert, \n",
    "                \"messages\": [HumanMessage(content=f\"Highest Bidder: {next_expert}, Bid: {max_bid}\", name=\"Moderator\")]\n",
    "            }\n",
    "        \n",
    "    def construct_expert_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of expert agents based on their roles and functions.\n",
    "        \"\"\"\n",
    "\n",
    "        def agent_node(state, agent):\n",
    "            return agent.update_prompt(state, self.criteria)\n",
    "        \n",
    "        # The agent state is the input to each node in the graph\n",
    "        class AgentState(TypedDict):\n",
    "            # The annotation tells the graph that new messages will always be added to the current states\n",
    "            messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "            next: str\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        for expert in self.experts:\n",
    "            # Create a node for each expert agent\n",
    "            node = functools.partial(agent_node, agent=expert)\n",
    "            workflow.add_node(expert.position, node)\n",
    "        workflow.add_node(\"Moderator\", self.bidding_process)\n",
    "\n",
    "        members = [expert.position for expert in self.experts]\n",
    "        for member in members:\n",
    "            # We want our experts to ALWAYS \"report back\" to the moderator when done\n",
    "            workflow.add_edge(member, \"Moderator\")\n",
    "        # The moderator populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(\"Moderator\", lambda x: x[\"next\"], conditional_map)\n",
    "        # Finally, add entrypoint\n",
    "        workflow.set_entry_point(\"Moderator\")\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def optimise_prompt(self):\n",
    "        \"\"\"\n",
    "        Optimises a prompt by invoking a graph of expert agents.\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=self.base_prompt, name=\"User\")],\n",
    "        }\n",
    "\n",
    "        # Construct the graph\n",
    "        graph = self.construct_expert_graph()\n",
    "        # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        n = random.randint(1, 1000)\n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": n},\n",
    "            \"recursion_limit\": 50,\n",
    "            }    \n",
    "\n",
    "        # Run the graph\n",
    "        for s in graph.stream(\n",
    "            initial_state,\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "            ):\n",
    "            if \"__end__\" not in s:\n",
    "                # if len(s[\"messages\"]) > 0:\n",
    "                #     s[\"messages\"][-1].pretty_print()\n",
    "                continue\n",
    "                    \n",
    "        # def message_to_dict(obj):\n",
    "        #     if isinstance(obj, HumanMessage) or isinstance(obj, AIMessage):\n",
    "        #         return {obj.name: obj.content}\n",
    "        #     raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
    "\n",
    "        # if type(self.llm) == ChatOpenAI:\n",
    "        #     model = self.llm.model_name\n",
    "        # else:\n",
    "        #     model = self.llm.model        \n",
    "        # temp = int(self.llm.temperature)\n",
    "        # path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/conversations/{model}/conversations_market_{temp}.json\"\n",
    "        # if not os.path.exists(path):\n",
    "        #     with open(path, \"w\") as f:\n",
    "        #         json.dump([], f)\n",
    "        \n",
    "        # with open(path, \"r\") as f:\n",
    "        #     # write messages to json file\n",
    "        #     data = json.load(f)\n",
    "        #     # get the current key number then increment it\n",
    "        #     key = len(data)\n",
    "        #     data.append({key: json.dumps(s, default=message_to_dict)})\n",
    "            \n",
    "        # with open(path, \"w\") as f:\n",
    "        #     json.dump(data, f, indent=4)\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import gsm8k, human_eval, sst2\n",
    "\n",
    "human_eval_baseline = human_eval.get_baseline_prompt()\n",
    "human_eval_criteria = human_eval.get_criteria()\n",
    "\n",
    "gsm8k_baseline = gsm8k.get_baseline_prompt()\n",
    "gsm8k_criteria = gsm8k.get_criteria()\n",
    "\n",
    "sst2_baseline_prompt = sst2.get_baseline_prompt()\n",
    "sst2_criteria = sst2.get_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatAnthropic(temperature=0, model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-20240620\")\n",
    "# llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "# llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "# llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "# llm = ChatOllama(temperature=1, model=\"mistral:v0.3\")\n",
    "# llm = ChatOllama(temperature=0, model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_suite import PromptDesignAgents, HumanEvalAgents, GSM8kAgents, SST2Agents\n",
    "\n",
    "prompt_design_agents = PromptDesignAgents()\n",
    "\n",
    "style_and_structure_expert = ExpertAgent(\"Style_and_Structure_Expert\", CorePrinciples(prompt_design_agents.get_style_and_structure_principles()), llm)\n",
    "conciseness_and_clarity_expert = ExpertAgent(\"Conciseness_and_Clarity_Expert\", CorePrinciples(prompt_design_agents.get_conciseness_and_clarity_principles()), llm)\n",
    "contextual_relevance_expert = ExpertAgent(\"Contextual_Relevance_Expert\", CorePrinciples(prompt_design_agents.get_contextual_relevance_principles()), llm)\n",
    "task_alignment_expert = ExpertAgent(\"Task_Alignment_Expert\", CorePrinciples(prompt_design_agents.get_task_alignment_principles()), llm)\n",
    "example_demonstration_expert = ExpertAgent(\"Example_Demonstration_Expert\", CorePrinciples(prompt_design_agents.get_example_demonstration_principles()), llm)\n",
    "incremental_prompting_expert = ExpertAgent(\"Incremental_Prompting_Expert\", CorePrinciples(prompt_design_agents.get_incremental_prompting_principles()), llm)\n",
    "\n",
    "human_eval_agents = HumanEvalAgents()\n",
    "\n",
    "code_reviewer = ExpertAgent(\"Code_Reviewer\", CorePrinciples(human_eval_agents.get_code_reviewer_principles()), llm)\n",
    "software_engineer = ExpertAgent(\"Software_Engineer\", CorePrinciples(human_eval_agents.get_software_engineering_principles()), llm)\n",
    "software_architect = ExpertAgent(\"Software_Architect\", CorePrinciples(human_eval_agents.get_software_architecture_principles()), llm)\n",
    "\n",
    "gsm8k_agents = GSM8kAgents()\n",
    "\n",
    "mathematician = ExpertAgent(\"Mathematician\", CorePrinciples(gsm8k_agents.get_mathematician_principles()), llm)\n",
    "word_problem_solver = ExpertAgent(\"Word_Problem_Solver\", CorePrinciples(gsm8k_agents.get_word_problem_solver_principles()), llm)\n",
    "\n",
    "sst2_agents = SST2Agents()\n",
    "\n",
    "graded_sentiment_analyst = ExpertAgent(\"Graded_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_graded_sentiment_analyst_principles()), llm)\n",
    "emotive_sentiment_analyst = ExpertAgent(\"Emotive_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_emotive_sentiment_analyst_principles()), llm)\n",
    "aspect_based_sentiment_analyst = ExpertAgent(\"Aspect_Based_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_aspect_based_sentiment_analyst_principles()), llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"\"\n",
    "if dataset == \"human_eval\":\n",
    "    baseline_prompt = human_eval_baseline\n",
    "    criteria = human_eval_criteria\n",
    "    domain_experts = [software_engineer, software_architect, code_reviewer]\n",
    "elif dataset == \"gsm8k\":\n",
    "    baseline_prompt = gsm8k_baseline\n",
    "    criteria = gsm8k_criteria\n",
    "    domain_experts = [mathematician, word_problem_solver]\n",
    "elif dataset == \"sst2\":\n",
    "    baseline_prompt = sst2_baseline_prompt\n",
    "    criteria = sst2_criteria\n",
    "    domain_experts = [graded_sentiment_analyst, emotive_sentiment_analyst, aspect_based_sentiment_analyst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:  Style_and_Structure_Expert\n",
      "Core Principles:  - Always structure prompts logically for the task\n",
      "- Always use a style and tone in prompts that is appropriate for the task\n",
      "- Always assign a role to the language model that is relevant to the task\n",
      "Position:  Conciseness_and_Clarity_Expert\n",
      "Core Principles:  - Always write clear and concise prompts\n",
      "- Always use simple and direct language in prompts\n",
      "- Always avoid ambiguity in prompts\n",
      "Position:  Contextual_Relevance_Expert\n",
      "Core Principles:  - Always provide context to help the model understand the task\n",
      "- Always write prompts informed by the context of the task\n",
      "- Always design contextually relevant roles for the language model\n",
      "Position:  Task_Alignment_Expert\n",
      "Core Principles:  - Always write prompts that align with the task criteria\n",
      "- Always tailor instructions to the task to guide the model\n",
      "- Always make the task abundantly clear to the model in the prompt\n",
      "Position:  Example_Demonstration_Expert\n",
      "Core Principles:  - Always provide examples to help the model understand the task\n",
      "- Always provide examples that cover a range of complexities\n",
      "- Always demonstrate the expected output of the model\n",
      "Position:  Incremental_Prompting_Expert\n",
      "Core Principles:  - Always break-down complex tasks\n",
      "- Always write clear step-by-step instructions to guide the model\n",
      "- Always write instructions appropriate for the task complexity\n"
     ]
    }
   ],
   "source": [
    "experts = [style_and_structure_expert, conciseness_and_clarity_expert, contextual_relevance_expert, task_alignment_expert, example_demonstration_expert, incremental_prompting_expert] + domain_experts\n",
    "market = MarketPlace(\n",
    "    base_prompt=baseline_prompt,\n",
    "    criteria=criteria,\n",
    "    experts=experts,\n",
    "    llm=llm\n",
    "    )\n",
    "for expert in market.experts:\n",
    "    print(\"Position: \", expert.position + \"\\nCore Principles: \", expert.core_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Bidder: Task_Alignment_Expert, Bid: 6\n",
      "Highest Bidder: Conciseness_and_Clarity_Expert, Bid: 3\n",
      "Time taken: 27.353060722351074\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Revised Prompt:**\n",
      "\n",
      "Today's date is {today}, and the time is {time}. You are a helpful customer support assistant for JP Morgan Asset Management. A customer will contact you to get help with their account information. Follow these steps to assist the customer:\n",
      "\n",
      "1. **Initial Inquiry:**\n",
      "   - Ask the customer for their account number.\n",
      "\n",
      "2. **Account Verification:**\n",
      "   - Use the `account_exists` tool to check if the account number provided by the customer exists.\n",
      "   - If the account does not exist, inform the customer politely and end the conversation.\n",
      "\n",
      "3. **Customer Authentication:**\n",
      "   - If the account exists, ask the customer to confirm their name.\n",
      "   - Then, ask for their address, postcode, and date of birth.\n",
      "   - Use the `authenticate_account` tool to verify these details.\n",
      "\n",
      "4. **Post-Authentication:**\n",
      "   - If the account is authenticated, inform the customer that their account has been successfully authenticated.\n",
      "   - Ask the customer to wait for a moment and do not provide any account information at this stage.\n",
      "\n",
      "5. **Handling Unrelated Questions:**\n",
      "   - If the customer asks a question unrelated to their account (e.g., about the weather), use the `get_tavily_response` tool to answer accurately.\n",
      "\n",
      "**Tools Available:**\n",
      "- `account_exists`: Check if the account number provided by the customer exists.\n",
      "- `authenticate_account`: Verify the customer's name, address, postcode, and date of birth.\n",
      "- `get_tavily_response`: Answer any questions unrelated to the customer's account.\n",
      "\n",
      "**Important Notes:**\n",
      "- Always maintain a polite and professional tone throughout the conversation.\n",
      "- Ensure you ask for authentication details first before providing any account-related information.\n",
      "- Follow the steps logically and ensure the customer feels supported and informed.\n",
      "\n",
      "**Example Usage of Tools:**\n",
      "- `account_exists`: Input the account number to check if it exists.\n",
      "- `authenticate_account`: Input the customer's name, address, postcode, and date of birth to verify their identity.\n",
      "- `get_tavily_response`: Input the customer's question to get an accurate response.\n",
      "\n",
      "By adhering to these steps, you will provide clear and effective support to the customer.\n",
      "\n",
      "---\n",
      "\n",
      "**Success Criteria Check:**\n",
      "\n",
      "1. **Clear and Concise:** The prompt provides a step-by-step guide that is easy to follow.\n",
      "2. **Logical Structure:** The steps are numbered and logically ordered.\n",
      "3. **Professional Conduct:** The prompt includes a reminder to maintain a polite and professional tone.\n",
      "4. **Authentication First:** The prompt emphasizes asking for authentication details before providing any account-related information.\n",
      "5. **Tool Details:** The tools are clearly defined, and their usage is explained with examples.\n",
      "\n",
      "**Guidelines Check:**\n",
      "\n",
      "- **No Modification of Restrictions:** Existing restrictions are maintained.\n",
      "- **No Modification of Placeholders:** Placeholders are used as provided.\n",
      "- **Adherence to Guidelines:** The prompt follows all provided guidelines.\n",
      "\n",
      "This revised prompt meets all the success criteria and adheres to the strict guidelines.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for _ in range(1):\n",
    "    start = time.time()\n",
    "    result = market.optimise_prompt()\n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end - start}\")\n",
    "    times.append(end - start)\n",
    "    result[\"messages\"][-2].pretty_print()\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Revised Prompt:**\\n\\nToday's date is {today}, and the time is {time}. You are a helpful customer support assistant for JP Morgan Asset Management. A customer will contact you to get help with their account information. Follow these steps to assist the customer:\\n\\n1. **Initial Inquiry:**\\n   - Ask the customer for their account number.\\n\\n2. **Account Verification:**\\n   - Use the `account_exists` tool to check if the account number provided by the customer exists.\\n   - If the account does not exist, inform the customer politely and end the conversation.\\n\\n3. **Customer Authentication:**\\n   - If the account exists, ask the customer to confirm their name.\\n   - Then, ask for their address, postcode, and date of birth.\\n   - Use the `authenticate_account` tool to verify these details.\\n\\n4. **Post-Authentication:**\\n   - If the account is authenticated, inform the customer that their account has been successfully authenticated.\\n   - Ask the customer to wait for a moment and do not provide any account information at this stage.\\n\\n5. **Handling Unrelated Questions:**\\n   - If the customer asks a question unrelated to their account (e.g., about the weather), use the `get_tavily_response` tool to answer accurately.\\n\\n**Tools Available:**\\n- `account_exists`: Check if the account number provided by the customer exists.\\n- `authenticate_account`: Verify the customer's name, address, postcode, and date of birth.\\n- `get_tavily_response`: Answer any questions unrelated to the customer's account.\\n\\n**Important Notes:**\\n- Always maintain a polite and professional tone throughout the conversation.\\n- Ensure you ask for authentication details first before providing any account-related information.\\n- Follow the steps logically and ensure the customer feels supported and informed.\\n\\n**Example Usage of Tools:**\\n- `account_exists`: Input the account number to check if it exists.\\n- `authenticate_account`: Input the customer's name, address, postcode, and date of birth to verify their identity.\\n- `get_tavily_response`: Input the customer's question to get an accurate response.\\n\\nBy adhering to these steps, you will provide clear and effective support to the customer.\\n\\n---\\n\\n**Success Criteria Check:**\\n\\n1. **Clear and Concise:** The prompt provides a step-by-step guide that is easy to follow.\\n2. **Logical Structure:** The steps are numbered and logically ordered.\\n3. **Professional Conduct:** The prompt includes a reminder to maintain a polite and professional tone.\\n4. **Authentication First:** The prompt emphasizes asking for authentication details before providing any account-related information.\\n5. **Tool Details:** The tools are clearly defined, and their usage is explained with examples.\\n\\n**Guidelines Check:**\\n\\n- **No Modification of Restrictions:** Existing restrictions are maintained.\\n- **No Modification of Placeholders:** Placeholders are used as provided.\\n- **Adherence to Guidelines:** The prompt follows all provided guidelines.\\n\\nThis revised prompt meets all the success criteria and adheres to the strict guidelines.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"][-2].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max time: \", max(times))\n",
    "print(\"Min time: \", min(times))\n",
    "print(\"Average time: \", sum(times) / len(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
