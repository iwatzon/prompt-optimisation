{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authoritarian Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessage, _convert_to_message\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.output_parsers import JsonOutputToolsParser, JsonOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions, parse_response\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessageGraph\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from typing import List, Sequence, TypedDict, Annotated, Dict, Any, Optional\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = \"Authoritarian Optimisation\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langsmith import Client\n",
    "\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorePrinciples:\n",
    "    def __init__(self, core_principles: List[str]):\n",
    "        self.core_principles = core_principles\n",
    "    \n",
    "    def add_principle(self, principle: str):\n",
    "        \"\"\"\n",
    "        Adds a principle to the core principles list.\n",
    "        \n",
    "        :param principle: The principle to be added.\n",
    "        \"\"\"\n",
    "        self.core_principles.append(principle)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the core principles, each principle is listed on a new line with a preceding dash.\n",
    "        \n",
    "        Example:\n",
    "        - principle 1\n",
    "        - principle 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        return \"\\n\".join([f\"- {principle}\" for principle in self.core_principles])\n",
    "\n",
    "\n",
    "class AdvisorAgent:\n",
    "    \"\"\"\n",
    "    Advisor Agent class defining agents that provide feedback on prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, position: str, core_principles: CorePrinciples, llm = ChatOpenAI(temperature=1.0, model=\"gpt-4o\")):\n",
    "        self.position = position\n",
    "        self.core_principles = str(core_principles)\n",
    "        self.system_message = f\"\"\"You are an experienced: {self.position}. Your core principles are:\n",
    "{self.core_principles}\"\"\"\n",
    "        self.llm = llm\n",
    "\n",
    "    def review_prompt(self, state: Sequence[BaseMessage], criteria: str):\n",
    "        \"\"\"\n",
    "        Generates a review of the prompt.\n",
    "        \"\"\"\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            prompt_text = f\"\"\"Your task is to provide feedback on the prompt in the conversation above in light of your core princples.\n",
    "Always think outside the box and consider unconventional ideas.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt as an experienced: {self.position}. Understand it's content and intent.\n",
    "2. Explain how you think the prompt can be improved in light of your core principles.\n",
    "3. Submit your feedback.\"\"\"\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "            chain = prompt | self.llm\n",
    "            result = chain.invoke({\"messages\": state})\n",
    "        \n",
    "    def approval(self, state: Sequence[BaseMessage]):\n",
    "        \"\"\"\n",
    "        Agent to approve or reject the prompt.\n",
    "        \"\"\"\n",
    "        prompt_text = f\"\"\"Your task is to decide if the prompt in the conversation above is optimal in light of your core principles.\n",
    "\n",
    "If you think the prompt is optimal and does not require improvements in light of your core principles, return True.\n",
    "If you think the prompt needs improvements in light of your core principles, return False.\n",
    "\n",
    "Your reviewal process should be as follows:\n",
    "1. Read the prompt as an experienced: {self.position}. Understand it's content and intent.\n",
    "2. Determine whether the prompt is optimal in light of your core principles.\n",
    "3. Submit your decision.\"\"\"\n",
    "        function_def = {\n",
    "            \"name\": \"approval\",\n",
    "            \"description\": \"Get approval decision of advisor.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"advisor\": {\"type\": \"string\", \"enum\": [self.position]},\n",
    "                    \"decision\": {\"type\": \"string\", \"enum\": [\"True\", \"False\"]},\n",
    "                },\n",
    "                \"required\": [\"decision\", \"advisor\"],\n",
    "            },\n",
    "        }\n",
    "        if isinstance(self.llm, ChatOpenAI):\n",
    "            messages = [\n",
    "                (\"system\", self.system_message),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                (\"system\", prompt_text),\n",
    "            ]\n",
    "            prompt = ChatPromptTemplate.from_messages(messages)\n",
    "            chain = (\n",
    "                prompt\n",
    "                | self.llm.bind_functions(functions=[function_def], function_call=\"approval\")\n",
    "                | JsonOutputFunctionsParser()\n",
    "            )\n",
    "            result = chain.invoke({\"messages\": state})\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeaderAgent:\n",
    "    \"\"\"\n",
    "    LeaderAgent class defining an agent that generates and communicates with advisor agents to help optimise prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_prompt: str, criteria: str = None, advisors: List[AdvisorAgent] = None, llm = ChatOpenAI(temperature=1.0, model=\"gpt-4o\")):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.criteria = criteria\n",
    "        self.system_message = f\"\"\"You are an experienced: Lead AI Prompt Engineer. Your core principles are:\n",
    "- Always pay attention to detail when designing prompts\n",
    "- Always make informed decisions when designing prompts\n",
    "- Always be open to new ideas when designing prompts\"\"\"\n",
    "        self.llm = llm\n",
    "        self.advisors = advisors\n",
    "        self.iterations = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the iterations counter to 0.\n",
    "        \"\"\"\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def run_approval(self, state: Sequence[BaseMessage]) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Run the approval process for the prompt. Run concurrently\n",
    "        \"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(advisor.approval, state) for advisor in self.advisors]\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        return results\n",
    "    \n",
    "    def leader_decision(self, state: Sequence[BaseMessage]):\n",
    "        \"\"\"\n",
    "        LeaderAgent to decide the next advisor or to finish.\n",
    "        \"\"\"\n",
    "        self.iterations += 1\n",
    "        approval_results = self.run_approval(state)\n",
    "        print(\"Approval results:\", approval_results)\n",
    "        # Extract decisions from the results\n",
    "        if all(str(approval_result['decision']) == \"True\" for approval_result in approval_results) or self.iterations >= 6:\n",
    "            self.reset()\n",
    "            return \"FINISH\"\n",
    "        else:\n",
    "            # Only ask for advise from advisors that disapproved the prompt\n",
    "            disapproved_advisors = [approval_result['advisor'] for approval_result in approval_results if str(approval_result['decision']) == \"False\"]\n",
    "            disapproved_advisors_details = \"\\n\".join([f\"{advisor.position}: {advisor.core_principles}\" for advisor in self.advisors if advisor.position in disapproved_advisors])\n",
    "            # shuffle the options to avoid positional bias\n",
    "            random.shuffle(disapproved_advisors)\n",
    "            # options = [\"FINISH\"] + positions\n",
    "            prompt_text = f\"\"\"Your task is to select the next advisor to provide feedback on the prompt in the conversation above.\n",
    "Think carefully about which advisor will provide the most valuable feedback to make improvements.\n",
    "            \n",
    "The success criteria for the prompt are as follows:\n",
    "{self.criteria}\n",
    "\n",
    "Select one of the below advisors that disapproved of the previous prompt to provide feedback on how to improve it: \n",
    "{disapproved_advisors}\n",
    "\n",
    "The details of all disapproving advisors and their core principles are as follows: \n",
    "{disapproved_advisors_details}\n",
    "\n",
    "Your selection process should be as follows:\n",
    "1. Read the prompt as an experienced: Lead AI Prompt Engineer. Understand it's content and intent.\n",
    "2. Explicitly detail how you think the prompt can be improved. Assume the prompt always needs improvement.\n",
    "3. Select which disapproving advisor you think will provide the most valuable feedback to make improvements.\n",
    "4. Submit your selection for the next advisor.\"\"\"\n",
    "            function_def = {\n",
    "                \"name\": \"next\",\n",
    "                \"description\": \"Get the next role.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"next\": {\"type\": \"string\", \"enum\": disapproved_advisors},\n",
    "                    },\n",
    "                    \"required\": [\"next\"],\n",
    "                },\n",
    "            }\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", self.system_message),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                    (\"system\", prompt_text),\n",
    "                ]\n",
    "            )\n",
    "            if type(self.llm) == ChatOpenAI:\n",
    "                chain = (\n",
    "                    prompt\n",
    "                    | self.llm.bind_functions(functions=[function_def], function_call=\"next\")\n",
    "                    | JsonOutputFunctionsParser()\n",
    "                )\n",
    "                result = chain.invoke({\"messages\": state})\n",
    "                return result[\"next\"]\n",
    "\n",
    "    def update_prompt(self, state: Sequence[BaseMessage]) -> str:\n",
    "        \"\"\"\n",
    "        Updates the prompt with the feedback from the advisor agent.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(state) == 1:\n",
    "            return {\"next\": self.leader_decision(state)}\n",
    "        \n",
    "        prompt_text = f\"\"\"Your task is to improve the prompt in the conversation above in light of your core principles.\n",
    "If you recieve feedback and recommendations for the prompt, respond with a revised version of your previous attempts actioning the feedback.\n",
    "\n",
    "The success criteria for the prompt are as follows:\n",
    "{self.criteria}\n",
    "You will be penalized if the prompt does not meet this criteria.\n",
    "\n",
    "Below are strict guidelines that you MUST follow if making changes to the prompt:\n",
    "- DO NOT modify existing restrictions.\n",
    "- DO NOT modify or remove negations.\n",
    "- DO NOT add, modify or remove placeholders denoted by curly braces. If you wish to use curly braces in your response, use double curly braces to avoid confusion with placeholders.\n",
    "- ALWAYS treat placeholders as the actual content.\n",
    "You will be penalized if you do not follow these guidelines.\n",
    "\n",
    "Your update process should be as follows:\n",
    "1. Read the prompt as an experienced: Head AI Engineer. Understand it's content and intent.\n",
    "2. Think carefully about how you can implement the most recent feedback and revise the prompt.\n",
    "3. Explcitly go through each success criteria and ensure the prompt meets them. If not, revise the prompt to make sure it does.\n",
    "4. Explicitly go through each guideline and ensure the changes adhere to them. If not, revise the prompt to make sure it does.\n",
    "5. Submit your revised prompt.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_message),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                (\"system\", prompt_text),\n",
    "            ]\n",
    "        )\n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({\"messages\": state})\n",
    "        return {\"messages\": [AIMessage(content=result.content, name=\"Leader\")], \"next\": self.leader_decision(state)}\n",
    "\n",
    "    def construct_advisor_graph(self):\n",
    "        \"\"\"\n",
    "        Constructs a graph of advisor agents based on their roles and functions.\n",
    "        \"\"\"\n",
    "\n",
    "        def advisor_node(state, agent):\n",
    "            return agent.review_prompt(state[\"messages\"], self.criteria)\n",
    "        \n",
    "        def leader_node(state):\n",
    "            return self.update_prompt(state[\"messages\"]) \n",
    "        \n",
    "        # The agent state is the input to each node in the graph\n",
    "        class AgentState(TypedDict):\n",
    "            messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "            next: str\n",
    "\n",
    "        workflow = StateGraph(AgentState)\n",
    "        for advisor in self.advisors:\n",
    "            # Create a node for each advisor agent\n",
    "            node = functools.partial(advisor_node, agent=advisor)\n",
    "            workflow.add_node(advisor.position, node)\n",
    "        workflow.add_node(\"Leader\", leader_node)\n",
    "\n",
    "        members = [advisor.position for advisor in self.advisors]\n",
    "        for member in members:\n",
    "            # We want our advisors to ALWAYS \"report back\" to the leader when done\n",
    "            workflow.add_edge(member, \"Leader\")\n",
    "        # The leader populates the \"next\" field in the graph state with routes to a node or finishes\n",
    "        conditional_map = {k: k for k in members}\n",
    "        conditional_map[\"FINISH\"] = END\n",
    "        workflow.add_conditional_edges(\"Leader\", lambda x: x[\"next\"], conditional_map)\n",
    "        workflow.set_entry_point(\"Leader\")\n",
    "\n",
    "        memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "        graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    def optimise_prompt(self):\n",
    "        \"\"\"\n",
    "        Optimises a prompt by invoking a graph of advisor agents.\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=self.base_prompt, name=\"User\")],\n",
    "        }\n",
    "\n",
    "        # Construct the graph\n",
    "        graph = self.construct_advisor_graph()\n",
    "        # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        n = random.randint(0, 1000)\n",
    "        config = {\n",
    "            \"configurable\": {\"thread_id\": n},\n",
    "            \"recursion_limit\": 50,\n",
    "            }    \n",
    "\n",
    "        # Run the graph\n",
    "        for s in graph.stream(\n",
    "            initial_state,\n",
    "            config,\n",
    "            stream_mode=\"values\",\n",
    "            ):\n",
    "            if \"__end__\" not in s:\n",
    "                if len(s[\"messages\"]) > 1:\n",
    "                    s[\"messages\"][-1].pretty_print()\n",
    "                continue\n",
    "        \n",
    "        def message_to_dict(obj):\n",
    "            if isinstance(obj, HumanMessage) or isinstance(obj, AIMessage):\n",
    "                return {obj.name: obj.content}\n",
    "            raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
    "\n",
    "        if type(self.llm) == ChatOpenAI:\n",
    "            model = self.llm.model_name\n",
    "        else:\n",
    "            model = self.llm.model\n",
    "        temp = int(self.llm.temperature)\n",
    "        path = f\"/Users/iwatson/Documents/Research Project/prompt-optimisation/src/conversations/{model}/conversations_authoritarian_{temp}.json\"\n",
    "        if not os.path.exists(path):\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump([], f)\n",
    "        \n",
    "        with open(path, \"r\") as f:\n",
    "            # write messages to json file\n",
    "            data = json.load(f)\n",
    "            # get the current key number then increment it\n",
    "            key = len(data)\n",
    "            data.append({key: json.dumps(s, default=message_to_dict)})\n",
    "            \n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatAnthropic(temperature=0, model=\"claude-3-haiku-20240307\")\n",
    "# llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "# llm = ChatOllama(temperature=1, model=\"mistral:v0.3\")\n",
    "# llm = ChatOllama(temperature=0, model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_suite import PromptDesignAgents, HumanEvalAgents, GSM8kAgents, SST2Agents\n",
    "\n",
    "prompt_design_agents = PromptDesignAgents()\n",
    "\n",
    "style_and_structure_expert = AdvisorAgent(\"Style_and_Structure_Expert\", CorePrinciples(prompt_design_agents.get_style_and_structure_principles()), llm)\n",
    "conciseness_and_clarity_expert = AdvisorAgent(\"Conciseness_and_Clarity_Expert\", CorePrinciples(prompt_design_agents.get_conciseness_and_clarity_principles()), llm)\n",
    "contextual_relevance_expert = AdvisorAgent(\"Contextual_Relevance_Expert\", CorePrinciples(prompt_design_agents.get_contextual_relevance_principles()), llm)\n",
    "task_alignment_expert = AdvisorAgent(\"Task_Alignment_Expert\", CorePrinciples(prompt_design_agents.get_task_alignment_principles()), llm)\n",
    "example_demonstration_expert = AdvisorAgent(\"Example_Demonstration_Expert\", CorePrinciples(prompt_design_agents.get_example_demonstration_principles()), llm)\n",
    "incremental_prompting_expert = AdvisorAgent(\"Incremental_Prompting_Expert\", CorePrinciples(prompt_design_agents.get_incremental_prompting_principles()), llm)\n",
    "\n",
    "human_eval_agents = HumanEvalAgents()\n",
    "\n",
    "code_reviewer = AdvisorAgent(\"Code_Reviewer\", CorePrinciples(human_eval_agents.get_code_reviewer_principles()), llm)\n",
    "software_engineer = AdvisorAgent(\"Software_Engineer\", CorePrinciples(human_eval_agents.get_software_engineering_principles()), llm)\n",
    "software_architect = AdvisorAgent(\"Software_Architect\", CorePrinciples(human_eval_agents.get_software_architecture_principles()), llm)\n",
    "\n",
    "gsm8k_agents = GSM8kAgents()\n",
    "\n",
    "mathematician = AdvisorAgent(\"Mathematician\", CorePrinciples(gsm8k_agents.get_mathematician_principles()), llm)\n",
    "word_problem_solver = AdvisorAgent(\"Word_Problem_Solver\", CorePrinciples(gsm8k_agents.get_word_problem_solver_principles()), llm)\n",
    "\n",
    "sst2_agents = SST2Agents()\n",
    "\n",
    "graded_sentiment_analyst = AdvisorAgent(\"Graded_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_graded_sentiment_analyst_principles()), llm)\n",
    "emotive_sentiment_analyst = AdvisorAgent(\"Emotive_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_emotive_sentiment_analyst_principles()), llm)\n",
    "aspect_based_sentiment_analyst = AdvisorAgent(\"Aspect_Based_Sentiment_Analyst\", CorePrinciples(sst2_agents.get_aspect_based_sentiment_analyst_principles()), llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position:  Style_and_Structure_Expert\n",
      "Core Principles:\n",
      " - Always structure prompts logically for the task\n",
      "- Always use a style and tone in prompts that is appropriate for the task\n",
      "- Always assign a role to the language model that is relevant to the task\n",
      "Position:  Conciseness_and_Clarity_Expert\n",
      "Core Principles:\n",
      " - Always write clear and concise prompts\n",
      "- Always use simple and direct language in prompts\n",
      "- Always avoid ambiguity in prompts\n",
      "Position:  Contextual_Relevance_Expert\n",
      "Core Principles:\n",
      " - Always provide context to help the model understand the task\n",
      "- Always write prompts informed by the context of the task\n",
      "- Always design contextually relevant roles for the language model\n",
      "Position:  Task_Alignment_Expert\n",
      "Core Principles:\n",
      " - Always write prompts that align with the task criteria\n",
      "- Always tailor instructions to the task to guide the model\n",
      "- Always make the task abundantly clear to the model in the prompt\n",
      "Position:  Example_Demonstration_Expert\n",
      "Core Principles:\n",
      " - Always provide examples to help the model understand the task\n",
      "- Always provide examples that cover a range of complexities\n",
      "- Always demonstrate the expected output of the model\n",
      "Position:  Incremental_Prompting_Expert\n",
      "Core Principles:\n",
      " - Always break-down complex tasks\n",
      "- Always write clear step-by-step instructions to guide the model\n",
      "- Always write instructions appropriate for the task complexity\n",
      "Position:  Graded_Sentiment_Analyst\n",
      "Core Principles:\n",
      " - Always consider the nuances of sentiment in text\n",
      "- Always consider the level of positivity or negativity in text\n",
      "- Always grade the sentiment of text before making a decision\n",
      "Position:  Emotive_Sentiment_Analyst\n",
      "Core Principles:\n",
      " - Always consider the emotional impact of text\n",
      "- Always consider the emotive language in text\n",
      "- Always consider lexical choices in text\n",
      "Position:  Aspect_Based_Sentiment_Analyst\n",
      "Core Principles:\n",
      " - Always break down the text into aspects\n",
      "- Always consider the sentiment of each aspect\n",
      "- Always consider how aspects contribute to the overall sentiment of the text\n"
     ]
    }
   ],
   "source": [
    "from prompts.gpt_4o.human_eval_prompts import HumanEvalPrompts\n",
    "from prompts.gpt_4o.gsm8k_prompts import GSM8KPrompts\n",
    "from prompts.gpt_4o.sst2_prompts import SST2Prompts\n",
    "\n",
    "human_eval_prompts = HumanEvalPrompts()\n",
    "gsm8k_prompts = GSM8KPrompts()\n",
    "sst2_prompts = SST2Prompts()\n",
    "\n",
    "baseline_prompt = sst2_prompts.get_baseline_prompt()\n",
    "criteria = sst2_prompts.get_criteria()\n",
    "\n",
    "leader_agent = LeaderAgent(\n",
    "    base_prompt=baseline_prompt,\n",
    "    criteria=criteria,\n",
    "    advisors=[\n",
    "        style_and_structure_expert,\n",
    "        conciseness_and_clarity_expert,\n",
    "        contextual_relevance_expert,\n",
    "        task_alignment_expert,\n",
    "        example_demonstration_expert,\n",
    "        incremental_prompting_expert,\n",
    "        graded_sentiment_analyst,\n",
    "        emotive_sentiment_analyst,\n",
    "        aspect_based_sentiment_analyst,\n",
    "    ],\n",
    "    llm = llm\n",
    ")\n",
    "for advisor in leader_agent.advisors:\n",
    "    print(\"Position: \", advisor.position + \"\\nCore Principles:\\n\", advisor.core_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approval results: [{'advisor': 'Incremental_Prompting_Expert', 'decision': 'False'}, {'advisor': 'Contextual_Relevance_Expert', 'decision': 'False'}, {'advisor': 'Graded_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Task_Alignment_Expert', 'decision': 'False'}, {'advisor': 'Emotive_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Example_Demonstration_Expert', 'decision': 'False'}, {'advisor': 'Style_and_Structure_Expert', 'decision': 'False'}, {'advisor': 'Conciseness_and_Clarity_Expert', 'decision': 'False'}, {'advisor': 'Aspect_Based_Sentiment_Analyst', 'decision': 'False'}]\n",
      "Approval results: [{'advisor': 'Style_and_Structure_Expert', 'decision': 'False'}, {'advisor': 'Task_Alignment_Expert', 'decision': 'False'}, {'advisor': 'Contextual_Relevance_Expert', 'decision': 'False'}, {'advisor': 'Emotive_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Graded_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Incremental_Prompting_Expert', 'decision': 'False'}, {'advisor': 'Aspect_Based_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Conciseness_and_Clarity_Expert', 'decision': 'False'}, {'advisor': 'Example_Demonstration_Expert', 'decision': 'False'}]\n",
      "Approval results: [{'advisor': 'Style_and_Structure_Expert', 'decision': 'False'}, {'advisor': 'Emotive_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Contextual_Relevance_Expert', 'decision': 'False'}, {'advisor': 'Example_Demonstration_Expert', 'decision': 'False'}, {'advisor': 'Conciseness_and_Clarity_Expert', 'decision': 'False'}, {'advisor': 'Incremental_Prompting_Expert', 'decision': 'False'}, {'advisor': 'Task_Alignment_Expert', 'decision': 'False'}, {'advisor': 'Aspect_Based_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Graded_Sentiment_Analyst', 'decision': 'False'}]\n",
      "Approval results: [{'advisor': 'Example_Demonstration_Expert', 'decision': 'False'}, {'advisor': 'Incremental_Prompting_Expert', 'decision': 'False'}, {'advisor': 'Conciseness_and_Clarity_Expert', 'decision': 'False'}, {'advisor': 'Task_Alignment_Expert', 'decision': 'False'}, {'advisor': 'Aspect_Based_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Emotive_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Style_and_Structure_Expert', 'decision': 'False'}, {'advisor': 'Contextual_Relevance_Expert', 'decision': 'False'}, {'advisor': 'Graded_Sentiment_Analyst', 'decision': 'False'}]\n",
      "Approval results: [{'advisor': 'Conciseness_and_Clarity_Expert', 'decision': 'False'}, {'advisor': 'Incremental_Prompting_Expert', 'decision': 'False'}, {'advisor': 'Example_Demonstration_Expert', 'decision': 'False'}, {'advisor': 'Contextual_Relevance_Expert', 'decision': 'False'}, {'advisor': 'Graded_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Style_and_Structure_Expert', 'decision': 'False'}, {'advisor': 'Task_Alignment_Expert', 'decision': 'False'}, {'advisor': 'Aspect_Based_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Emotive_Sentiment_Analyst', 'decision': 'False'}]\n",
      "Approval results: [{'advisor': 'Style_and_Structure_Expert', 'decision': 'False'}, {'advisor': 'Contextual_Relevance_Expert', 'decision': 'False'}, {'advisor': 'Incremental_Prompting_Expert', 'decision': 'False'}, {'advisor': 'Emotive_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Example_Demonstration_Expert', 'decision': 'False'}, {'advisor': 'Conciseness_and_Clarity_Expert', 'decision': 'False'}, {'advisor': 'Task_Alignment_Expert', 'decision': 'False'}, {'advisor': 'Graded_Sentiment_Analyst', 'decision': 'False'}, {'advisor': 'Aspect_Based_Sentiment_Analyst', 'decision': 'False'}]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/iwatson/Documents/Research Project/prompt-optimisation/src/conversations/gpt-4o-mini/conversations_authoritarian_0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mleader_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimise_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m     times\u001b[38;5;241m.\u001b[39mappend(end \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[7], line 213\u001b[0m, in \u001b[0;36mLeaderAgent.optimise_prompt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/iwatson/Documents/Research Project/prompt-optimisation/src/conversations/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/conversations_authoritarian_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    214\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump([], f)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# write messages to json file\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Research Project/prompt-optimisation/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/iwatson/Documents/Research Project/prompt-optimisation/src/conversations/gpt-4o-mini/conversations_authoritarian_0.json'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "times = []\n",
    "for _ in range(1):\n",
    "    start = time.time()\n",
    "    result = leader_agent.optimise_prompt()\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "    result[\"messages\"][-1].pretty_print()\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmin\u001b[39m(times))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(times) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(times))\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "print(\"Max time: \", max(times))\n",
    "print(\"Min time: \", min(times))\n",
    "print(\"Average time: \", sum(times) / len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
